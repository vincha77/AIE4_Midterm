{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Engineering Cohort#4 Midterm Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### This notebook has all the code EXCEPT Model Finetuning (which is in a separate notebook)\n",
        ">\n",
        "> Here is the link to the notebook that has all the code used for finetuning.  \n",
        ">\n",
        "> [Click Here](vc_completed_aie4_midterm_finetuning_embeddings_pipeline.ipynb) to access the finetuning notebook.\n",
        ">\n",
        "> (NOTE - this notebook and the finetuning notebook leverage a number of utilities that are in [this](myutils) folder.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NOTE - May need to pin langchain_core version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN13TZlSCv4",
        "outputId": "424a6920-0cea-4e28-dce0-3de6f0a4cc3c"
      },
      "outputs": [],
      "source": [
        "# NOTE!!!\n",
        "# May need to pin version: langchain_core==0.2.38\n",
        "# !pip install -U -q langchain langchain-openai langchain_core==0.2.38 langchain-community langchainhub langchain-qdrant langchain_huggingface   langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -qU openai ragas qdrant-client pymupdf pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -qU faiss-cpu unstructured==0.15.7 python-pptx==1.0.2 nltk==3.9.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note - pin the version of pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip uninstall -y pyarrow\n",
        "# !pip install -qU sentence_transformers datasets pyarrow==14.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key here: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vinodchandrashekaran/.virtualenvs/aie4challenge/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "\n",
        "from myutils.rag_pipeline_utils import SimpleTextSplitter, SemanticTextSplitter, VectorStore, AdvancedRetriever\n",
        "from myutils.ragas_pipeline import RagasPipeline\n",
        "from myutils.finetuning import FineTuneModelAndEvaluateRetriever\n",
        "from myutils.rag_pipeline_utils import load_all_pdfs, get_vibe_check_on_list_of_questions\n",
        "\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1 - Load the Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Make a local copy of the two pdfs needed for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf -O ./data/docs_for_rag/Blueprint-for-an-AI-Bill-of-Rights.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# !wget https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf -O ./data/docs_for_rag/NIST.AI.600-1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load pdfs into Langchain Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_file_paths = [\n",
        "    './data/docs_for_rag/Blueprint-for-an-AI-Bill-of-Rights.pdf',\n",
        "    './data/docs_for_rag/NIST.AI.600-1.pdf'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded ./data/docs_for_rag/Blueprint-for-an-AI-Bill-of-Rights.pdf with 73 pages \n",
            "loaded ./data/docs_for_rag/NIST.AI.600-1.pdf with 64 pages \n",
            "loaded all files: total number of pages: 137 \n"
          ]
        }
      ],
      "source": [
        "documents = load_all_pdfs(pdf_file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quick Overview of Documents\n",
        "\n",
        "a.  2022: Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People\n",
        "    \n",
        "This is really two docs in one\n",
        "first doc sets up five principles and practices\n",
        "second one is labeled a technical companion; it expands on each principle as well as how to operationalize it; each principle is reiterated, followed by an articulation of what the principle is important, what should be expected of automated systems in regard to following this principle, and examples of how these principles can move into practice.\n",
        "\n",
        "\n",
        "b.  2024: National Institute of Standards and Technology (NIST) Artificial Intelligent Risk Management Framework\n",
        "\n",
        "First part describes the risks as well as Trustworthy AI characteristics to mitigate the risk\n",
        "Second part, in tabular form, describes mitigation plan for each risks; each risk is identified in the table by a serial number based on the first part of the document rather than by the actual name of the risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunking Strategy\n",
        "\n",
        "It is clear that chunking strategies should account for the semantics in the document, as well as the fact that there are strong connections between the first and second parts of the document.  This comment applies to both documents in this assignment.\n",
        "\n",
        "I will examine two alternatives:\n",
        "\n",
        "(a) BASELINE: use the Swiss-army-knife chunking approach: RecursiveCharacterTextSplitter\n",
        "\n",
        "(b) ADVANCED: Semantic Chunking\n",
        "\n",
        "\n",
        "\n",
        "WHY I CHOSE THESE TWO CHUNKING STRATEGIES\n",
        "1. RecursiveCharacterTextSplitter: if the chunk_size and chunk_overlap are set to reasonable numbers, this approach is surprisingly effective across a range of document content.  It is cost-effective, relatively easy to tune if needed, is well-suited for answering queries that are SIMPLE and those that require MULTI-CONTEXT.\n",
        "\n",
        "\n",
        "2. Semantic chunking has great appeal as it groups content that is contiguous and semantically similar in a single chunk.  To that end, the chunk sizes may be rather uneven.  Advantage: It avoids artificially splitting content that may be very similar into multiple chunks which would make the retriever work harder during the retrieval process and/or perhaps miss relevant context.  The downside is that it is not as cost-effective as it requires the use of an LLM during the chunking process.  It is likely to perform well for MULTI-CONTEXT and potentially queries that require REASONING."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Formulate and Load My Test Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_test_questions(filename):\n",
        "    \"\"\"\n",
        "    Loads a text file with questions\n",
        "\n",
        "    Input\n",
        "        name of file which contains a set of questions to test the RAG pipeline\n",
        "    \n",
        "    Output\n",
        "        List of questions\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        all_q = f.read()\n",
        "        all_q_list = all_q.split('\\n')\n",
        "    return all_q_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What process was followed to generate the AI Bill of Rights?',\n",
              " 'What is the AI Bill of Rights?',\n",
              " 'What are the set of five principles in the AI bill of Rights?',\n",
              " 'Who led the formulation of the AI Bill of Rights?',\n",
              " 'What rights do I have to ensure protection against algorithmic discrimination?',\n",
              " 'What rights do I have to ensure that my data stays private?',\n",
              " 'What rights do I have to ensure safe and effective systems?',\n",
              " 'What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?',\n",
              " 'What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?',\n",
              " 'How can organizations put data privacy into practice?',\n",
              " 'How can organizations put into practice protection against algorithmic discrimination',\n",
              " 'How can foreign actors spread misinformation through the use of AI?',\n",
              " 'How can US entities counter the use of AI to spread misinformation during the elections?',\n",
              " 'According to NIST, what are the major risks of generative AI?',\n",
              " 'How can AI developers reduce the risk of hallucinations?',\n",
              " 'What can be done to prevent AI from being used to harm society?',\n",
              " 'Does generative AI have bad environmental impacts?',\n",
              " 'How can we prevent the bad environmental impact of AI?',\n",
              " 'How can we safeguard against AI generating toxic or harmful content?',\n",
              " 'Is AI likely to be biased against certain types of people?  If so, how can we prevent this?']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_test_questions = load_test_questions(filename='./data/rag_questions_and_answers/my_test_questions.txt')\n",
        "my_test_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2 - Quick End-to-end Prototype RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Up RAG Template and RAG Prompt\n",
        "> NOTE that the RAG template and RAG Prompt below will be used throughout this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_template = \"\"\"\n",
        "Use the provided context to answer the following question.\n",
        "If you can't answer the question based on the context, say you don't know.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(template=rag_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Up OpenAI Embeddings and Chat Model For Use in Prototype and for Comparison Throughout This Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_embeddings_small = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "openai_embeddings_small_dimension = 1536\n",
        "openai_embeddings_small_context_window = 8191\n",
        "\n",
        "openai_chat_gpt4omini = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the large embeddings in Semantic Chunking below!!!\n",
        "openai_embeddings_large = OpenAIEmbeddings(model='text-embedding-3-large')\n",
        "openai_embeddings_large_dimension = 3072\n",
        "openai_embeddings_large_context_window = 8191\n",
        "\n",
        "# Set up the lmore performant chat model just in case I decide to use it later...\n",
        "openai_chat_gpt4o = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Snowflake-arctic-embed-m Model \n",
        "#### (Will be Finetuned Later in The Exercise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Why I Chose This Model\n",
        ">\n",
        "> On the AIE4 midterm, we are asked to state why we chose the particular embedding model that we did for finetuning.  These are the criteria I used:\n",
        ">\n",
        "> 1.  PARSIMONY: This model has approx 110 million parameters, so we can feasibly finetune the model with consumer-grade access to GPU and memory resources.  It can be done very quickly in a Colab notebook, for instance, with access to their GPU.  I chose to use the A100 to speed up the process, but the training would work just as well with other GPUs like T4 etc.\n",
        ">\n",
        "> 2.  PERFORMANCE: Despite the far fewer parameters, the model holds its own in terms of performance on benchmark tasks.\n",
        ">\n",
        "> 3.  CONVENIENT ACCESS: This model is conveniently available via Huggingface, so I could leverage the model hub as well as all the libraries that support access to this type of model (SentenceTransformer) as well as all the training/finetuning capabilities.\n",
        ">\n",
        "> 4.  NO-BRAINER REASON: It is an open-source model so we have access to all parameters and configurations needed for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-m\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "arctic_original_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\")\n",
        "arctic_original_embeddings_dimension = 768\n",
        "arctic_original_context_window_in_tokens = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Recursive Character Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "chunk_overlap = 300\n",
        "\n",
        "# instantiate baseline text splitter -\n",
        "# NOTE!!! The `SimpleTextSplitter` below is my wrapper around Langchain RecursiveCharacterTextSplitter!!!!\n",
        "# (see module for the code if needed)\n",
        "baseline_text_splitter = \\\n",
        "    SimpleTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, documents=documents)\n",
        "\n",
        "# split text for baseline case\n",
        "baseline_text_splits = baseline_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "557"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(baseline_text_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Semantic Chunking - NOTE Using OpenAI Embeddings Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded 137 to be split \n",
            "returning docs split into 266 chunks \n"
          ]
        }
      ],
      "source": [
        "# instantiate semantic text splitter\n",
        "#  NOTE!!!! SemanticTextSplitter is my wrapper around Langchain SemanticChunker\n",
        "#  see my module for code if needed\n",
        "# NOTE!!! I use openai large embeddings model to get the best possible representation of the semantics of sentences\n",
        "# and to ensure high-quality semantic chunking\n",
        "sem_text_splitter = \\\n",
        "    SemanticTextSplitter(llm_embeddings=openai_embeddings_large, threshold_type=\"interquartile\", documents=documents)\n",
        "\n",
        "# split text for semantic-chunking case\n",
        "sem_text_splits = sem_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vibe Check on My Test Questions - Read This First!!!\n",
        "\n",
        "NOTE:  Four RAG Pipelines are run below!!!  These are:\n",
        "\n",
        "1.  `Demo_Baseline_OpenAI`: This uses baseline chunking (`RecursiveCharacterTextSplitter`) and OpenAI embeddings as a Demo.\n",
        "\n",
        "2.  `Demo_Semantic_OpenAI`: uses semantic chunking (`SemanticChunker`) and OpenAI embeddings as a Demo.\n",
        "\n",
        "3.  `Baseline_Arctic_Original`: uses baseline chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "4.  `Semantic_Arctic_Original`: uses semantic chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "NOTE!!!\n",
        "Later in this notebook, I will finetune the `Snowflake/snowflake-arctic-embed-m` model embeddings and will then compare the finetuned embeddings from this model against the runs in 3. and 4. above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The AI Bill of Rights was generated through extensive consultation with the American public. It consists of five principles and associated practices designed to guide the design, use, and deployment of automated systems, ensuring they align with democratic values and protect civil rights, civil liberties, and privacy. The process involved input from experts across various sectors, including the private sector, governments, and international organizations.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. The framework was developed through extensive consultation with the American public and includes guidance for various organizations on how to uphold these values.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Equitable Design and Use**: Automated systems should be designed and used in an equitable way, ensuring that they do not contribute to unjustified different treatment based on protected classifications such as race, ethnicity, gender, religion, age, disability, and more.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are required to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design, which includes using representative data and protecting against proxies for demographic features.\n",
            "\n",
            "4. **Accessibility**: Systems should be designed to ensure accessibility for people with disabilities.\n",
            "\n",
            "5. **Disparity Testing**: There should be pre-deployment and ongoing disparity testing and mitigation to identify and address potential biases.\n",
            "\n",
            "6. **Organizational Oversight**: Clear organizational oversight is necessary to ensure compliance with these protections.\n",
            "\n",
            "7. **Independent Evaluation**: Independent evaluations and plain language reporting in the form of algorithmic impact assessments should be conducted, including results from disparity testing and mitigation efforts, and made public whenever possible.\n",
            "\n",
            "These rights aim to ensure that all individuals are treated fairly and equitably in their interactions with automated systems.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Protection from Abusive Data Practices**: You should be protected from abusive data practices through built-in protections.\n",
            "\n",
            "2. **Agency Over Data Use**: You have the right to have agency over how data about you is used, including the ability to make decisions regarding the collection, use, access, transfer, and deletion of your data.\n",
            "\n",
            "3. **Consent and Reasonable Expectations**: Data collection should conform to reasonable expectations, and only data that is strictly necessary for a specific context should be collected. Consent should only be used to justify data collection when it is appropriate.\n",
            "\n",
            "4. **Design Choices for Privacy**: Systems should be designed to include privacy protections by default, ensuring that user experience does not obfuscate user choice or burden users with privacy-invasive defaults.\n",
            "\n",
            "5. **Access to Your Data**: You have the right to access and correct your data, as outlined in laws like the Privacy Act of 1974, which limits data retention and requires that only relevant and necessary data is stored.\n",
            "\n",
            "6. **Enhanced Protections for Sensitive Data**: There are additional protections for data related to sensitive domains such as health, employment, and education, recognizing the intimate nature of this information.\n",
            "\n",
            "7. **Oversight of Surveillance Technologies**: You should be free from unchecked surveillance, and surveillance technologies should be subject to heightened oversight to protect your privacy and civil liberties.\n",
            "\n",
            "8. **Reporting on Data Decisions**: Whenever possible, you should have access to reporting that confirms your data decisions have been respected and assesses the potential impact of surveillance technologies on your rights.\n",
            "\n",
            "These rights are designed to empower you and protect your privacy in the context of data collection and usage.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. They should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to demonstrate their safety and effectiveness based on intended use. Additionally, you should have meaningful access to examine these systems, and there should be safeguards to protect the public from harm proactively. Systems should not be designed in a way that endangers your safety or the safety of your community, and independent evaluations should confirm their safety and effectiveness.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed when an automated system is being used and to understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of these systems are required to provide accessible documentation that includes clear descriptions of the system's functioning, the role of automation, notice of its use, and explanations of outcomes that are clear, timely, and accessible. Additionally, you should be notified of significant changes in the use case or key functionalities of the system. This right to notice and explanation is essential for ensuring transparency and accountability in the use of AI systems.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This means you should have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. This human consideration should be accessible, equitable, effective, and not impose an unreasonable burden on the public. In some cases, a human alternative may be required by law, especially for reasonable accommodations for people with disabilities.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adhering to Legal Frameworks**: Organizations should comply with laws such as the Privacy Act of 1974, which mandates privacy protections for personal information, limits data retention, and grants individuals the right to access and correct their data.\n",
            "\n",
            "2. **Limiting Data Retention**: They should only retain data that is \"relevant and necessary\" for their statutory purposes, thereby minimizing the amount of personal information stored.\n",
            "\n",
            "3. **Proactive Risk Management**: Organizations should identify potential privacy harms and manage them effectively, which may include deciding not to process data when risks outweigh benefits or implementing measures to mitigate acceptable risks.\n",
            "\n",
            "4. **Implementing Privacy-Preserving Security Practices**: This includes using privacy-enhancing technologies, fine-grained permissions, and access control mechanisms to ensure that data does not leak beyond its intended use.\n",
            "\n",
            "5. **Independent Evaluation**: Allowing for independent evaluations of data policies and making these evaluations public can help ensure accountability and transparency.\n",
            "\n",
            "6. **User Reporting**: Organizations should provide users with clear, machine-readable reports on what data is being collected or stored about them, ensuring that this information is understandable.\n",
            "\n",
            "7. **Design Choices**: Systems should be designed to include built-in privacy protections by default, ensuring that data collection aligns with reasonable expectations and only collects necessary information. User consent should be sought in a clear manner, without obfuscation or burdensome defaults.\n",
            "\n",
            "By following these practices, organizations can better protect data privacy and empower users regarding their personal information.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several proactive and continuous measures. These include:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct equity assessments as part of the system design process to identify and address potential biases.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used in automated systems is representative of the diverse populations they affect, avoiding reliance on proxies for demographic features.\n",
            "\n",
            "3. **Accessibility Considerations**: Design and develop systems that are accessible to people with disabilities.\n",
            "\n",
            "4. **Disparity Testing and Mitigation**: Perform pre-deployment and ongoing testing to identify and mitigate any disparities in how different groups are treated by the automated systems.\n",
            "\n",
            "5. **Organizational Oversight**: Establish clear oversight within the organization to monitor and address issues related to algorithmic discrimination.\n",
            "\n",
            "6. **Independent Evaluation**: Conduct independent evaluations and provide plain language reporting through algorithmic impact assessments, which should include results from disparity testing and information on mitigation efforts. These assessments should be made public whenever possible to ensure transparency and accountability.\n",
            "\n",
            "By following these practices, organizations can work towards designing and deploying automated systems in a more equitable manner, thereby reducing the risk of algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI (GAI) systems to create both text-based disinformation and highly realistic deepfakes, which are synthetic audiovisual content and photorealistic images. These systems can facilitate the deliberate production and dissemination of false or misleading information at scale, especially when the intent is to deceive or cause harm. \n",
            "\n",
            "Additionally, GAI can assist in creating compelling imagery and propaganda that supports disinformation campaigns, potentially increasing their reach and engagement on social media platforms. The ability to manipulate text or images subtly can also enhance the sophistication of these disinformation efforts, allowing for targeted messaging aimed at specific demographics. Overall, the capabilities of GAI make it easier for malicious actors to produce and spread misinformation effectively.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "US entities can counter the use of AI to spread misinformation during elections by integrating tools designed to analyze content provenance, detect data anomalies, verify the authenticity of digital signatures, and identify patterns associated with misinformation or manipulation. Additionally, they can develop metrics to evaluate the effectiveness of these tools across diverse populations and assess novel methods for measuring risks related to generative AI, ensuring that the outputs remain valid, reliable, and factually accurate.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI (GAI) can be categorized into three main areas:\n",
            "\n",
            "1. **Technical / Model Risks**: These include risks from malfunction such as confabulation, dangerous or violent recommendations, data privacy issues, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses malicious uses of GAI, including the dissemination of chemical, biological, radiological, and nuclear (CBRN) information or capabilities, data privacy violations, and the generation of obscene or degrading content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These risks pertain to broader systemic issues, including data privacy concerns, environmental impacts, and intellectual property challenges.\n",
            "\n",
            "Additionally, some risks may cross-cut these categories, indicating that they can affect multiple areas simultaneously.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "The provided context does not specify methods for AI developers to reduce the risk of hallucinations. Therefore, I don't know how AI developers can reduce the risk of hallucinations.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be implemented:\n",
            "\n",
            "1. **Establishing Ethical Principles**: Organizations and governments can adopt principles for the ethical use of AI, such as those outlined in the OECD's recommendations and the U.S. Executive Orders on trustworthy AI.\n",
            "\n",
            "2. **Regulatory Compliance**: AI systems should be required to be lawful, purposeful, accurate, safe, understandable, responsible, and accountable. Regular monitoring and transparency in AI operations are essential.\n",
            "\n",
            "3. **Governance Tools**: Organizations can implement governance tools and protocols to restrict AI applications that cause harm or conflict with their values. This includes auditing, assessment, and change-management controls.\n",
            "\n",
            "4. **Public Engagement**: Engaging the public in discussions about AI's potential harms and benefits can help shape policies and frameworks that protect society.\n",
            "\n",
            "5. **Collaboration Across Sectors**: Collaboration between government agencies, private sector companies, and researchers can lead to the development of innovative guardrails and protections in AI design and use.\n",
            "\n",
            "6. **Frameworks for Ethical Use**: Specific frameworks for the ethical use of AI can be developed by government agencies, as seen with the Department of Energy and the Department of Defense, to address ethical concerns in AI deployment.\n",
            "\n",
            "These measures collectively aim to ensure that AI technologies are developed and used in ways that prioritize safety, accountability, and societal well-being.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The context indicates that training, maintaining, and operating generative AI systems are resource-intensive activities with potentially large energy and environmental footprints. For example, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive than non-generative tasks.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, several strategies can be implemented:\n",
            "\n",
            "1. **Model Distillation and Compression**: Creating smaller versions of trained models can reduce energy consumption and carbon emissions during inference. This involves techniques like model distillation or compression.\n",
            "\n",
            "2. **Environmental Impact Assessment**: Organizations should measure or estimate the environmental impacts, such as energy and water consumption, associated with training, fine-tuning, and deploying AI models. This includes verifying trade-offs between resources used at inference time versus those required at training time.\n",
            "\n",
            "3. **Carbon Capture and Offset Programs**: Verifying the effectiveness of carbon capture or offset programs for AI training and applications can help mitigate environmental impacts. It's important to address concerns related to green-washing in these initiatives.\n",
            "\n",
            "4. **Governance Tools and Protocols**: Applying governance tools and protocols that are used for other types of AI systems can also be beneficial. This includes auditing, assessment, and alignment with organizational values to ensure responsible AI use.\n",
            "\n",
            "By implementing these strategies, organizations can work towards minimizing the environmental footprint of AI technologies.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Feedback Mechanisms**: Use feedback from internal and external users to assess the impact of AI-generated content. Structured feedback mechanisms can help capture user input to detect shifts in quality or alignment with community values.\n",
            "\n",
            "2. **Content Filters**: Implement content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content. These filters can be rule-based or utilize machine learning models to flag problematic inputs and outputs.\n",
            "\n",
            "3. **Real-Time Monitoring**: Establish real-time monitoring processes to analyze generated content continuously. This helps in identifying and addressing harmful content as it occurs.\n",
            "\n",
            "4. **Incident Response Plans**: Develop and practice incident response plans for addressing the generation of inappropriate or harmful content. This includes conducting post-mortem analyses to understand root causes and implement preventive measures.\n",
            "\n",
            "5. **Regular Monitoring and Reporting**: Conduct regular monitoring of generative AI systems and publish reports detailing performance, feedback received, and improvements made.\n",
            "\n",
            "6. **Evaluate User-Reported Content**: Integrate user-reported problematic content into system updates to improve the AI's performance and reduce harmful outputs.\n",
            "\n",
            "7. **Transparency and Disclosure**: Consider disclosing the use of generative AI to end users in relevant contexts, which can help manage expectations and inform users about potential risks.\n",
            "\n",
            "By implementing these strategies, organizations can better manage the risks associated with AI-generated content and work towards minimizing the generation of toxic or harmful outputs.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. This bias can manifest in various forms, such as underrepresentation of women, racial minorities, and people with disabilities in AI-generated outputs. To prevent this, it is essential to implement algorithmic discrimination protections during the design, deployment, and ongoing use of AI systems. This includes conducting bias testing as part of product quality assessments, developing standards and guidance for automated systems, and ensuring oversight of human-based systems to mitigate potential biases. Additionally, addressing challenges related to datasets, testing and evaluation, and human factors is crucial in managing AI bias.\n"
          ]
        }
      ],
      "source": [
        "baseline_openai_retrieval_chain, baseline_openai_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Demo_Baseline_OpenAI\",\n",
        "                                        embeddings=openai_embeddings_small,  # <- openai embeddings\n",
        "                                        embed_dim=openai_embeddings_small_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=baseline_text_splits, # <- baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public over the course of a year. The White House Office of Science and Technology Policy led this initiative, seeking input from a diverse range of stakeholders, including impacted communities, industry representatives, technology developers, experts from various fields, and policymakers. This input was gathered through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The insights and experiences shared during these engagements played a central role in shaping the Blueprint for an AI Bill of Rights, which aims to protect the rights of the American public in the age of artificial intelligence.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework that outlines five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that civil rights, civil liberties, and privacy are upheld, and that individuals have equitable access to opportunities and critical resources. The framework is intended to address potential harms caused by automated systems and to promote democratic values. It includes guidance for various organizations on how to incorporate these protections into policy and practice.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not specify the exact five principles in the AI Bill of Rights. Therefore, I don't know the specific principles.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "The White House Office of Science and Technology Policy led the formulation of the AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Equitable Design and Use**: Automated systems should be designed and used in an equitable way, ensuring that they do not contribute to unjustified different treatment based on protected classifications such as race, gender, age, and disability.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are expected to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design, which includes using representative data and protecting against proxies for demographic features.\n",
            "\n",
            "4. **Accessibility**: Systems should be designed to ensure accessibility for people with disabilities.\n",
            "\n",
            "5. **Disparity Testing**: There should be pre-deployment and ongoing disparity testing and mitigation to identify and address potential biases.\n",
            "\n",
            "6. **Independent Evaluation**: Independent evaluations of automated systems should be conducted to assess potential algorithmic discrimination, and the results should be made public whenever possible.\n",
            "\n",
            "7. **Algorithmic Impact Assessment**: Entities responsible for automated systems should perform and publicly report on algorithmic impact assessments, which include disparity testing results and mitigation information.\n",
            "\n",
            "These rights aim to protect you from algorithmic discrimination and ensure that automated systems are used fairly and responsibly.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how your data is collected, used, accessed, transferred, and deleted. This includes the right to give or withhold consent for data collection.\n",
            "\n",
            "2. **Informed Consent**: Consent for data collection should be meaningful and understandable, provided in plain language, and specific to the context of use. Consent requests should not be complicated or misleading.\n",
            "\n",
            "3. **Protection from Abusive Practices**: You should be protected from abusive data practices through built-in protections and design choices that prioritize privacy.\n",
            "\n",
            "4. **Limited Data Collection**: Data collection should conform to reasonable expectations, and only data that is strictly necessary for a specific context should be collected.\n",
            "\n",
            "5. **Enhanced Protections for Sensitive Data**: There are additional protections for data related to sensitive domains such as health, education, and finance. Your data in these areas should only be used for necessary functions and should be subject to ethical review.\n",
            "\n",
            "6. **Transparency and Reporting**: You should have access to reporting that confirms your data decisions have been respected and provides an assessment of the impact of surveillance technologies on your rights.\n",
            "\n",
            "7. **Legal Rights**: Under laws like the Privacy Act of 1974, you have the right to access and correct your data held by federal agencies, and you can seek legal relief if your privacy rights are violated.\n",
            "\n",
            "These rights are designed to protect your privacy and ensure that your data is handled responsibly and ethically.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective automated systems. This includes the following specific rights:\n",
            "\n",
            "1. **Consultation**: You should be consulted during the design, implementation, deployment, acquisition, and maintenance phases of automated systems, particularly with diverse communities and stakeholders to identify concerns and risks.\n",
            "\n",
            "2. **Pre-deployment Testing**: Automated systems should undergo thorough testing to ensure they are safe and effective before being deployed.\n",
            "\n",
            "3. **Ongoing Monitoring**: There should be continuous monitoring of systems to ensure they remain safe and effective throughout their use.\n",
            "\n",
            "4. **Risk Mitigation**: Systems should be designed to mitigate risks and avoid outcomes that could endanger your safety or the safety of your community.\n",
            "\n",
            "5. **Opt-out Options**: You should have the ability to opt out of automated systems in favor of human alternatives when appropriate.\n",
            "\n",
            "6. **Access to Human Consideration**: You should have access to timely human consideration and remedy if an automated system fails or produces an error.\n",
            "\n",
            "7. **Independent Evaluation**: There should be independent evaluations of automated systems to confirm their safety and effectiveness, with results made public whenever possible.\n",
            "\n",
            "8. **Data Privacy**: You have rights regarding your data, including access to your data, the ability to correct it, and the ability to withdraw consent for data use.\n",
            "\n",
            "These rights are designed to ensure that automated systems are developed and operated in a manner that prioritizes your safety and well-being.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice when an automated system is being used that impacts you. This includes being informed about how and why decisions are made by such systems. The entity responsible for the automated system must provide documentation that describes how the system works, the role of automation in decision-making, and the identity of the responsible parties. \n",
            "\n",
            "Additionally, you should receive explanations that are tailored to your needs, which can help you understand the outcomes that affect you. These explanations should be available at the time of the decision or shortly thereafter, and they should be kept up-to-date. The notice and explanations should be assessed for clarity and accessibility, ensuring they are understandable to all users, including those with disabilities. \n",
            "\n",
            "In summary, you have the right to be informed and to understand the use of AI systems that affect your rights, opportunities, or access.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This means you should have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. These human alternatives should be accessible, equitable, effective, and not impose an unreasonable burden on the public. Additionally, in sensitive domains such as criminal justice, employment, education, and health, there should be tailored human oversight and consideration for high-risk decisions.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key principles and practices:\n",
            "\n",
            "1. **Privacy by Design and Default**: Automated systems should be designed with privacy protections built in from the outset. This includes assessing privacy risks throughout the development lifecycle and ensuring that data collection is minimized and clearly communicated.\n",
            "\n",
            "2. **Data Minimization**: Organizations should only collect data that is strictly necessary for specific, identified goals. This helps avoid \"mission creep\" where data is used for purposes beyond its original intent.\n",
            "\n",
            "3. **User Consent and Control**: Organizations should seek user permission for data collection and respect their decisions regarding the use, access, transfer, and deletion of their data. Consent requests should be clear, brief, and understandable.\n",
            "\n",
            "4. **Transparency**: Entities should provide clear information about what data is being collected, how it will be used, and who has access to it. Users should be able to access their data and correct it if necessary.\n",
            "\n",
            "5. **Data Retention Policies**: Clear timelines for data retention should be established, with data deleted as soon as possible in accordance with legal or policy-based limitations.\n",
            "\n",
            "6. **Risk Identification and Mitigation**: Organizations should proactively identify potential harms related to data collection and implement measures to mitigate these risks.\n",
            "\n",
            "7. **Independent Evaluation and Reporting**: Organizations should allow for independent evaluations of their data policies and practices, and provide reports to users about what data is being collected and how it is being used.\n",
            "\n",
            "8. **Enhanced Protections for Sensitive Data**: There should be stricter protections for data related to sensitive domains such as health, education, and finance, ensuring that such data is only used for necessary functions.\n",
            "\n",
            "By adopting these practices, organizations can better protect individual privacy and build trust with their users.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several proactive measures, including:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct assessments as part of the system design to identify potential biases and ensure equitable outcomes.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used in algorithms is representative of the diverse populations affected by the systems to avoid biases.\n",
            "\n",
            "3. **Protection Against Proxies for Demographic Features**: Implement safeguards to prevent the use of indirect indicators that could lead to discriminatory outcomes.\n",
            "\n",
            "4. **Accessibility for People with Disabilities**: Design and develop systems that are accessible to individuals with disabilities.\n",
            "\n",
            "5. **Pre-deployment and Ongoing Disparity Testing**: Test algorithms before deployment and continuously monitor them to identify and mitigate any disparities that arise.\n",
            "\n",
            "6. **Clear Organizational Oversight**: Establish oversight mechanisms to ensure accountability in the design and deployment of automated systems.\n",
            "\n",
            "7. **Independent Evaluation and Reporting**: Conduct independent evaluations and provide plain language reporting, including algorithmic impact assessments and results of disparity testing, to confirm that protections are in place.\n",
            "\n",
            "By following these practices, organizations can work towards minimizing the risk of algorithmic discrimination and ensuring that their automated systems are designed and used in an equitable manner.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI systems to produce and disseminate false or misleading content at scale. These systems can create both text-based disinformation and highly realistic deepfakes, which are synthetic audiovisual content and photorealistic images. The sophistication of these AI models allows malicious actors to target specific demographics with tailored disinformation campaigns. For instance, subtle manipulations in text or images can significantly influence human and machine perception, making the misinformation more convincing.\n",
            "\n",
            "Additionally, generative AI can assist in creating compelling imagery and propaganda that enhances the reach and engagement of disinformation on social media platforms. This capability can lead to significant societal impacts, such as eroding public trust in valid information and evidence, as demonstrated by instances where synthetic images have caused real-world consequences, like fluctuations in stock markets. Overall, the use of AI in misinformation campaigns can be both deliberate and unintentional, with the potential for widespread effects on public perception and trust.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "US entities can counter the use of AI to spread misinformation during elections by implementing several strategies based on the principles outlined in the context provided:\n",
            "\n",
            "1. **Establish Ethical Frameworks**: Government agencies, such as the Department of Energy and the Department of Defense, have developed ethical principles for AI use. These frameworks can guide the responsible development and deployment of AI systems to prevent misuse in spreading misinformation.\n",
            "\n",
            "2. **Promote Transparency and Accountability**: The Executive Order on Promoting the Use of Trustworthy Artificial Intelligence emphasizes the need for AI systems to be transparent and accountable. This includes regular monitoring and validation of AI outputs to ensure they do not contribute to misinformation.\n",
            "\n",
            "3. **Research and Development**: Funding and supporting research through programs like the National AI Research Institutes can help develop safe, trustworthy, and explainable AI systems. This research can focus on identifying and mitigating the risks of AI-generated misinformation.\n",
            "\n",
            "4. **Public Engagement and Education**: Engaging with stakeholders and the public about the risks of AI-generated misinformation can help raise awareness and promote critical thinking. This can be achieved through community-based participatory research and educational initiatives.\n",
            "\n",
            "5. **Risk Management Frameworks**: Utilizing frameworks like the NIST AI Risk Management Framework can help organizations assess and manage the risks associated with AI systems, including those that may produce or disseminate misinformation.\n",
            "\n",
            "6. **Regulatory Measures**: Implementing laws and policies that require AI systems to be free of bias and to undergo validation before deployment can help ensure that these systems do not contribute to misinformation.\n",
            "\n",
            "By combining these approaches, US entities can create a robust strategy to counter the potential misuse of AI in spreading misinformation during elections.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI (GAI) can be categorized into several areas:\n",
            "\n",
            "1. **Technical / Model Risks**: These include issues such as confabulation, dangerous or violent recommendations, data privacy concerns, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses risks related to malicious use, such as the dissemination of chemical, biological, radiological, and nuclear (CBRN) information, data privacy violations, and the generation of obscene or degrading content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These risks pertain to broader impacts on society, including data privacy issues, environmental concerns, and intellectual property challenges.\n",
            "\n",
            "Additionally, some risks are cross-cutting between these categories, and the document notes that GAI can exacerbate existing AI risks while also creating unique risks. The risks can vary based on the stage of the AI lifecycle, the scope of the application, the source of the risk, and the time scale over which they may materialize.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by implementing several strategies, including:\n",
            "\n",
            "1. **Establishing Policies and Procedures**: Develop policies that define roles and responsibilities for oversight of AI systems, ensuring independent evaluations or assessments of generative AI models proportional to identified risks.\n",
            "\n",
            "2. **Testing and Evaluation**: Adjust organizational roles across the lifecycle stages of AI systems, including thorough testing, evaluation, validation, and red-teaming of generative AI systems.\n",
            "\n",
            "3. **User Feedback Mechanisms**: Create policies for user feedback that include clear instructions and mechanisms for recourse, allowing users to report inaccuracies or issues.\n",
            "\n",
            "4. **Risk Measurement and Improvement**: Implement continual improvement processes for measuring risks associated with generative AI, addressing issues of explainability and transparency through documentation and various analytical techniques.\n",
            "\n",
            "5. **Standardized Measurement Protocols**: Use standardized protocols for risk measurement in the context of use, including structured public feedback exercises like AI red-teaming or independent evaluations.\n",
            "\n",
            "By focusing on these areas, developers can enhance the reliability of AI systems and mitigate the occurrence of hallucinations.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be implemented:\n",
            "\n",
            "1. **Establish Ethical Frameworks**: Government agencies, such as the Department of Energy and the Department of Defense, have developed ethical principles and frameworks for the responsible use of AI. These frameworks guide the development and deployment of AI systems to ensure they align with societal values and ethical standards.\n",
            "\n",
            "2. **Implement Risk Management Frameworks**: The National Institute of Standards and Technology (NIST) is developing a risk management framework that focuses on trustworthiness, accuracy, explainability, and the mitigation of harmful bias. This framework aims to incorporate these considerations throughout the AI lifecycle.\n",
            "\n",
            "3. **Conduct Regular Monitoring and Auditing**: AI systems should be regularly monitored and audited to ensure they remain compliant with ethical standards and do not cause harm. This includes independent evaluations and assessments proportional to the identified risks.\n",
            "\n",
            "4. **Promote Transparency and Accountability**: Policies should be in place to ensure transparency in AI systems, including public access to information about how these systems operate and the data they use. Accountability mechanisms should also be established to address any negative impacts.\n",
            "\n",
            "5. **Engage Stakeholders**: Meaningful stakeholder engagement in the design and implementation of AI systems can help identify potential risks and ensure that diverse perspectives are considered.\n",
            "\n",
            "6. **Establish Clear Use Policies**: Organizations should define acceptable use policies for AI systems, including guidelines on the types of queries AI applications should refuse to respond to, thereby preventing misuse.\n",
            "\n",
            "7. **Foster a Safety-First Mindset**: Organizations should cultivate a culture that prioritizes safety and critical thinking in the design and deployment of AI systems, minimizing potential negative impacts.\n",
            "\n",
            "By implementing these strategies, society can better safeguard against the harmful use of AI technologies.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The training, maintenance, and operation of generative AI systems are resource-intensive and can have large energy and environmental footprints. For instance, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive compared to non-generative tasks. While methods like model distillation or compression may reduce environmental impacts at inference time, the training and tuning of these models still contribute to their overall environmental footprint.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, several strategies can be implemented:\n",
            "\n",
            "1. **Assess Environmental Impacts**: Document and assess the anticipated environmental impacts of AI model development, maintenance, and deployment during product design decisions.\n",
            "\n",
            "2. **Measure Resource Consumption**: Measure or estimate the environmental impacts, such as energy and water consumption, for training, fine-tuning, and deploying AI models. This includes verifying trade-offs between resources used during inference versus those required during training.\n",
            "\n",
            "3. **Implement Carbon Capture Programs**: Verify the effectiveness of carbon capture or offset programs for AI training and applications, addressing concerns about green-washing.\n",
            "\n",
            "4. **Develop Smaller Models**: Utilize methods like model distillation or compression to create smaller versions of trained models, which can reduce environmental impacts during inference.\n",
            "\n",
            "5. **Establish Frameworks for Ethical Use**: Follow frameworks developed by organizations like the National Institute of Standards and Technology (NIST) that focus on trustworthiness, including environmental sustainability considerations.\n",
            "\n",
            "By integrating these practices, the environmental footprint of AI systems can be significantly reduced.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Monitoring and Testing**: Regularly monitor the robustness and effectiveness of risk controls and mitigation plans through methods like red-teaming, field testing, and user feedback mechanisms.\n",
            "\n",
            "2. **Content Comparison**: Compare AI system outputs against pre-defined organizational risk tolerance, guidelines, and principles, and review AI-generated content to ensure compliance with these standards.\n",
            "\n",
            "3. **Document Training Data**: Maintain documentation of training data sources to trace the origin and provenance of AI-generated content, which helps in understanding potential biases and harmful outputs.\n",
            "\n",
            "4. **Feedback Loops**: Establish feedback loops between AI system content provenance and human reviewers, and update protocols as necessary to ensure ongoing effectiveness.\n",
            "\n",
            "5. **Bias Evaluation**: Evaluate AI-generated content for representational biases and employ techniques such as re-sampling, re-ranking, or adversarial training to mitigate these biases.\n",
            "\n",
            "6. **Due Diligence**: Conduct due diligence to analyze AI outputs for harmful content, misinformation, and other inappropriate material.\n",
            "\n",
            "7. **Content Filters**: Implement content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content, including specific measures against non-consensual intimate imagery and child sexual abuse material.\n",
            "\n",
            "8. **Real-Time Monitoring**: Utilize real-time monitoring processes to analyze generated content for performance and trustworthiness, triggering alerts for human intervention when deviations from desired standards occur.\n",
            "\n",
            "By applying these strategies, organizations can better manage the risks associated with AI-generated content and reduce the likelihood of producing toxic or harmful outputs.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. The context provides several examples of how biases can manifest in AI systems, such as healthcare algorithms discriminating against Black patients, automated sentiment analyzers being biased against Jews and gay people, and hiring algorithms reinforcing racial and gender stereotypes.\n",
            "\n",
            "To prevent this bias, the context suggests several strategies:\n",
            "\n",
            "1. **Bias Testing**: Companies can implement bias testing as part of their product quality assessment and launch procedures to identify and mitigate biases before products are released.\n",
            "\n",
            "2. **Algorithmic Bias Safeguards**: Initiatives have been developed to create structured questionnaires for businesses to evaluate the data and models used in their AI systems, focusing on training data, biases identified, and mitigation steps.\n",
            "\n",
            "3. **Standards and Guidelines**: Organizations like NIST have released guidelines and standards for identifying and managing bias in AI, which include addressing challenges related to datasets, testing, evaluation, and human factors.\n",
            "\n",
            "4. **Transparency and Audits**: Non-profits and companies can conduct audits and impact assessments to identify potential algorithmic discrimination and provide transparency to the public.\n",
            "\n",
            "5. **Incorporating Accessibility Criteria**: Standards organizations have developed guidelines to incorporate accessibility criteria into technology design processes, ensuring that AI systems are equitable and inclusive.\n",
            "\n",
            "Overall, a combination of proactive measures, standards, and ongoing evaluation is necessary to mitigate bias in AI systems.\n"
          ]
        }
      ],
      "source": [
        "sem_openai_retrieval_chain, sem_openai_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Demo_Semantic_OpenAI\",\n",
        "                                        embeddings=openai_embeddings_small, # <- openai embeddings\n",
        "                                        embed_dim=openai_embeddings_small_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=sem_text_splits, # <- semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The context does not provide specific details about the process followed to generate the AI Bill of Rights. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework designed to assist governments and the private sector in implementing principles that protect civil rights, civil liberties, and privacy in the context of automated systems. It aims to ensure that the transformative potential of AI technologies is harnessed while preventing potential harms. The framework includes recommendations for moving principles into practice and serves as a blueprint for developing additional technical standards and practices tailored to specific sectors and contexts.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "Based on the context provided, you have several rights to ensure protection against algorithmic discrimination:\n",
            "\n",
            "1. **Proactive Measures**: Organizations deploying automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination. This includes conducting proactive equity assessments during the system design phase.\n",
            "\n",
            "2. **Use of Representative Data**: There should be an emphasis on using representative data to avoid biases that could lead to discrimination.\n",
            "\n",
            "3. **Protection Against Proxies**: Organizations should ensure that proxies for demographic features are not used in a way that leads to discrimination.\n",
            "\n",
            "4. **Accessibility**: Systems should be designed to be accessible for people with disabilities.\n",
            "\n",
            "5. **Disparity Testing and Mitigation**: There should be pre-deployment and ongoing testing for disparities, with measures in place to mitigate any identified issues.\n",
            "\n",
            "6. **Organizational Oversight**: Clear oversight from organizations is necessary to ensure these protections are upheld.\n",
            "\n",
            "7. **Independent Evaluation**: Independent evaluations and algorithmic impact assessments should be conducted, with results made public whenever possible to confirm that protections against discrimination are effective.\n",
            "\n",
            "These rights aim to ensure that automated systems are designed and used in an equitable manner, minimizing the risk of algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how data about you is collected, used, accessed, transferred, and deleted. This includes the right to give or withhold permission for data collection.\n",
            "\n",
            "2. **Built-in Protections**: Systems should be designed with privacy protections by default, ensuring that only data strictly necessary for a specific context is collected.\n",
            "\n",
            "3. **Transparency and Clarity**: Designers and developers should not create user experiences that obscure your choices or burden you with privacy-invasive defaults.\n",
            "\n",
            "4. **Ethical Review and Oversight**: There should be ethical reviews and prohibitions on the use of your data, and you should be protected from unchecked surveillance.\n",
            "\n",
            "5. **Access to Reporting**: You should have access to reports that confirm your data decisions have been respected and assess the impact of surveillance technologies on your rights and opportunities.\n",
            "\n",
            "These rights aim to protect you from abusive data practices and ensure that your privacy is respected in an increasingly automated society.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities and experts to identify risks and impacts. They must undergo pre-deployment testing, risk identification, and ongoing monitoring to ensure they are safe and effective. You also have the right to not have a system deployed or to have it removed if it poses a risk to your safety or the safety of your community. Automated systems should be designed to proactively protect you from unintended harms.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed when an automated system is being used and to understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of these systems are required to provide accessible documentation that includes:\n",
            "\n",
            "1. Clear descriptions of the overall system functioning and the role of automation.\n",
            "2. Notice that such systems are in use.\n",
            "3. Information about the individual or organization responsible for the system.\n",
            "4. Explanations of outcomes that are clear, timely, and accessible.\n",
            "\n",
            "Additionally, you should be notified of significant changes in use cases or key functionalities. You have the right to know how and why an outcome affecting you was determined by the automated system, including when it is not the sole input determining the outcome. The explanations provided should be technically valid, meaningful, and useful to you and others who need to understand the system.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by proactively identifying potential harms and managing them to avoid, mitigate, and respond appropriately to identified risks. This includes determining not to process data when privacy risks outweigh the benefits and implementing measures to mitigate acceptable risks. \n",
            "\n",
            "Additionally, organizations should follow privacy-preserving security best practices, such as using privacy-enhancing technologies and fine-grained permissions and access control mechanisms to ensure that data and metadata do not leak beyond the specific consented use case. \n",
            "\n",
            "Consent should be meaningful and understandable, with requests presented in plain language, allowing users to have agency over their data collection. Enhanced protections should be in place for sensitive domains, ensuring that data is only used for necessary functions and that users are protected by ethical reviews and prohibitions on misuse. \n",
            "\n",
            "Finally, organizations should avoid design choices that obscure user choice or impose privacy-invasive defaults, ensuring that users are free from unchecked surveillance.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key strategies:\n",
            "\n",
            "1. **Regular Assessment**: Organizations should regularly assess their automated systems to determine if they have led to algorithmic discrimination. This includes monitoring the system's outcomes and identifying any patterns of unusual results.\n",
            "\n",
            "2. **Diverse Testing Approaches**: The assessment can be conducted using various methods, such as testing with a sample of users or conducting qualitative user experience research, especially when demographic information of impacted individuals is available.\n",
            "\n",
            "3. **Increased Monitoring for High-Risk Systems**: Systems that are deemed riskier or have a higher impact should be monitored and assessed more frequently to ensure they adhere to equity standards.\n",
            "\n",
            "4. **Disparity Mitigation**: If the assessments reveal disparities, organizations should take steps to mitigate these issues. This may involve adjusting the system or reverting to earlier procedures that better meet equity standards.\n",
            "\n",
            "5. **Proxy Features Management**: Organizations should ensure that any proxy features used in their systems are not given undue weight and should monitor these closely to prevent algorithmic discrimination.\n",
            "\n",
            "By following these practices, organizations can work towards minimizing the risk of algorithmic discrimination in their automated systems.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, strategies might include focusing on safety as a primary concern, implementing data-driven approaches, ensuring valid and accurate systems, and addressing issues such as confirmation bias and the potential for unjust outcomes. Additionally, there should be an emphasis on the impact of AI systems on individuals and communities, with mechanisms in place to mitigate negative effects, such as job loss and surveillance-related chilling effects. Effective oversight, human review, and management of AI technologies are also essential to manage their risks effectively.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts due to high compute resource utilization in training or operating models, which can adversely affect ecosystems.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is important to address three broad challenges: improving datasets, enhancing testing and evaluation processes, and considering human factors. This involves ensuring that datasets are representative and free from bias, rigorously testing AI systems for discriminatory outcomes, and being aware of how human biases can influence AI development and deployment. Additionally, a socio-technical perspective can help in identifying and managing AI bias effectively.\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_original_retrieval_chain, baseline_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=baseline_text_splits, # <- baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to seek and distill input from various stakeholders, including impacted communities, industry representatives, technology developers, experts across different fields, and policymakers. This input was gathered through panel discussions, public listening sessions, meetings, a formal request for information, and contributions to a publicly accessible email address. The discussions highlighted both the transformative potential of AI and the necessity of preventing its harms, which played a central role in shaping the Blueprint for an AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to align automated systems with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to incorporate these protections into their policies and practices.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not provide the specific set of five principles in the AI Bill of Rights. Therefore, I don't know the answer.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by ensuring that data provided to them is used solely for its intended purpose. For example, if a health insurance company receives data to facilitate payment, that data should only be utilized for that specific purpose. Additionally, organizations should implement measures to mitigate, transfer, or avoid risks that exceed their risk tolerances, which can help in managing data privacy effectively. Regular information sharing, maintaining change management records, and keeping version history and metadata can also support organizations in responding to and managing data privacy incidents.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement reasonable measures that can prevent, flag, or take other actions in response to outputs that reproduce particular training data. This includes monitoring for and addressing issues related to plagiarized, trademarked, patented, licensed content, or trade secret material. Regular information sharing, change management records, version history, and metadata can also empower AI actors in responding to and managing AI incidents effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is critically important to ensure extensive human oversight in AI systems. This oversight can help identify and mitigate biases that may arise from the data used to train AI models or from the algorithms themselves.\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_original_retrieval_chain, sem_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=sem_text_splits, # <- semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Summary of The Anecdotal Responses to My Questions Above\n",
        "\n",
        "Note that when I use OpenAI Embeddings, the RAG Pipeline does a pretty decent job of responding to the test questions.  This is true for the baseline chunking as well as semantic chunking.\n",
        "\n",
        "However, the results appear to be only marginally ok for the two cases when I used the `snowflake-arctic-embed-m` embeddings out-of-the-box.  Of course, it is not as good as OpenAI embeddings.  But the other thing I noticed is that the context window for this model's embeddings is only 512 (compared to 8191 for OpenAI embeddings).  We should expect that in the formal RAGAS evaluation (coming up next), this model does pretty poorly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save Test Questions and Answers in File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def save_df_to_csv(q_a_data, csvfilename):\n",
        "    qa_df = pd.DataFrame(q_a_data, \n",
        "                         columns=['questions', 'answers'])\n",
        "    \n",
        "    filepath = Path(csvfilename)\n",
        "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "    qa_df.to_csv(filepath, index=False)\n",
        "    return\n",
        "\n",
        "\n",
        "save_df_to_csv(baseline_openai_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/baseline_openai_test_q_and_a.csv')\n",
        "\n",
        "save_df_to_csv(sem_openai_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/sem_openai_test_q_and_a.csv')\n",
        "\n",
        "save_df_to_csv(baseline_arctic_original_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/baseline_arctic_original_test_q_and_a.csv')\n",
        "\n",
        "save_df_to_csv(sem_arctic_original_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/sem_arctic_original_test_q_and_a.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DETOUR - TASK 2\n",
        "\n",
        "> At this stage, we have built quite a bit of the functionality needed for the Fast Prototype.\n",
        ">\n",
        "> There is a separate `app_v1.py` script and other resources around it (such as `Dockerfile`, `requirements.txt`, etc.) that were created at this stage.  *A fast prototype of the app was deployed to Huggingface Spaces.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loom Video to Demo the Fast Prototype of Working App on HF Spaces\n",
        "\n",
        "1.  Here is a link to the Loom video showing a demo of the prototype:\n",
        "\n",
        "        https://www.loom.com/share/3396b23b33f445ffb531ddcc8858487e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The stack I chose and some thoughts on it!!\n",
        "\n",
        "Heres my stack:\n",
        "1. PDF document loader:  `PyMuPDF` to load pdf documents  Ive found it to be acceptable as a general-purpose PDF loader; it is also conveniently packaged with Langchain tools as one of several PDF loaders.\n",
        "\n",
        "2. Chunking:  `Langchain`: for general purpose chunking (recursive character text splitter) as well as semantic chunking (it is their implementation of an open-source idea).  Extremely convenient and easy to use their different text splitters.\n",
        "\n",
        "3. Vector Store: `Qdrant`: I only implemented an in-memory vector store for this project, but I chose this application because it can potentially also be scaled very easily for industrial-strength use-cases.\n",
        "\n",
        "4. Retrieval chain (retriever, prompt and LLM): `Langchains LCEL`: to build out the retrieval chain; this is extremely convenient not only for fast prototyping but also scales very easily.\n",
        "\n",
        "5. Embeddings to vectorize the text\n",
        "-       OpenAI Embeddings  for the fast prototype, I used OpenAI text-embedding-3-small model embeddings.  These are very good as a general-purpose set of embeddings.  They are medium-sized vectors (dimension of 1536) and have decent context length (8191), so they can be used to encode fairly long chunks of text well.\n",
        "-       Finetuned Snowflake/snowflake-arctic-embed-m Embeddings: the base embeddings perform quite well; the model is parsimonious (110 million parameters) so it can be easily finetuned with consumer-grade resources; model is conveniently distributed via Huggingface\n",
        "-       Important to note  it was necessary to finetune the embeddings as the content of the corpus has fairly unique vocabulary that is unique to this domain, so in my stack I use the finetuned version of the model.\n",
        "\n",
        "6. OpenAI Chat Model: I used `gpt-4o-mini` as the LLM chat model throughout this project.  It is highly performant, cost-effective and quite fast.\n",
        "\n",
        "7. Web app: `Chainlit`: A very easy-to-use LLM-customized web-application; using Chainlit made it very easy to deploy the app on a hosting service such as Huggingface Spaces.\n",
        "\n",
        "8. Web hosting: `Hugging Face spaces`: HF has set up HF spaces as a Github repo that automatically detects when there are pushes or changes to the underlying app and immediately restarts the app.  For our purposes, this web hosting service was quite adequate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3 - Synthetically Generate Test Questions Using the RAGAS Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Up RAGAS Pipeline Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM models used in RAGAS pipeline\n",
        "ragas_generator_llm_model = 'gpt-3.5-turbo'\n",
        "ragas_critic_llm_model = 'gpt-4o-mini'\n",
        "\n",
        "# embeddings used for RAGAS pipeline\n",
        "ragas_openai_embeddings_model = 'text-embedding-3-small'\n",
        "\n",
        "# text splitter params\n",
        "ragas_chunk_size = 1500\n",
        "ragas_chunk_overlap = 500\n",
        "\n",
        "# number of qa pairs needed - reduce if running into rate limit issues\n",
        "ragas_number_of_qa_pairs = 20\n",
        "\n",
        "# initialize distributions - desired distribution of question types\n",
        "distributions = {\n",
        "    simple: 0.5,\n",
        "    multi_context: 0.4,\n",
        "    reasoning: 0.1\n",
        "}\n",
        "\n",
        "# name of file to persist RAGAS Q&A on disk\n",
        "ragas_testset_filename = \"./data/rag_questions_and_answers/ragas_questions_and_answers.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FLAG TO INDICATE IF RAGAS TESTSET SHOULD BE GENERATED IN THIS RUN\n",
        "# IF it is run, note the cost and time estimate below!!!\n",
        "generate_ragas_testset_now = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up list of RAGAS metrics used below\n",
        "ragas_metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Instantiate RAGAS Pipeline, Run Pipeline, Generate Test Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE - this cell will incur significant cost due to SDG's use of OpenAI models\n",
        "# Time taken on my local machine: ~ 15 mins\n",
        "\n",
        "ragas_pipeline = RagasPipeline(\n",
        "        generator_llm_model=ragas_generator_llm_model,\n",
        "        critic_llm_model=ragas_critic_llm_model,\n",
        "        embedding_model=ragas_openai_embeddings_model,\n",
        "        number_of_qa_pairs=ragas_number_of_qa_pairs,\n",
        "        chunk_size=ragas_chunk_size,\n",
        "        chunk_overlap=ragas_chunk_overlap,\n",
        "        documents=documents,\n",
        "        distributions=distributions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if generate_ragas_testset_now is True:\n",
        "    ragas_testset_df = ragas_pipeline.generate_testset()\n",
        "    ragas_testset_df.to_csv(ragas_testset_filename)\n",
        "else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load RAGAS Q&A from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "ragas_test_df = pd.read_csv(ragas_testset_filename)\n",
        "ragas_test_questions = ragas_test_df[\"question\"].values.tolist()\n",
        "ragas_test_groundtruths = ragas_test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate RAG Pipeline Using RAGAS Generated Synthetic Questions\n",
        "\n",
        "NOTE!!!\n",
        "\n",
        "The four cells below evaluate the four RAG pipelines built above:\n",
        "1.  Baseline chunking plus OpenAI embeddings\n",
        "2.  Semantic chunking plus OpenAI Embeddings\n",
        "3.  Baseline chunking plus Snowflake/snowflake-arctic-embed-m embeddings\n",
        "4.  Semantic chunking plus Snowflake/snowflake-arctic-embed-m embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:40<00:00,  1.97it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_openai_results, baseline_openai_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(baseline_openai_retrieval_chain, # <- baseline chunking + openai embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:40<00:00,  1.96it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_openai_results, sem_openai_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(sem_openai_retrieval_chain, # <- semantic chunking + openai embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:31<00:00,  2.56it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_original_results, baseline_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(baseline_arctic_original_retrieval_chain, # <- baseline chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:30<00:00,  2.63it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_original_results, sem_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(sem_arctic_original_retrieval_chain, # <- semantic chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compare The Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>BaselineChunkOpenAI</th>\n",
              "      <th>SemanticChunkOpenAI</th>\n",
              "      <th>BaselineChunkArcticOrig</th>\n",
              "      <th>SemanticChunkArcticOrig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.905942</td>\n",
              "      <td>0.892594</td>\n",
              "      <td>0.645067</td>\n",
              "      <td>0.315934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.977323</td>\n",
              "      <td>0.975088</td>\n",
              "      <td>0.680872</td>\n",
              "      <td>0.297117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.933542</td>\n",
              "      <td>0.961042</td>\n",
              "      <td>0.584583</td>\n",
              "      <td>0.412917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.891667</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.786667</td>\n",
              "      <td>0.204167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric  BaselineChunkOpenAI  SemanticChunkOpenAI  \\\n",
              "0       faithfulness             0.905942             0.892594   \n",
              "1   answer_relevancy             0.977323             0.975088   \n",
              "2  context_precision             0.933542             0.961042   \n",
              "3     context_recall             0.891667             0.916667   \n",
              "\n",
              "   BaselineChunkArcticOrig  SemanticChunkArcticOrig  \n",
              "0                 0.645067                 0.315934  \n",
              "1                 0.680872                 0.297117  \n",
              "2                 0.584583                 0.412917  \n",
              "3                 0.786667                 0.204167  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_baseline_openai = pd.DataFrame(list(baseline_openai_results.items()), columns=['Metric', 'BaselineChunkOpenAI'])\n",
        "df_sem_openai = pd.DataFrame(list(sem_openai_results.items()), columns=['Metric', 'SemanticChunkOpenAI'])\n",
        "df_merged_openai = pd.merge(df_baseline_openai, df_sem_openai, on='Metric')\n",
        "\n",
        "df_baseline_arctic_original = pd.DataFrame(list(baseline_arctic_original_results.items()), columns=['Metric', 'BaselineChunkArcticOrig'])\n",
        "df_sem_arctic_original = pd.DataFrame(list(sem_arctic_original_results.items()), columns=['Metric', 'SemanticChunkArcticOrig'])\n",
        "df_merged_arctic_original = pd.merge(df_baseline_arctic_original, df_sem_arctic_original, on='Metric')\n",
        "\n",
        "df_all_merged = pd.merge(df_merged_openai, df_merged_arctic_original, on='Metric')\n",
        "\n",
        "df_all_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of RAGAS Evaluation of RAG Pipelines Built So Far\n",
        "\n",
        "The table above shows the results of the four pipelines that Ive carried this far.  \n",
        "\n",
        "The two on the left are using OpenAI embeddings (baseline chunking and semantic chunking) and the two on the right are using the original downloaded version of snowflake-arctic-embed-m model embeddings.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Takeaways:\n",
        "---------\n",
        "1.  OpenAI dominates the snowflake-arctic-embed-m embedding based pipelines; not at all a surprise.\n",
        "\n",
        "2.  Retrieval-based measures show a slight improvement for OpenAI embeddings when we use semantic chunking rather than simple chunks on text splitting.  This is to be expected as the semantic chunks are organizing chunks based on semantic content precisely so that retrieval is better.\n",
        "\n",
        "3.  Generation-based measures such as faithfulness (measuring factual accuracy of generated answer) and answer relevancy (relevance of answer to question) also suffer with poor retrieval performance.  Notice the poor performace of the snowflake-arctic-embed-m models generation measures and how the retrieval measures are also pretty low.\n",
        "\n",
        "4.  Semantic chunking adversely affects the performance of snowflake-arctic-embed-m model.  I suspect it might be due to the context window of the model being rather low at 512 tokens.  It is possible that semantic chunks, at least some of them, are long.  The recursive text splitter may be better suited to smaller context length embedding models as one can control the size of the chunks relatively easily.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions about Effectiveness and Performance of RAG Ppelines so far\n",
        "\n",
        "1.  OpenAI performs very well out-of-the-box and is a great default choice for many such applications.\n",
        "\n",
        "2.  If we want to use open-source models like snowflake-arctic-embed-m in specialized RAG pipelines, we will need to finetune the model.\n",
        "\n",
        "3.  We enter the finetuning process (below) with healthy skepticism as the base model does not perform well and its context window is rather small (512 compared to 8191).  Nonetheless, it is worth a shot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4 - Fine-tuning Embeddings for RAG and Pull Down Finetuned Embeddings\n",
        "\n",
        "#### *NOTE: As mentioned at start of this notebook, I built a separate pipeline to do the finetuning of the embedding model.  Please refer to that notebook for the full code for finetuning.  Below, I pull down the finetuned model embeddings from my HF repo for use in the remainder of this notebook.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### [Here](vc_completed_aie4_midterm_finetuning_embeddings_pipeline.ipynb) is a link to the notebook that has the finetuning pipeline end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### And [here](https://huggingface.co/vincha77/finetuned_arctic) is a link to the Huggingface Hub where I have placed the results of my finetuned model called `vincha77/finetuned_arctic` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why I Chose to Finetune the `snowflake-arctic-embed-m` Model\n",
        "\n",
        "On the AIE4 midterm, we are asked to state why we chose the particular embedding model that we did for finetuning.  These are the criteria I used:\n",
        "\n",
        "1.  PARSIMONY: This model has approx 110 million parameters, so we can feasibly finetune the model with consumer-grade access to GPU and memory resources.  It can be done very quickly in a Colab notebook, for instance, with access to their GPU.  I chose to use the A100 to speed up the process, but the training would work just as well with other GPUs like T4 etc.\n",
        "\n",
        "2.  PERFORMANCE: Despite the far fewer parameters, the model holds its own in terms of performance on benchmark tasks.\n",
        "\n",
        "3.  CONVENIENT ACCESS: This model is conveniently available via Huggingface, so I could leverage the model hub as well as all the libraries that support access to this type of model (SentenceTransformer) as well as all the training/finetuning capabilities.\n",
        "\n",
        "4.  NO-BRAINER REASON: It is an open-source model so we have access to all parameters and configurations needed for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "## code here to pull from hub\n",
        "model_id = \"vincha77/finetuned_arctic\"\n",
        "arctic_finetuned_model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "arctic_finetuned_embeddings = HuggingFaceEmbeddings(model_name=\"vincha77/finetuned_arctic\")\n",
        "arctic_finetuned_embeddings_dimension = 768\n",
        "arctic_finetuned_context_window_in_tokens = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the TEST SET that was created during model finetuning (training and validation also saved here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('./data/finetuning_data/test_dataset.jsonl', \"r\") as f:\n",
        "    test_json = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiate a model evaluator to compute hit rate using testdata and different embeddings models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE that the class being instantiated below is used extensively during the finetuning process\n",
        "# I am only instantiating it to use the method defined there to run the Evaluations on the test dataset\n",
        "evr = FineTuneModelAndEvaluateRetriever(train_data=None, val_data=None, test_data=test_json, batch_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 414/414 [01:42<00:00,  4.04it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9347826086956522"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "te3_results = evr.evaluate_embeddings_model(openai_embeddings_small, top_k_for_retrieval=5)\n",
        "\n",
        "te3_results_df = pd.DataFrame(te3_results)\n",
        "\n",
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 414/414 [00:10<00:00, 39.61it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5217391304347826"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arctic_embed_m_results = evr.evaluate_embeddings_model(arctic_original_embeddings, top_k_for_retrieval=5)\n",
        "\n",
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)\n",
        "\n",
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 414/414 [00:11<00:00, 37.04it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9734299516908212"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetuned_results = evr.evaluate_embeddings_model(arctic_finetuned_embeddings, top_k_for_retrieval=5)\n",
        "\n",
        "finetuned_results_df = pd.DataFrame(finetuned_results)\n",
        "\n",
        "finetuned_hit_rate = finetuned_results_df[\"is_hit\"].mean()\n",
        "finetuned_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of Hit Rate Metric for the Three Pipelines\n",
        "\n",
        "1.  OpenAI `text-embeddings-3-small` model hit rate:    0.935\n",
        "2.  Snowflake `snowflake-arctic-embed-m` hit rate:      0.522\n",
        "3.  Finetuned version `finetuned_arctic` hit rate:      0.973"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway from these results\n",
        "\n",
        "1.  Another confirmation that OpenAI `text-embeddings-3-small` model is pretty good out-of-the-box.\n",
        "\n",
        "2.  Another confirmation that `snowflake-arctic-embed-m` model embeddings are not that great out-of-the-box.\n",
        "\n",
        "3.  The key takeaway though is that `FINETUNING WORKS`!!!  The `finetuned_arctic` model embeddings outperform OpenAI embeddings on this test corpus, quite an incredible feat!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vibe Check on My Test Questions\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on my test set of questions that I formulated first!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Recursive Character Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_chunk_size = 600\n",
        "new_chunk_overlap = 200\n",
        "\n",
        "# instantiate baseline text splitter -\n",
        "# NOTE!!! The `SimpleTextSplitter` below is my wrapper around Langchain RecursiveCharacterTextSplitter!!!!\n",
        "# (see module for the code if needed)\n",
        "new_baseline_text_splitter = \\\n",
        "    SimpleTextSplitter(chunk_size=new_chunk_size, chunk_overlap=new_chunk_overlap, documents=documents)\n",
        "\n",
        "# split text for baseline case\n",
        "new_baseline_text_splits = new_baseline_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "936"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(new_baseline_text_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Semantic Chunking - NOTE Using OpenAI Embeddings Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded 137 to be split \n",
            "returning docs split into 266 chunks \n"
          ]
        }
      ],
      "source": [
        "# instantiate semantic text splitter\n",
        "#  NOTE!!!! SemanticTextSplitter is my wrapper around Langchain SemanticChunker\n",
        "#  see my module for code if needed\n",
        "# NOTE!!! I use openai large embeddings model to get the best possible representation of the semantics of sentences\n",
        "# and to ensure high-quality semantic chunking\n",
        "new_sem_text_splitter = \\\n",
        "    SemanticTextSplitter(llm_embeddings=openai_embeddings_large, threshold_type=\"interquartile\", documents=documents)\n",
        "\n",
        "# split text for semantic-chunking case\n",
        "new_sem_text_splits = new_sem_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vibe Check on My Test Questions - Read This First!!!\n",
        "\n",
        "NOTE:  Four RAG Pipelines are run below!!!  These are:\n",
        "\n",
        "1.  `Baseline_Arctic_Original`: uses baseline chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "2.  `Baseline_Arctic_Finetuned`: uses baseline chunking and `Finetuned_Arctic` model embeddings.\n",
        "\n",
        "3.  `Semantic_Arctic_Original`: uses semantic chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "4.  `Semantic_Arctic_Finetuned`: uses semantic chunking and `Finetuned_arctic` model embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The context does not provide specific details about the process followed to generate the AI Bill of Rights. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, as outlined in the \"Blueprint for an AI Bill of Rights,\" is a framework designed to ensure that automated systems work for the American people while upholding civil rights, civil liberties, and privacy. It includes principles and guidelines for the responsible use of automated systems, aiming to assist both governments and the private sector in protecting these values. The document emphasizes the importance of evaluating and addressing the harms of automated systems at both individual and community levels.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Legal Protections**: Algorithmic discrimination may violate existing legal protections based on classifications such as race, gender, age, disability, and more. This means you can seek recourse if you believe you have been discriminated against by an automated system.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are encouraged to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination. This includes implementing practices for audits and impact assessments to identify potential biases.\n",
            "\n",
            "3. **Transparency**: There should be transparency in the design and deployment of automated systems, allowing the public to understand how these systems operate and how they mitigate biases.\n",
            "\n",
            "4. **Equitable Design**: Protections should be built into the design, deployment, and ongoing use of automated systems to ensure they operate in an equitable manner.\n",
            "\n",
            "5. **Bias Testing**: Some organizations are already implementing bias testing as part of their product quality assessments, which can lead to changes in products to prevent discrimination.\n",
            "\n",
            "These rights and measures aim to safeguard individuals from discrimination in both their digital and daily lives.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how data about you is used, which includes the ability to give or withhold permission for data collection.\n",
            "\n",
            "2. **Built-in Protections**: You should be protected from abusive data practices through built-in protections that are included by default in systems.\n",
            "\n",
            "3. **Reasonable Expectations**: Data collection should conform to reasonable expectations, meaning that only data strictly necessary for a specific context should be collected.\n",
            "\n",
            "4. **Transparency and Reporting**: You should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.\n",
            "\n",
            "5. **Legal Protections**: The use of your data is governed by legal protections that help to protect civil liberties and provide limits on data retention in some cases.\n",
            "\n",
            "6. **Consumer Data Privacy Regimes**: Many states have enacted consumer data privacy protection laws to address harms related to data use, although these are not yet standard practices across the United States.\n",
            "\n",
            "These rights aim to ensure that your personal data is handled with respect and care, minimizing risks to your privacy.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. Additionally, these systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective for their intended use. If safety violations or unintended consequences are identified, those systems should not be used until the risks can be mitigated. Ongoing risk mitigation may require significant modifications or rollbacks of launched automated systems.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed about the use of AI systems that impact your rights. This includes receiving adequate explanations regarding how these systems operate and the effects they may have on you. The information should be provided in clear, plain language and in a machine-readable format. Additionally, there are existing notice and explanation requirements in some sectors, and there is a push for consistent application of these requirements across all sectors to ensure the public is aware when automated systems are being used in ways that affect their rights.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. You should also have access to a person who can quickly consider and remedy any problems you encounter. The determination of appropriateness for opting out should be based on reasonable expectations in a given context, ensuring broad accessibility and protection from harmful impacts. In some cases, a human or other alternative may be required by law.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing privacy-enhancing technologies, establishing fine-grained permissions and access control mechanisms, and adhering to conventional system security protocols. Additionally, they should ensure that the use of data is governed by legal protections that help protect civil liberties and provide limits on data retention. Organizations can also adopt laws and policies that promote privacy protections, such as those outlined in the Privacy Act of 1974, which requires limits on data retention and provides individuals with rights regarding their personal information. Furthermore, organizations should ensure that sensitive data is subject to extra oversight and that any reuse of such data is legally authorized and beneficial, with appropriate risk mitigation measures in place.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by taking proactive and continuous measures to protect individuals and communities. This includes designing and deploying automated systems with built-in protections against bias, conducting bias testing as part of product quality assessments, and ensuring that governance structures are in place to oversee these systems. Additionally, organizations should be prepared to update the operation of their systems to mitigate any discriminatory effects that may arise.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI can be categorized into several areas:\n",
            "\n",
            "1. **Technical / Model Risks**: This includes risks from malfunction such as confabulation, dangerous or violent recommendations, data privacy issues, value chain and component integration problems, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses malicious uses such as the dissemination of chemical, biological, radiological, and nuclear (CBRN) information or capabilities, data privacy violations, human-AI configuration issues, and the generation of obscene, degrading, or abusive content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These risks include data privacy concerns, environmental impacts, and intellectual property issues.\n",
            "\n",
            "Some risks are noted to be cross-cutting between these categories.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "The provided context mentions \"Ecosystem / societal risks (or systemic risks): Environmental\" but does not provide specific details about the environmental impacts of generative AI. Therefore, I don't know if generative AI has bad environmental impacts based on the context provided.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement governance structures that can oversee and update the operation of AI systems. This includes addressing risks such as algorithmic discrimination, harmful bias, and the potential for misuse by humans. Additionally, a socio-technical perspective should be taken to identify and manage these risks effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is important to implement governance structures that can oversee and update the operation of AI systems to mitigate these biases. Additionally, a socio-technical perspective should be taken to identify and manage AI bias effectively.\n"
          ]
        }
      ],
      "source": [
        "new_baseline_arctic_original_retrieval_chain, new_baseline_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_baseline_text_splits, # <- NEW baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The AI Bill of Rights was generated through extensive consultation with the American public. It consists of five principles and associated practices designed to guide the design, use, and deployment of automated systems, ensuring they align with democratic values and protect civil rights, civil liberties, and privacy. The process involved collaboration among various stakeholders, including industry, civil society, researchers, policymakers, technologists, and the public.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, as outlined in the \"Blueprint for an AI Bill of Rights,\" is a set of five principles and associated practices designed to guide the design, use, and deployment of automated systems. Its purpose is to protect the rights of the American public in the age of artificial intelligence. The framework was developed through extensive consultation with the American public and aims to ensure that automated systems align with democratic values while safeguarding civil rights, civil liberties, and privacy.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The set of five principles in the AI Bill of Rights includes:\n",
            "\n",
            "1. Safe and Effective Systems\n",
            "2. Algorithmic Discrimination Protections\n",
            "3. Data Privacy\n",
            "4. Notice and Explanation\n",
            "5. Human Alternatives, Consideration, and Fallback\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have the right to protection against algorithmic discrimination, which includes the assurance that automated systems should be designed and used in an equitable manner. This means you should not face unjustified different treatment or negative impacts based on characteristics such as race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Additionally, depending on the specific circumstances, algorithmic discrimination may violate legal protections, and designers, developers, and deployers of automated systems are expected to take proactive measures to prevent such discrimination.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Protection from Abusive Data Practices**: You should be protected from harmful data practices through built-in protections and have agency over how your data is used.\n",
            "\n",
            "2. **Consent and Control**: Designers and developers of automated systems should seek your permission before collecting or using your data. You should have clear mechanisms to control access to and use of your data, including your metadata.\n",
            "\n",
            "3. **Data Minimization**: Only data that is strictly necessary for a specific context should be collected, and there should be limitations on data use and collection.\n",
            "\n",
            "4. **Legal Protections**: The Privacy Act of 1974 provides privacy protections for personal information in federal records systems, including limits on data retention and a general right to access and correct your data.\n",
            "\n",
            "5. **Transparency**: You should be informed about how your data is being collected, used, and shared.\n",
            "\n",
            "These rights are part of a broader framework aimed at protecting your privacy and ensuring that data practices are ethical and transparent. However, it's important to note that the current legal landscape may vary, and comprehensive protections may not be uniformly applied across all contexts.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. These systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective based on their intended use. Additionally, there should be safeguards in place to prevent the systems from endangering your safety or the safety of your community, including the possibility of not deploying or removing a system from use if it poses risks.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice regarding the use of AI systems that impact you. This includes:\n",
            "\n",
            "1. **Documentation**: The entity responsible for the AI system must provide documentation that describes how the system works in plain language, making it easy to find and understand.\n",
            "\n",
            "2. **Accountability**: Notices should clearly identify the entities responsible for designing and using the AI system.\n",
            "\n",
            "3. **Timeliness**: You should be notified of the use of automated systems in advance or while being impacted by the technology. Explanations should be available at the time of the decision or soon thereafter, and notices should be kept up-to-date.\n",
            "\n",
            "4. **Explanations**: You are entitled to receive explanations about how and why decisions were made by the AI system, which should be clear and valid.\n",
            "\n",
            "These rights are part of a framework aimed at ensuring transparency and accountability in the use of AI systems.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. You should also have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. This human consideration should be accessible, equitable, effective, and not impose an unreasonable burden on the public. In some cases, a human or other alternative may be required by law.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adhering to Legal Frameworks**: Following laws such as the Privacy Act of 1974, which mandates privacy protections for personal information, including limits on data retention.\n",
            "\n",
            "2. **Designing with Privacy in Mind**: Ensuring that systems are designed to include built-in protections for users, such as collecting only the data that is strictly necessary for specific contexts and conforming to reasonable expectations of privacy.\n",
            "\n",
            "3. **Obtaining User Consent**: Seeking permission from users before collecting or using their data, and providing clear information about the risks associated with data sharing.\n",
            "\n",
            "4. **Implementing Privacy-Preserving Security Practices**: Utilizing best practices in privacy and security to prevent data leaks, such as employing privacy-enhancing technologies and fine-grained permissions and access control mechanisms.\n",
            "\n",
            "5. **Training and Documentation**: Documenting data curation policies and connecting new data privacy policies to existing governance and compliance frameworks.\n",
            "\n",
            "By integrating these practices, organizations can better protect user data and uphold privacy rights.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key measures:\n",
            "\n",
            "1. **Bias Testing**: Companies can institute bias testing as part of their product quality assessment and launch procedures. This involves evaluating algorithms for potential biases before they are deployed, which can lead to necessary changes or even the decision not to launch a biased product.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination. This includes ensuring that systems are designed and used in an equitable manner.\n",
            "\n",
            "3. **Audits and Impact Assessments**: Organizations should establish practices for conducting audits and impact assessments to identify potential algorithmic discrimination. This helps in providing transparency to the public regarding how biases are mitigated.\n",
            "\n",
            "4. **Ongoing Monitoring**: Continuous monitoring of algorithms and their impacts is essential to ensure that they do not contribute to unjustified different treatment based on protected classifications.\n",
            "\n",
            "By integrating these practices into their operations, organizations can better safeguard against algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI systems to create both text-based disinformation and highly realistic deepfakes. These systems can produce misleading content at scale, either intentionally (disinformation) or unintentionally (misinformation), manipulating human and machine perception through subtle changes to text or images. Additionally, generative AI can assist in creating compelling imagery and propaganda that enhances the reach and engagement of disinformation campaigns on social media platforms. This capability allows malicious actors to impersonate others and disseminate fraudulent content, further eroding public trust in valid information.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "The provided context does not specify how US entities can counter the use of AI to spread misinformation during elections. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by applying explainable AI (XAI) techniques as part of ongoing continuous improvement processes. This includes methods such as analysis of embeddings, model compression/distillation, gradient-based attributions, occlusion/term reduction, counterfactual prompts, and word clouds. Additionally, documenting how pre-trained models have been adapted for specific generative tasks can help mitigate risks related to unexplainable AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, the Blueprint for an AI Bill of Rights suggests several measures, including:\n",
            "\n",
            "1. **Building Protections into Policy and Practice**: Communities, industries, and governments should implement policies that incorporate protections against potential harms from AI technologies.\n",
            "\n",
            "2. **Guiding Principles for Design and Use**: Establishing principles that guide the design, use, and deployment of automated systems to protect the public.\n",
            "\n",
            "3. **Ethical Frameworks**: Developing specific frameworks for the ethical use of AI systems, as seen in initiatives by U.S. government agencies.\n",
            "\n",
            "4. **Transparency and Public Consultation**: Ensuring ongoing transparency in AI systems and engaging with impacted communities to understand potential harms and incorporate their feedback into the design process.\n",
            "\n",
            "5. **Value Sensitive and Participatory Design**: Focusing on design processes that are sensitive to societal values and involve participation from various stakeholders.\n",
            "\n",
            "These steps can help mitigate the risks associated with AI technologies and promote their use in ways that align with societal values.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has been found to have bad environmental impacts. Specifically, emissions for large language model (LLM) inference and generative tasks, such as text summarization, are more energy- and carbon-intensive compared to non-generative tasks like text classification. While methods exist to create smaller versions of trained models to reduce environmental impacts at inference time, the training and tuning of these models can still contribute to their overall environmental footprint. Currently, there is no agreed-upon method to estimate the environmental impacts from generative AI.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, several strategies can be employed:\n",
            "\n",
            "1. **Model Distillation and Compression**: Creating smaller versions of trained models through techniques like model distillation or compression can reduce the energy and carbon intensity associated with inference tasks.\n",
            "\n",
            "2. **Assessing Environmental Impact**: It is important to assess and document the environmental impact and sustainability of AI model training and management activities. This includes evaluating the energy and resource utilization during training, fine-tuning, and inference.\n",
            "\n",
            "3. **Optimizing Resource Use**: Implementing practices that optimize the use of computational resources can help minimize the environmental footprint of AI systems.\n",
            "\n",
            "4. **Developing Agreed-upon Methods**: Establishing standardized methods to estimate the environmental impacts of generative AI (GAI) can help in understanding and mitigating these impacts effectively.\n",
            "\n",
            "By focusing on these areas, the negative environmental effects of AI can be significantly reduced.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Content Filters**: Implement content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content. These filters can be rule-based or utilize additional machine learning models to flag problematic inputs and outputs.\n",
            "\n",
            "2. **User Feedback Integration**: Evaluate user-reported problematic content and integrate this feedback into system updates to improve the AI's performance and reduce harmful outputs.\n",
            "\n",
            "3. **Monitoring and Evaluation**: Establish real-time monitoring systems to evaluate the AI-generated content against established guidelines and principles. This includes documenting training data sources to trace the origin of AI-generated content.\n",
            "\n",
            "4. **Bias Mitigation Techniques**: Use techniques such as re-sampling, re-ranking, or adversarial training to mitigate biases in the generated content.\n",
            "\n",
            "5. **Due Diligence**: Engage in due diligence to analyze AI output for harmful content, misinformation, and other sensitive material.\n",
            "\n",
            "By implementing these strategies, organizations can better manage the risks associated with generative AI and reduce the likelihood of producing harmful content.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. This bias can stem from various factors, including systemic, statistical, and human biases, as well as non-representative training data. To prevent this, it is important to address challenges related to datasets, testing and evaluation, and human factors. Implementing preliminary guidance for managing AI bias from a socio-technical perspective can also help mitigate these issues.\n"
          ]
        }
      ],
      "source": [
        "new_baseline_arctic_finetuned_retrieval_chain, new_baseline_arctic_finetuned_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Finetuned\",\n",
        "                                        embeddings=arctic_finetuned_embeddings, # <- arctic finetuned embeddings\n",
        "                                        embed_dim=arctic_finetuned_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_baseline_text_splits, # <- NEW baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to gather input from various stakeholders, including impacted communities, industry representatives, technology developers, experts from different fields, and policymakers. This input was collected through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The discussions highlighted both the potential benefits and harms of AI technologies, which played a central role in shaping the Blueprint for an AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to incorporate these protections into their policies and practices.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not provide the specific set of five principles in the AI Bill of Rights. Therefore, I don't know the answer.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "Based on the provided context, you have the right to ensure safe and effective systems through the following means:\n",
            "\n",
            "1. **Mitigation of Risks**: You have the right to mitigate, transfer, or avoid risks that exceed your organization's risk tolerances, as stated in the NIST AI Risk Management Framework.\n",
            "\n",
            "2. **Human Oversight**: The context emphasizes the importance of extensive human oversight in settings involving artificial intelligence, which suggests that you have the right to demand such oversight to ensure safety and effectiveness.\n",
            "\n",
            "These rights are aimed at promoting a safe and effective use of AI systems.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by ensuring that data is only used for its intended purpose, as highlighted in the context. For example, if data is provided to an entity like a health insurance company for payment facilitation, it should only be utilized for that specific purpose. Additionally, organizations should implement measures to mitigate, transfer, or avoid risks that exceed their risk tolerances, which can help in managing data privacy effectively. Regular information sharing, maintaining change management records, and keeping version history and metadata can also support organizations in managing data privacy and responding to incidents.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, regular information sharing, change management records, version history, and metadata can empower AI actors to respond to and manage AI incidents effectively. Additionally, implementing principles and practices outlined in frameworks like an AI Bill of Rights can help ensure that automated systems work for the benefit of the public and protect their rights.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement reasonable measures that can prevent, flag, or take other actions in response to outputs that reproduce particular training data. This includes monitoring for and addressing issues related to plagiarized, trademarked, patented, licensed content, or trade secret material. Regular information sharing, change management records, version history, and metadata can also empower AI actors in responding to and managing AI incidents effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "new_sem_arctic_original_retrieval_chain, new_sem_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_sem_text_splits, # <- NEW semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to gather input from various stakeholders, including impacted communities, industry representatives, technology developers, experts from different fields, and policymakers. This input was collected through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The feedback received played a central role in shaping the Blueprint for an AI Bill of Rights, which aims to protect the rights of the American public in the age of artificial intelligence.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to implement protections in policy, practice, and technological design.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The set of five principles in the AI Bill of Rights includes:\n",
            "\n",
            "1. Safe and Effective Systems\n",
            "2. Algorithmic Discrimination Protections\n",
            "3. Data Privacy\n",
            "4. Notice and Explanation\n",
            "5. Human Alternatives, Consideration, and Fallback\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "The formulation of the AI Bill of Rights was led by the White House Office of Science and Technology Policy (OSTP).\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Protection from Discrimination**: You should not face discrimination by algorithms, and systems should be designed and used in an equitable manner.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are required to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design to identify potential discrimination and its effects on equity.\n",
            "\n",
            "4. **Use of Representative Data**: Systems should utilize representative data and protect against proxies for demographic features to avoid biased outcomes.\n",
            "\n",
            "5. **Accessibility**: Ensuring accessibility for people with disabilities in the design and development of automated systems is essential.\n",
            "\n",
            "6. **Disparity Testing and Mitigation**: There should be pre-deployment and ongoing disparity testing and mitigation to address any identified biases.\n",
            "\n",
            "7. **Organizational Oversight**: Clear organizational oversight is necessary to ensure compliance with these protections.\n",
            "\n",
            "8. **Independent Evaluation and Reporting**: Independent evaluations and plain language reporting, including algorithmic impact assessments and disparity testing results, should be performed and made public whenever possible to confirm these protections.\n",
            "\n",
            "These rights aim to ensure that automated systems do not contribute to unjustified different treatment based on protected classifications such as race, gender, age, and disability, among others.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, as outlined in the context provided:\n",
            "\n",
            "1. **Right to Access and Correct Data**: Under the Privacy Act of 1974, you have the right to access your personal information stored in federal records systems and to correct any inaccuracies.\n",
            "\n",
            "2. **Limitations on Data Retention**: The Privacy Act requires that federal agencies only retain data that is \"relevant and necessary\" for their purposes, which helps limit the scope of data retention.\n",
            "\n",
            "3. **Local Control and Access**: Access to sensitive data should be limited based on necessity, meaning that individuals closest to the data subject (like a teacher to a students data) have more access than those who are less proximate.\n",
            "\n",
            "4. **Transparency and Reporting**: Entities that collect, use, or share sensitive data are expected to provide public reports on data security lapses, ethical reviews, and any data sold or shared, ensuring transparency in how your data is handled.\n",
            "\n",
            "5. **Legal Recourse**: If a federal agency does not comply with the Privacy Act, you have the right to seek legal relief, which may include having your data corrected or receiving monetary damages for adverse effects caused by inaccurate data.\n",
            "\n",
            "6. **Enhanced Protections for Sensitive Domains**: There are additional protections for data related to sensitive domains such as health, employment, and education, which are recognized as deserving of enhanced privacy protections.\n",
            "\n",
            "These rights are designed to help protect your personal data from unauthorized access and misuse, ensuring that you have control over your own information.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective automated systems. This includes the following specific rights:\n",
            "\n",
            "1. **Consultation**: You should be involved in the design, implementation, deployment, acquisition, and maintenance of automated systems, particularly through early-stage consultations that engage diverse communities and stakeholders.\n",
            "\n",
            "2. **Pre-deployment Testing**: Automated systems should undergo rigorous testing to identify risks and ensure they are safe and effective for their intended use.\n",
            "\n",
            "3. **Ongoing Monitoring**: There should be continuous monitoring of automated systems to demonstrate their safety and effectiveness, including the ability to mitigate unsafe outcomes.\n",
            "\n",
            "4. **Transparency and Accountability**: The outcomes of safety measures should be made public whenever possible, and independent evaluations should confirm that systems are safe and effective.\n",
            "\n",
            "5. **Protection from Harm**: Automated systems should be designed to proactively protect you from foreseeable harms, including those stemming from unintended uses.\n",
            "\n",
            "6. **Data Use Protections**: You should be safeguarded from inappropriate or irrelevant data use in the design and deployment of automated systems.\n",
            "\n",
            "7. **Possibility of Non-deployment**: There should be a possibility of not deploying a system or removing it from use if it is found to be unsafe or ineffective.\n",
            "\n",
            "These rights aim to ensure that automated systems do not endanger your safety or the safety of your community.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice regarding the use of automated systems. This includes:\n",
            "\n",
            "1. **Documentation**: The entity responsible for the automated system must provide documentation that describes how the system works in plain language, including any human components involved.\n",
            "\n",
            "2. **Accountability**: Notices should clearly identify the entities responsible for designing and using the system.\n",
            "\n",
            "3. **Timeliness**: You should be notified of the use of automated systems in advance or while being impacted by the technology, with explanations available at the time of the decision or soon thereafter.\n",
            "\n",
            "4. **Clarity**: Notices and explanations should be brief and assessed for user understanding, ensuring they are accessible to all, including individuals with disabilities.\n",
            "\n",
            "5. **Tailored Explanations**: Explanations should be tailored to the specific purpose and audience, allowing you to understand how and why decisions were made by the automated system.\n",
            "\n",
            "These rights are designed to ensure transparency and accountability in the use of AI systems, allowing you to contest decisions and understand their implications.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This includes having access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. These human alternatives should be accessible, equitable, effective, and not impose an unreasonable burden on the public. Additionally, in sensitive domains such as criminal justice, employment, education, and health, there should be tailored human oversight and consideration for high-risk decisions.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adopting Legal Frameworks**: Organizations can follow laws such as the Privacy Act of 1974, which requires limits on data retention and provides individuals the right to access and correct their data. This includes retaining only data that is \"relevant and necessary\" for specific purposes.\n",
            "\n",
            "2. **Utilizing Privacy Frameworks**: The NIST Privacy Framework offers a comprehensive approach for managing privacy risks. Organizations can use this framework to identify and communicate their privacy risks and goals, supporting ethical decision-making in the design and deployment of systems, products, and services.\n",
            "\n",
            "3. **Implementing Privacy by Design**: Organizations should integrate privacy protections into their products and services by default. This includes minimizing data collection, ensuring transparency about data use, and allowing users to control their data.\n",
            "\n",
            "4. **Enhancing User Consent Mechanisms**: Consent requests should be clear, brief, and understandable, allowing users to make informed decisions about their data. Organizations should respect user choices regarding data collection, use, access, transfer, and deletion.\n",
            "\n",
            "5. **Conducting Impact Assessments**: Before deploying surveillance technologies, organizations should conduct assessments to evaluate potential harms and ensure that privacy and civil liberties are protected.\n",
            "\n",
            "6. **Providing Transparency and Reporting**: Organizations should offer reporting mechanisms that confirm users' data decisions have been respected and assess the impact of surveillance technologies on their rights and opportunities.\n",
            "\n",
            "7. **Ensuring Compliance with Regulations**: Organizations must comply with applicable laws and regulations regarding data privacy, including reporting requirements related to surveillance and data collection practices.\n",
            "\n",
            "By implementing these practices, organizations can better protect individual privacy and foster trust with their users.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key measures:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct assessments during the design phase of automated systems to ensure they are equitable and do not perpetuate existing biases.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used to train algorithms is representative of diverse populations to avoid biased outcomes.\n",
            "\n",
            "3. **Protection Against Proxies for Demographic Features**: Implement safeguards to prevent the use of indirect indicators that could lead to discrimination based on protected characteristics.\n",
            "\n",
            "4. **Accessibility for People with Disabilities**: Design and develop systems that are accessible to individuals with disabilities, ensuring inclusivity.\n",
            "\n",
            "5. **Pre-deployment and Ongoing Disparity Testing**: Test algorithms before deployment and continuously monitor their performance to identify and mitigate any disparities in outcomes.\n",
            "\n",
            "6. **Clear Organizational Oversight**: Establish oversight mechanisms to ensure accountability in the design and deployment of automated systems.\n",
            "\n",
            "7. **Independent Evaluation and Reporting**: Conduct independent evaluations and provide plain language reporting on algorithmic impact assessments, including results from disparity testing and mitigation efforts, making this information public whenever possible.\n",
            "\n",
            "By integrating these practices, organizations can work towards minimizing the risk of algorithmic discrimination and promoting equitable outcomes.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI (GAI) systems to produce and disseminate false or misleading content at scale. These systems can create both text-based disinformation and highly realistic synthetic media, such as deepfakes, which can manipulate human and machine perception. \n",
            "\n",
            "Malicious actors can use GAI to craft targeted disinformation campaigns aimed at specific demographics, enhancing the sophistication and reach of their efforts. For instance, subtle alterations to text or images can lead to significant misinterpretations, while the ability to generate compelling imagery and propaganda can increase engagement on social media platforms. \n",
            "\n",
            "Additionally, GAI can assist in creating fraudulent content that impersonates others, further complicating the landscape of information integrity. The potential for GAI to produce content that is indistinguishable from human-generated material poses a significant risk, as it can erode public trust in valid information and evidence.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "The provided context does not specify concrete strategies for US entities to counter the use of AI to spread misinformation during elections. Therefore, I don't know how US entities can specifically counter this issue based on the given information.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by implementing several strategies, including:\n",
            "\n",
            "1. **Applying Explainable AI (XAI) Techniques**: Utilizing methods such as analysis of embeddings, model compression, gradient-based attributions, and counterfactual prompts can help in understanding and mitigating the risks associated with unexplainable generative AI systems.\n",
            "\n",
            "2. **Monitoring Pre-trained Models**: Regular monitoring and maintenance of pre-trained models used in development can help identify and address issues related to hallucinations.\n",
            "\n",
            "3. **Documenting Model Adaptations**: Keeping detailed records of how pre-trained models have been adapted for specific tasks, including any modifications made, can aid in debugging and understanding the model's behavior.\n",
            "\n",
            "4. **Evaluating Training Data**: Documenting the sources and types of training data, as well as potential biases, can help in assessing the integrity of the outputs generated by the AI system.\n",
            "\n",
            "5. **Integrating User Feedback**: Evaluating user-reported problematic content and incorporating that feedback into system updates can help improve the accuracy and reliability of the AI outputs.\n",
            "\n",
            "6. **Implementing Content Filters**: Using content filters to prevent the generation of inappropriate or false content can help mitigate the risks associated with hallucinations.\n",
            "\n",
            "7. **Real-time Monitoring**: Establishing processes for real-time monitoring of generated content can help identify deviations from desired standards and trigger alerts for human intervention when necessary.\n",
            "\n",
            "By employing these strategies, AI developers can work towards minimizing the occurrence of hallucinations in generative AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be taken:\n",
            "\n",
            "1. **Community Input**: Ensure that community input is integrated at the beginning of the design process for AI systems. This helps to identify potential harms and address them proactively.\n",
            "\n",
            "2. **Opt-Out Mechanisms**: Provide ways for individuals to opt out of AI systems and utilize human-driven alternatives when necessary.\n",
            "\n",
            "3. **Timeliness of Benefits**: Ensure that benefit payments and services provided through AI systems are timely and reliable.\n",
            "\n",
            "4. **Transparency**: Offer clear notice about the use of AI systems, including explanations of how they operate and the data they use. This transparency can help build trust and understanding among users.\n",
            "\n",
            "5. **Algorithmic Discrimination Protections**: Implement protections against algorithmic discrimination by incorporating bias testing and audits into the design and deployment of AI systems. This includes developing standards and best practices to identify and mitigate biases.\n",
            "\n",
            "6. **Public Consultation**: Engage with impacted communities to understand the potential harms of technologies and incorporate their feedback into the design and implementation of AI systems.\n",
            "\n",
            "7. **Focus on Positive Outcomes**: Use technology to enhance benefits for individuals rather than to take away support. Automated decision-making systems should aim to provide positive outcomes for users.\n",
            "\n",
            "By adopting these strategies, the risks associated with AI can be mitigated, and its benefits can be maximized for society.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The training, maintenance, and operation of generative AI systems are resource-intensive and can have large energy and environmental footprints. For instance, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive compared to non-generative tasks. While methods exist to reduce environmental impacts at inference time, the training and tuning of models still contribute to their overall environmental footprint.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, the following measures can be taken:\n",
            "\n",
            "1. **Assess Environmental Impact**: Document and assess the environmental impacts of AI model training, maintenance, and deployment. This includes evaluating energy and water consumption associated with these activities.\n",
            "\n",
            "2. **Evaluate Training Data**: Assess the proportion of synthetic to non-synthetic training data to ensure that the training data is not overly homogenous, which can mitigate concerns of model collapse.\n",
            "\n",
            "3. **Measure Resource Consumption**: Measure or estimate the environmental impacts, such as energy and carbon emissions, for training, fine-tuning, and deploying models. It's important to verify trade-offs between resources used during inference versus those required during training.\n",
            "\n",
            "4. **Carbon Capture and Offsetting**: Verify the effectiveness of carbon capture or offset programs related to AI training and applications, and address any concerns regarding green-washing.\n",
            "\n",
            "5. **Develop Smaller Models**: Explore methods for creating smaller versions of trained models, such as model distillation or compression, which could reduce environmental impacts during inference.\n",
            "\n",
            "By implementing these strategies, organizations can work towards minimizing the environmental footprint of AI systems.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, several measures can be implemented:\n",
            "\n",
            "1. **Output Restrictions**: Current systems often restrict model outputs to limit certain types of content. This involves setting up filters that prevent the generation of harmful recommendations in response to explicit prompts.\n",
            "\n",
            "2. **Prompt Management**: Addressing the issue of \"jailbreaking,\" where users manipulate prompts to bypass output controls, is crucial. This may involve developing more sophisticated prompt handling techniques to detect and mitigate such attempts.\n",
            "\n",
            "3. **User Interaction Studies**: Conducting studies to understand how end users perceive and interact with AI-generated content can help assess whether the content aligns with user expectations and how they may act upon it.\n",
            "\n",
            "4. **Bias and Stereotype Evaluation**: Evaluating potential biases and stereotypes in AI-generated content using computational testing methods and structured feedback can help identify and mitigate harmful outputs.\n",
            "\n",
            "5. **Monitoring and Response Mechanisms**: Implementing reasonable measures to prevent, flag, or take action in response to harmful outputs is essential. This includes monitoring for outputs that reproduce sensitive or harmful content.\n",
            "\n",
            "By combining these strategies, it is possible to reduce the risk of AI generating toxic or harmful content.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. The context highlights that AI systems can perpetuate and amplify harmful biases, particularly against women, racial minorities, and people with disabilities. These biases can manifest in various ways, such as underrepresentation in generated content or discriminatory decision-making based on race, gender, or other protected classes.\n",
            "\n",
            "To prevent this bias, the following measures can be taken:\n",
            "\n",
            "1. **Conduct Studies**: Understand how end users perceive and interact with AI-generated content, ensuring it aligns with their expectations and needs.\n",
            "\n",
            "2. **Evaluate Biases**: Use appropriate methodologies, including computational testing and structured feedback, to assess potential biases and stereotypes in AI-generated content.\n",
            "\n",
            "3. **Diverse Training Data**: Ensure that the training data used for AI systems is diverse and representative of different demographic groups to minimize bias.\n",
            "\n",
            "4. **Transparency and Accountability**: Implement transparent processes for AI development and decision-making, allowing for accountability in cases of bias.\n",
            "\n",
            "5. **Continuous Monitoring**: Regularly monitor AI systems for biased outcomes and make necessary adjustments to algorithms and training data.\n",
            "\n",
            "By implementing these strategies, the risk of bias in AI systems can be mitigated.\n"
          ]
        }
      ],
      "source": [
        "new_sem_arctic_finetuned_retrieval_chain, new_sem_arctic_finetuned_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Finetuned\",\n",
        "                                        embeddings=arctic_finetuned_embeddings, # <- arctic finetuned embeddings\n",
        "                                        embed_dim=arctic_finetuned_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_sem_text_splits, # <- NEW semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Summary of The Anecdotal Responses to My Questions Above\n",
        "\n",
        "Anecdotally, I see fewer `I don't know` responses with the finetuned model.  This is for both chunking strategies, compared with the original model.\n",
        "\n",
        "I also see that the finetuned model's are probably the only ones to actually articulate an answer to the question `What are the set of five principles in the AI bill of Rights?`.  Even the OpenAI model embeddings struggle to retrieve the relevant context here and as a result the most common answer is `I don't know`.  But both the finetuned pipelines, baseline chunking as well as semantic chunking, are able to answer the question well.\n",
        "\n",
        "Based on these anecdotal results, I would expect that the finetuned model performs much better than the original model in the full RAGAS evaluations below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate RAG Pipeline Using RAGAS Generated Synthetic Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:27<00:00,  2.86it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_original_results, baseline_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_baseline_arctic_original_retrieval_chain, # <- baseline chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:40<00:00,  1.96it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_finetuned_results, baseline_arctic_finetuned_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_baseline_arctic_finetuned_retrieval_chain, # <- baseline chunking + arctic finetuned embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:22<00:00,  3.57it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_original_results, sem_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_sem_arctic_original_retrieval_chain, # <- semantic chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|| 80/80 [00:39<00:00,  2.03it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_finetuned_results, sem_arctic_finetuned_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_sem_arctic_finetuned_retrieval_chain, # <- semantic chunking + arctic finetuned embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compare The Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>BaselineChunkArcticOrig</th>\n",
              "      <th>BaselineChunkArcticFinetuned</th>\n",
              "      <th>SemanticChunkArcticOrig</th>\n",
              "      <th>SemanticChunkArcticFinetuned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.709273</td>\n",
              "      <td>0.892459</td>\n",
              "      <td>0.223940</td>\n",
              "      <td>0.887186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.869910</td>\n",
              "      <td>0.968701</td>\n",
              "      <td>0.296473</td>\n",
              "      <td>0.974746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.699861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.408750</td>\n",
              "      <td>0.956736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric  BaselineChunkArcticOrig  BaselineChunkArcticFinetuned  \\\n",
              "0       faithfulness                 0.709273                      0.892459   \n",
              "1   answer_relevancy                 0.869910                      0.968701   \n",
              "2  context_precision                 0.699861                      1.000000   \n",
              "3     context_recall                 0.720000                      0.875000   \n",
              "\n",
              "   SemanticChunkArcticOrig  SemanticChunkArcticFinetuned  \n",
              "0                 0.223940                      0.887186  \n",
              "1                 0.296473                      0.974746  \n",
              "2                 0.408750                      0.956736  \n",
              "3                 0.204167                      0.916667  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_baseline_arctic_original = pd.DataFrame(list(baseline_arctic_original_results.items()), columns=['Metric', 'BaselineChunkArcticOrig'])\n",
        "df_baseline_arctic_finetuned = pd.DataFrame(list(baseline_arctic_finetuned_results.items()), columns=['Metric', 'BaselineChunkArcticFinetuned'])\n",
        "df_merged_arctic_baseline = pd.merge(df_baseline_arctic_original, df_baseline_arctic_finetuned, on='Metric')\n",
        "\n",
        "df_sem_arctic_original = pd.DataFrame(list(sem_arctic_original_results.items()), columns=['Metric', 'SemanticChunkArcticOrig'])\n",
        "df_sem_arctic_finetuned = pd.DataFrame(list(sem_arctic_finetuned_results.items()), columns=['Metric', 'SemanticChunkArcticFinetuned'])\n",
        "df_merged_arctic_sem = pd.merge(df_sem_arctic_original, df_sem_arctic_finetuned, on='Metric')\n",
        "\n",
        "df_all_merged = pd.merge(df_merged_arctic_baseline, df_merged_arctic_sem, on='Metric')\n",
        "\n",
        "df_all_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways from these results\n",
        "\n",
        "1.  Finetuning helps across the board.  It obviously starts with retrieval.  Regardless of the chunking strategy used, finetuning helps to improve retrieval-based measures like context_precision and context_recall tremendously.  The improvements are extremely significant!\n",
        "\n",
        "2.  The improvements in retrieval carry over to the generation realm.  Both measures  faithfulness and answer_relevancy  are significantly improved.  The improvements are much more stark with semantic chunking, where the original model performs particularly poorly.\n",
        "\n",
        "3.  Comparing the two finetuning results above with those of OpenAI embeddings shows that this modest amount of finetuning allows the model to achieve the same level of performance across all these measures, an incredible feat indeed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendation for the Demo App\n",
        "\n",
        "No hesitation in recommending the `finetuned_arctic` finetuned model embeddings in the RAG pipeline that is used in the demo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which one is best and why?\n",
        "-------------------------\n",
        "Overall, the finetuned_arctic model embeddings are quite good.  \n",
        "1.  The RAGAS metrics show that their performance is at about the same level as the OpenAI embeddings.  \n",
        "2.  Further, some anecdotal results on test-questions (documented n my notebook) show that the finetuned model is better able to grasp the nuances of the content of the documents in the collection.\n",
        "\n",
        "RECOMMENDATION\n",
        "--------------\n",
        "I would recommend using the finetuned-arctic model embeddings in the final version of the app.  In addition to the points above, given that the purpose of the app is to showcase the advances in AI, it makes sense to use the partially homegrown embeddings as it can be another illustration of the reach of AI and its potential.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STORY TO CEO\n",
        "------------\n",
        "\n",
        "##### Preamble\n",
        "\n",
        "In focus groups as well as in water-cooler conversations, many employees have shared that theyd like to understand how AI is evolving.  And, how we as a company can arm ourselves with the knowledge of AIs potential but also its risks.\n",
        "\n",
        "##### Rapid progress\n",
        "\n",
        "Most people, even experts in the field, were caught by surprise at the rapid progress made in the field in the past few years.  Partly due to the sheer pace of innovative work, but also due to the statistical machinery deployed in these models, we have to move thoughtfully but also rapidly to understand the potential of AI as well as its drawbacks.\n",
        "\n",
        "##### What weve done\n",
        "\n",
        "What better way to help us all understand the implications of AI than \"use AI to answer questions about AI\"?  We in Technology have worked hard to create a chatbot.  We've used a few key policy and framework proposals from the US government that this chatbot can search for a response to employees questions around the risks of AI, how to measure AI risks, and best practices to manage these risks in an enterprise setting.\n",
        "\n",
        "##### What wed like to do\n",
        "\n",
        "Wed like to roll out the chatbot to at least 50 different internal stakeholders over the next month.  Our own application, just like the recent advances in AI, are somewhat brittle.  Occasionally, the chatbot may respond with \"I don't know\".  If it does that, try a more specific variation of your question.  Our chatbot is only designed to answer questions about AIs risks, framework for measuring and managing its risks and mitigating/reducing the likelihood of adverse societal outcomes from poor management of AI tools and models.  To that end, wed like the stakeholders that we recruit to help us on this double mission: educate themselves on the risks and educate themselves on how we as a company can adopt ideas from AI into our own business units.  And, in turn, educate us all!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Incorporate New Information Into Our RAG Pipeline\n",
        "\n",
        "In this application, I used an in-memory instance of the vector database to store content from the two PDFs provided for this assignment.  If we need to add more documents with the exact same pipeline, we will literally need to re-instantiate the entire pipeline by rebuilding the vector database: (a) because the vector DB is in memory and (b) there is a single monolithic block of code that does everything from the creation of the vector indexing as well as the RAG querying.  Clearly this is not a scalable way of doing things.\n",
        "\n",
        "How we can augment this approach:\n",
        "\n",
        "a.\t*Implement persistent on-disk vector databases.*   All major vector database providers offer this capability.\n",
        "\n",
        "b.\t*Build separate pipelines to manage the process to ingest documents and other information into vector databases.*   Separate this part of the pipeline from the part that deals with querying the database (eg RAG applications).\n",
        "\n",
        "c.\t*Improve the architecture of the retrieval process itself.*  For example, if there are new versions of previously released documents, then we may need our vector DB to maintain, for audit and other reasons, the older versions of these documents.  In that case, we can put the metadata to work  e.g., identify documents by their release date, or date-added to our vector DB.  Additionally, use metadata in the retrieval process e.g., we select out more recent versions of documents to search for and phase out older documents from the search and retrieval process.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Clyykfe6xOIo"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00afceb39c074975b6b88d6d0d4d2901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9932859168ad436e9aeef09279b534b1",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_474f44771cb04a4693585273a03a5548",
            "value": 95
          }
        },
        "0267d8c4d9cc48b0a4d60d206de62a91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09c3173c05f54539ae025937b1525e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0be98b57b4894cf9a92818ae1dd72976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda4d1ec0f0043c8b4d254a4ada3e9bf",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b24ed8b36764c39aef39c92430fdc1d",
            "value": 20
          }
        },
        "126c30cc07c4452ab73fefce09dab617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1535c2c75a104f3abb262c5fb7859c14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b24ed8b36764c39aef39c92430fdc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c0f9aeab5de4e32af8bfef423a64f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0267d8c4d9cc48b0a4d60d206de62a91",
            "placeholder": "",
            "style": "IPY_MODEL_126c30cc07c4452ab73fefce09dab617",
            "value": "Generating:100%"
          }
        },
        "1e2026abc1314d3caf37d74af7a407e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32bc4bb09af4ac5a608e56f87317596",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b53095cea92740dfb967120a77310283",
            "value": 95
          }
        },
        "22c5f6324de545ba814402c3f71d84f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353b6b9a974048499d854774fe4c882c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2e50139c234d19ac3e32515e575883": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "474f44771cb04a4693585273a03a5548": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d9ba78dc78040f494df9122ddc7ba1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4e76e5d4fba404a9ed4ff059f3a0c04",
              "IPY_MODEL_1e2026abc1314d3caf37d74af7a407e7",
              "IPY_MODEL_fb306876e3244dc69312e2af46c4da02"
            ],
            "layout": "IPY_MODEL_b319ae78e30d437c81f07d5a062ba805"
          }
        },
        "63d6044414e24c5ea55efa925f7a3b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754827da55fa4240bce3710048d1645b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd96dd318c1b4e1481c039321e052081",
            "placeholder": "",
            "style": "IPY_MODEL_3b2e50139c234d19ac3e32515e575883",
            "value": "embeddingnodes:100%"
          }
        },
        "764b7b6827c9437b90c9c948b9f1037b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "771597df670f417794f66408b05a7eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ab80823e1344b638ddd1646367a6ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b61421d62964b00ba440ecba21f4b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353b6b9a974048499d854774fe4c882c",
            "max": 1248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09c3173c05f54539ae025937b1525e90",
            "value": 1248
          }
        },
        "8025a0f161d3475794daa9cd88209d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "824fe37b12d4414a9376e266ddd086f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9496fc3f26cb42ec9ace36175eb14906",
            "placeholder": "",
            "style": "IPY_MODEL_8025a0f161d3475794daa9cd88209d5c",
            "value": "Evaluating:100%"
          }
        },
        "90af75e58cef440a8d38ee6621e0f4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f2e2d3123c4cd88d7c5755342ae154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9496fc3f26cb42ec9ace36175eb14906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9932859168ad436e9aeef09279b534b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a40d4ba626f4563b062a5765325d8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c0f9aeab5de4e32af8bfef423a64f3b",
              "IPY_MODEL_0be98b57b4894cf9a92818ae1dd72976",
              "IPY_MODEL_c7550f460273484a913d211381630626"
            ],
            "layout": "IPY_MODEL_ba8f638b7f6343d9b07cce6e54e9be1c"
          }
        },
        "9ccac42dd9f04713b0ed9fe09c35b5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "b319ae78e30d437c81f07d5a062ba805": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53095cea92740dfb967120a77310283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba8f638b7f6343d9b07cce6e54e9be1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96dd318c1b4e1481c039321e052081": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bda4d1ec0f0043c8b4d254a4ada3e9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d31c6cf07143aea1bbe76aa13fbca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_754827da55fa4240bce3710048d1645b",
              "IPY_MODEL_7b61421d62964b00ba440ecba21f4b52",
              "IPY_MODEL_c67b66e1f2d34ce4b10789fc2fca5843"
            ],
            "layout": "IPY_MODEL_9ccac42dd9f04713b0ed9fe09c35b5b0"
          }
        },
        "c67b66e1f2d34ce4b10789fc2fca5843": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faa998b429774e4eb6aaaa5477bb6977",
            "placeholder": "",
            "style": "IPY_MODEL_7ab80823e1344b638ddd1646367a6ce6",
            "value": "1248/1248[07:00&lt;00:00,49.86s/it]"
          }
        },
        "c7550f460273484a913d211381630626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1535c2c75a104f3abb262c5fb7859c14",
            "placeholder": "",
            "style": "IPY_MODEL_92f2e2d3123c4cd88d7c5755342ae154",
            "value": "20/20[01:17&lt;00:00,12.75s/it]"
          }
        },
        "ce0b10aca9064bc092cf3305eb0dab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ced3689d335c4f1ca62d39b908d6cb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_824fe37b12d4414a9376e266ddd086f5",
              "IPY_MODEL_00afceb39c074975b6b88d6d0d4d2901",
              "IPY_MODEL_e8c20cb22ecb40dbaf61959fc7d087cb"
            ],
            "layout": "IPY_MODEL_771597df670f417794f66408b05a7eb9"
          }
        },
        "d020211480b149cab1761b14ae631eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e32bc4bb09af4ac5a608e56f87317596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e76e5d4fba404a9ed4ff059f3a0c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c5f6324de545ba814402c3f71d84f1",
            "placeholder": "",
            "style": "IPY_MODEL_764b7b6827c9437b90c9c948b9f1037b",
            "value": "Evaluating:100%"
          }
        },
        "e8c20cb22ecb40dbaf61959fc7d087cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90af75e58cef440a8d38ee6621e0f4d1",
            "placeholder": "",
            "style": "IPY_MODEL_ce0b10aca9064bc092cf3305eb0dab04",
            "value": "95/95[00:30&lt;00:00,1.25it/s]"
          }
        },
        "faa998b429774e4eb6aaaa5477bb6977": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb306876e3244dc69312e2af46c4da02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d020211480b149cab1761b14ae631eb1",
            "placeholder": "",
            "style": "IPY_MODEL_63d6044414e24c5ea55efa925f7a3b56",
            "value": "95/95[00:24&lt;00:00,1.20it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
