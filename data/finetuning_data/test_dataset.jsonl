{"questions": {"58c76623-8ba8-46fc-878c-ed0496e7e893": "What are some circumstances in which the application of automated systems principles may not be appropriate for government agency missions?", "c7640a6f-db1f-4caa-900e-8aa294e75af2": "Why might future sector-specific guidance be necessary for the use of automated systems in settings like school security or health diagnostics?", "e00e11fe-d95f-45ec-972e-c80d2194e65d": "What factors contribute to the difficulty in estimating risks associated with GAI?", "1edbc8f1-9b78-401a-be35-5140318eea05": "Why are speculative risks related to future GAI systems not considered in the current document?", "05244258-5fd9-42e4-aeab-507827fd00e7": "What are the key considerations for assigning liability and responsibility for incidents involving third-party data and systems in AI deployments?", "3f9645e0-c1b7-4ccf-81a8-53e03d41ef71": "How should Service Level Agreements (SLAs) be structured in vendor contracts to effectively address incident response and support availability?", "148984d4-e2db-4e3f-919f-8d52e009ea02": "What are some potential negative consequences of system use mentioned in the context?", "e1048962-f7c9-4778-a2bc-9aac8b9946c3": "Why do panelists believe that transparency alone is insufficient for achieving accountability in technology governance?", "322d9471-bc69-4fb0-9155-4f718e7bc6ac": "What are some of the mental health problems mentioned in the context that are being bred by certain practices?", "166b630e-5e72-41fe-bc3d-997bc69f0b12": "How are some companies addressing concerns related to consumer privacy according to the context?", "7243b4af-7df9-4a27-b56f-ecf0f412ce0f": "What is the primary goal of the program on Explainable Artificial Intelligence?", "899ec171-2740-4987-9dfd-906596b43708": "How does the National Science Foundation's program on Fairness in Artificial Intelligence relate to explainable AI?", "6848555a-d948-4f94-a35d-8fe31b7bc652": "How do continuous positive airway pressure machines impact patient privacy regarding their medical data and insurance coverage?", "179aab3b-cc49-4b57-8859-ac9a9784876a": "What ethical concerns arise from the use of predictive analytics by a department store to infer personal information about consumers?", "1b2e799e-d3cd-4cfe-bbd9-6b7a3ae5a421": "What are the suggested actions for evaluating and documenting AI system security and resilience as per Measure 2.7?", "975957ba-b647-49af-8c50-9fc0572ac23a": "How should GAI system security and resilience be benchmarked according to the provided context?", "418326ff-7ae1-49da-a1a9-afe447cae08d": "What is the focus of the document titled \"Intelligence, and Disability Discrimination in Hiring\" dated May 12, 2022?", "5febb330-5ff8-47c7-a6c4-e8e331cef9a1": "Who are the authors of the study \"Dissecting racial bias in an algorithm used to manage the health of populations\" published in Science in October 2019?", "2d5d25de-cde4-4af4-a95b-997a2f5b06fc": "What are the two types of prompt injection attacks mentioned in the context?", "0dd704b0-6435-4519-a5d4-77b1f0d0acad": "How can indirect prompt injection attacks exploit vulnerabilities in LLM-integrated applications?", "53123bce-0def-42f6-a795-c62e558f3e9d": "What factors can affect the accuracy of automated signature matching systems in the voting process?", "8bae5a6e-d677-4a1b-8a26-0ffdb1b6e5b1": "Why is a human curing process important in the context of mail-in ballots?", "b5132632-a1bb-400f-9d0f-ad196d5d2734": "What is the purpose of the report titled \"Study to Identify Methods to Assess Equity\" published by the Office of Management and Budget in August 2021?", "0dc8e7e3-7b4b-41ed-ba2d-0dcc505b8cd9": "When was the U.S. Department of Energy's Artificial Intelligence Advancement Council established?", "a882b70f-5c06-4236-87af-2ed9711aa971": "What are some examples of technologies that are designed to violate the safety of others?", "b4424cc2-8fe9-4dc5-a76a-b03569f236c1": "How are companies taking actions to prevent harms resulting from new technologies?", "465a58e2-ec3f-4315-ab1c-14a16dda898f": "What are some of the special requirements that govern national security and defense activities mentioned in the context?", "103355d8-dadd-444c-8158-db504cd5fdfd": "How do federal departments and agencies ensure compliance with judicial, privacy, and civil liberties oversight in relation to automated systems?", "4e43646b-423a-434d-a0dc-cef90edc01a5": "What factors should be considered when tailoring an explanation to a specific audience?", "60939948-850b-4e98-8208-6c72653b5b74": "How can user experience research be utilized to assess the effectiveness of tailored explanations?", "2747bcba-be96-4ebd-81ea-c37f7cfc643e": "What are some suggested actions for applying and documenting machine learning explanation results according to Measure 2.9?", "d2d3d24a-e083-469b-a029-80b30f38cc7e": "What key details should be documented about a GAI model to ensure responsible use and governance?", "cbb99eed-aac5-4b70-9a9b-26d93b160b98": "What are some examples of \"sensitive domains\" that require enhanced data protections?", "8e93a701-7f97-4e9d-a4d2-e545b7d8b7a5": "How do societal norms and context influence the classification of sensitive domains over time?", "c6189ced-e849-4882-875d-2cca789dea92": "What are the potential risks associated with the presence of PII or sensitive data in generated output across different media types?", "145b2152-9d2e-4c81-8e0c-dd263900ef09": "How can approaches be leveraged to detect harmful bias and homogenization in generated content?", "b44dcfe3-2cda-44a7-84d4-f9cf61c604be": "What are the expectations for automated systems intended to serve as a blueprint for?", "7b63c950-8284-49b9-b9c6-a5d2e4b43c8d": "What types of entities should be responsible for reporting on the automated systems they develop or use?", "54d4b06d-7336-4631-ad07-0340c4e0d204": "What are the key components of the deployment approval policies and procedures mentioned in the context?", "ef5398dd-0ef5-4725-8378-c193878c1cb2": "Why is it important to establish a test plan and response policy before developing highly capable models?", "7e694c4f-06aa-428b-92c8-e4c611a32b55": "What is the primary purpose of the Policy, Organization, and Priorities Act of 1976 as it relates to the Executive Office of the President?", "d625930d-79a1-4a00-ba1c-67fe5dc422bb": "How does the Office of Science and Technology Policy (OSTP) assist the Office of Management and Budget (OMB) regarding Federal research and development?", "e51f10b6-68cd-4011-8984-2f8ceabc9a76": "What are the key aspects to review and document regarding data used at different stages of the AI life cycle according to MP-2.3-002?", "2e8aba57-b12f-4d66-aa5c-a796c7df3479": "What techniques should be deployed to verify the accuracy and veracity of information generated by GAI systems as outlined in MP-2.3-003?", "4f3a89e2-f867-40d5-b21c-412aaf266134": "What is the focus of the research conducted by Glazunov et al. in their 2024 publication on Project Naptime?", "d6d15569-cee7-402d-8ae0-4977804adc3f": "How do Greshake et al. (2023) address the issue of compromising LLM-integrated applications in their study?", "390e2573-4af6-4ac6-9067-e6501204991a": "What are the different forms in which notices and explanations should be made available to meet accessibility expectations for the American public?", "7b6b878e-b69d-4806-851d-4ac47088672c": "How should explanations be tailored to serve different purposes, such as recourse or informational needs?", "34db6c24-d439-4b7f-a742-05cc7ddb458f": "How can documenting and reporting GAI incidents assist AI Actors in managing risks within the AI ecosystem?", "cc0500a4-da2f-4e49-a869-721aa7b4fb07": "What role do guidelines for incident reporting play in helping organizations prevent future GAI incidents?", "db61bc02-b476-47a5-ad4c-748c351a1ea4": "What should be considered when creating consent requests to ensure they are understandable for users?", "e675f2c1-9abf-4114-8927-461efb0d83c6": "Why is it important to avoid using \"dark patterns\" in user experience design related to consent requests?", "3c878a67-a2ed-4e71-8c82-ed8bd7dec17a": "What are the intended recipients of the reports generated by the automated system?", "240c3727-349b-4490-b20d-4bb1f29302d3": "Why is transparency in reporting expectations important for the American people?", "d2cf9d7f-3fed-4f2a-bfd7-5da8f85528e8": "What types of information can provenance metadata include regarding GAI content?", "f7d2495c-d9d7-44a8-a2fa-32c043b0d76a": "How can provenance data tracking techniques assist in assessing the authenticity and integrity of digital content?", "532f8104-4969-444f-a709-9c477e5e6e7b": "What measures should be taken to ensure that data collection is clearly communicated to individuals whose data is being collected?", "f9698d2c-fa9d-4e20-a9f9-7cf5fb74b33e": "How can user experience research help confirm that individuals understand the data collection process and its purposes?", "fd25c2a9-8fbe-44d3-b3f1-76993b2a8c76": "What types of experts are recommended for consultations regarding the potential impacts of automated systems?", "bdc5f7ce-15f6-4cf6-a383-b776c92f181b": "Why is it important for systems to undergo extensive testing before deployment?", "a3e02f6f-bdca-46cc-96af-44240f8ea623": "What are the key characteristics that the real-time monitoring processes aim to analyze in generated content?", "82fd7534-ee80-4dba-8e40-ec68e9c9a31c": "How do the models mentioned in the context help in addressing harmful bias and dangerous content?", "fcd9c1ad-6098-4720-9299-13cdd5ecf6d6": "What are the main objectives outlined by the National Institute of Standards and Technology in their 2022 publication regarding bias in artificial intelligence?", "f3d6cd1f-f99f-43f2-9af6-3797cf7192d4": "How do the OECD papers from 2023 and 2024 contribute to the understanding of accountability and incident definitions in AI?", "2425fd73-ed11-4b24-a0d3-0fe8f6cab09b": "What is the purpose of implementing a use-case based supplier risk assessment framework according to the context provided?", "7615d378-05e1-4101-a7e9-aaad238e381b": "What actions should be taken to promote content provenance when third parties make changes to organizational content?", "03ec7d16-3bd8-452f-b830-a548005e2a88": "What are the risks to privacy associated with GAI system training?", "0509969a-701e-4bda-bda9-ca1c8b0bf5c6": "How does data memorization in GAI systems potentially affect individuals' sensitive information?", "5a2df045-f605-4872-8b59-9da52f8a9dce": "What is the purpose of the Blueprint for an AI Bill of Rights as outlined in the context?", "0bc24585-8a72-4d34-8179-94706a4f78ac": "Is the Blueprint for an AI Bill of Rights considered binding guidance for the public or Federal agencies?", "bf791b9b-f5d1-4510-8774-8681564639d0": "What types of information are included in identity-related information according to the context?", "56d96e20-2cf1-4e9d-acc8-3f36333b0210": "What are the guidelines regarding the use of continuous surveillance and monitoring systems in various settings?", "90a1cf9c-813c-4c36-b7da-1e7268bcf76d": "What types of data are considered sensitive according to the context provided?", "2e340ab9-7b43-4d91-b481-1f0a17de1c51": "Why is data generated by or about individuals who are not yet legal adults classified as sensitive?", "47c81ced-6a89-474b-8827-a44a4eab7148": "What are the potential risks associated with data reuse in sensitive domains such as criminal justice and finance?", "8ac7be36-a1a7-41a6-9554-57ad9f80883e": "Under what conditions should data from sensitive domains be reused in other contexts?", "3f8594b4-b817-4044-bc42-cb3bd2c8192f": "What should be done to ensure that people impacted by an automated system are informed of significant changes?", "66672547-2527-4b16-be30-3b4c7bebefb9": "How should automated systems provide explanations for outcomes that impact individuals?", "68984396-ec0e-471f-8dbc-780a8ed6a904": "What are the key components involved in the algorithmic impact assessment process as described in the context?", "c49a28b6-5d7e-4fb8-ae2a-0d84c34198bf": "Why is it important for the reporting of algorithmic impact assessments to be clear and machine-readable?", "ab07a6cb-ddf4-47b7-8db3-2c8fe1093b33": "What policies are suggested to enhance oversight of GAI systems according to the provided context?", "06fb33e0-2ae9-426c-86e3-c190a7ef0f94": "How should organizational roles be adjusted across the lifecycle stages of large or complex GAI systems?", "360ac6ac-ec5f-4916-97ea-3096e8eb7c2e": "What is the purpose of the executive order mentioned in the context regarding advancing racial equity and support for underserved communities?", "c87e113d-e0ee-4f83-8a0d-baa17e1470fa": "Where can one find the definition of \"Navigator\" as it relates to HealthCare.gov?", "accdded6-19be-40d1-9e5d-adcd86e6f1d9": "What measures should be taken to protect individuals from abusive data practices?", "2b69a321-b409-4b41-b718-88730665848f": "How should designers and developers ensure that data collection aligns with users' reasonable expectations?", "86e08664-f461-4df6-a45f-23107864f76f": "What was the main focus of the event discussed in the context regarding technology in the healthcare system?", "c7cd6a4e-a23a-4e9c-9b58-4353e2ef9b78": "Who served as the moderator for the panel on current and emerging uses of technology in healthcare?", "81d4094c-9341-45c4-95a2-32de07502437": "What information must be included in the written description of quotas provided to employees?", "4e8ace9c-c5b3-4566-b51b-e9bce8b15d65": "What is the goal of the Defense Advanced Research Projects Agency's program on Explainable Artificial Intelligence?", "25f83546-38b4-4af2-96e3-34f1f52f5a52": "What are some reasons why certain GAI risks cannot be measured quantitatively?", "b2129d81-8154-4946-88b3-7a1a69dfc18d": "Who should be involved in the regular assessments and updates of GAI systems according to the context provided?", "c33ab4c3-294b-43ea-bbfc-426bdfd13fd9": "What techniques can be employed to mitigate representational biases in AI-generated content?", "86bd4103-3920-4d7b-ac01-d2d368491c90": "How can real-time monitoring systems ensure the effectiveness of content provenance protocols in GAI systems?", "ef0a4d9c-2df2-4696-b46c-8fbea684db06": "What must labor organizations report to the Department of Labor Office of Labor-Management Standards in connection with surveillance during a labor dispute?", "d30a855a-d172-4dfb-b787-de6a07527ea6": "How can privacy choices on smartphones be designed to ensure meaningful user agency without being overwhelming?", "0b7c6267-868b-4446-a8ce-0ad9528b00a8": "What are the environmental impacts associated with training and operating GAI systems?", "5edbdbfd-66ac-41ac-bef4-cb16032eb2a7": "How do the energy consumption and carbon emissions of generative tasks compare to those of discriminative tasks in LLM inference?", "626ecdef-80f6-4c89-9733-40874385be2d": "What are the key components mentioned for ensuring third-party transparency and risk management in GAI systems?", "2e4c4478-fc82-4a9e-9610-ff68958b095d": "How do robust TEVV processes contribute to the pre-deployment testing of GAI systems?", "070489d4-9073-48e8-af06-1d6ec028d89b": "What are some examples of professions that are underrepresented by current text-to-image models when generating images?", "fe23ff3b-6f44-4436-b5e5-e17d5925e37d": "How can harmful bias in GAI models affect individuals and communities?", "6efd79e3-ed71-4919-a11a-a554a8d67f38": "What is the title of the 2014 Federal Trade Commission report mentioned in the context?", "5e74474e-4d9a-4739-9017-d412b03f7989": "What are some potential negative effects of school surveillance of students via laptops, according to Nir Kshetri?", "322d9b7e-35d1-4227-9587-4ab7099c3f23": "What are the key capabilities and limitations of GAI systems regarding digital content transparency for AI actors and the public?", "63121d6f-b445-4ca2-abe8-7918bfeb7861": "How can structured feedback about content provenance be effectively recorded and integrated from various stakeholders, including operators and impacted communities?", "79d55432-bc90-48a8-9a3d-c28085865968": "What are the intended recipients of the reports generated by the automated system according to the expectations about reporting?", "cbce8e18-0678-4d2d-ba0e-abfa94f9390e": "Why are the reporting expectations considered important for transparency in relation to the American people?", "643db8cb-b1f6-4355-9219-66408cdcd50d": "What types of testing should systems undergo before deployment according to the context?", "606e2c01-7e0d-4aab-a8f7-43eab2fd84b9": "Why is it important for testing conditions to closely mirror the actual deployment conditions?", "3c65e039-515f-4c33-8f58-03d07407cc01": "What are the key components that should be included in the terms of use and terms of service for GAI systems?", "242b529d-42f6-4f19-bb64-45f310da9827": "How should organizations ensure that the impacts of downstream GAI systems, including third-party plugins, are documented?", "0865c0c8-06cc-4f5d-97e9-4560cc11a303": "What is Jennifer Clark's role at Ohio State University?", "f9950a37-f05d-485c-893f-e4b91c1837b1": "Who is the National Campaign Director for the Partnership for Working Families?", "aeb4b31c-7375-43cc-9188-445c9a7e68e2": "What are the characteristics of trustworthy AI that should be integrated into organizational policies and practices?", "f8acdedd-0fc5-42db-8084-86eb1d972464": "How can organizations establish transparency in documenting the origin and history of training data for GAI applications?", "fff04d51-127d-48c1-8d60-176c18e21dfb": "What are the differences in energy consumption and carbon emissions between generative and non-generative tasks in LLM inference?", "78494e2d-8233-45ba-a66f-bfeabb92a133": "What methods can be used to reduce the environmental impacts of trained models during inference time?", "dd3421c7-9a95-40a7-aa07-9702f1711f7c": "What methods are suggested for seeking feedback from affected communities regarding GAI outputs?", "5b3e0c0a-9992-455d-a546-7ddcb6dac7d0": "How can the quality and integrity of data used in training GAI systems be evaluated?", "636fbb2c-fa62-4e61-b933-e24eb0b1e2c4": "What is the primary focus of the framework mentioned in the context regarding automated system development and use?", "23810807-92c8-41ce-b833-3f3fbc027c21": "How have state and local governments responded to the issues related to automated systems in recent years?", "d56ee088-a4d2-4933-9492-3b456b436c50": "What are the potential risks associated with the ideation and design of novel harmful chemical or biological agents using AI tools?", "e30780d0-d2e1-45fb-9e7f-3a5069cc629e": "How can ongoing assessments of risks related to CBRN weapons planning be improved through monitoring AI tools?", "f348356f-6b44-449b-9211-7311d1213435": "What are the suggested actions for categorizing different types of GAI content with associated third-party rights?", "89fc08e0-939d-46d3-b388-69633a2dc19f": "How should organizations document interactions with GAI systems prior to engaging in activities that involve significant risks?", "b5c121c1-7072-4302-a8a4-c00cec23c32d": "What are the five principles outlined in the Blueprint for an AI Bill of Rights intended to protect against?", "265c0aa5-7461-4065-b2a8-6709856625ab": "How does the Technical Companion support individuals or entities involved in automated systems?", "a7c358eb-bc29-4e4e-ad16-6593e289bfc5": "What concerns do students, professors, and education experts have regarding the use of race as a predictor of student success in major universities?", "a6174aa2-bed5-4d94-81e1-b50443684d3a": "How might the use of race as a predictor impact Black students in the fields of math and science?", "0f8d8717-721d-4c55-835f-8bf932a4745a": "What are the key areas of law and regulation that GAI development and use must align with according to GV-1.1-001?", "580448d0-2c0e-409a-9fa2-32c506098182": "Who are considered AI Actors as defined by the OECD in the context of GAI risks?", "f05f94de-4e7b-4e17-b240-1d07d779e460": "What types of error rates are monitored and reported in the context provided?", "0afd2875-0efe-490c-b523-44f96de9beb0": "How are the results from independent evaluations communicated according to the reporting guidelines?", "0dc7a56b-2500-4ac2-91ae-948083c29907": "What are the suggested actions for managing GAI risks related to legal and regulatory requirements?", "59348a9f-a961-44eb-9ee4-803216da6593": "How do the AI Actor Tasks differ between AI development and AI deployment in the context of GAI risks?", "e3c157e9-9f49-48bb-b0ae-5fb93eae30aa": "What is the purpose of defining relevant groups of interest in the context of gathering structured public feedback on GAI technology?", "d5b13bdd-4e05-4785-8ad0-73e64336dc3e": "Why is it important to engage in internal and external evaluations and consultations with representative AI Actors during impact assessments?", "cd9ef25c-3705-4ebd-ae33-a316e012cafe": "What are the primary considerations derived from the GAI PWG consultation process?", "941c1458-5747-406f-bb85-ce3590ea8029": "Who were the contributors acknowledged for their analysis and contributions to the GAI PWG?", "333cb273-9e63-471c-b44f-5b8d89bc2960": "What is the definition of \"algorithmic discrimination\" as used in the provided context?", "c90ef8c2-d458-4b20-ade3-edfe25677f16": "How is an \"automated system\" characterized in relation to decision-making and data collection?", "561a4cfb-5f0a-4b8d-bebe-e517b35fbedf": "What criteria should automated systems meet to provide technically valid and meaningful explanations?", "723e1cd2-226b-4cb9-9e41-6a55f98b1e3d": "Why is it important for reporting on automated systems to be made public?", "44cd2a42-7f57-49b7-8cf2-916427f2186e": "What are the key oversight functions that need to be established for the GAI lifecycle according to the context?", "0ecb1d45-9171-439d-a2e8-31364a9f6eb7": "How should organizational teams communicate the risks and potential impacts of the AI technology they work with?", "72d2e627-641b-4c3c-97f8-70c0b5f45bab": "What is the title of the report published by the Executive Office of the President in May 2016 regarding algorithmic systems and civil rights?", "cee11532-6b13-4f41-80f6-fbd7f21e0bbc": "What concerns are raised in the shared statement by The Leadership Conference Education Fund regarding pretrial \"risk assessment\" instruments?", "b3d14d1a-ad78-44f2-964b-064180e1dacd": "What are the key characteristics that define high-integrity information according to the provided context?", "becad471-a231-48c6-b9ad-2a101e584ab7": "How does high-integrity information contribute to distinguishing fact from fiction in society?", "c5ca2388-8103-4d55-9204-f62ca362ff44": "What is the purpose of the Federal Trade Commission's guidance on using consumer reports for credit decisions?", "6f69b0b4-bdde-4ce2-be33-a5e5c74b0e60": "What actions did the Consumer Financial Protection Bureau take to protect the public from black-box credit models?", "ba1576f8-0aad-4af1-83dd-990d4a898657": "What are some of the dimensions of life that should be considered to ensure fair treatment when using automated systems?", "7e8568cc-5542-4cbd-98f4-36c80c65c040": "How did the automated system's loan underwriting model impact applicants who attended Historically Black Colleges or Universities compared to those who did not?", "a06277a5-208d-4696-93c5-cafd09116312": "What was the main issue highlighted in the article by Scott Ikeda regarding the data broker's exposure of social media profiles?", "b7815adf-bff4-4b72-ab7c-8979bba8c46d": "How did the use of facial recognition technology in public housing, as reported by Lola Fadulu, lead to public backlash?", "0c682729-dd6d-4004-932d-c4ba79b32ba0": "What types of systems are mentioned in the context that could potentially lead to algorithmic discrimination?", "d8cdaf5c-3dbf-460a-99f9-ff38278b0419": "How do surveillance and criminal justice system algorithms impact privacy according to the provided context?", "b2f06f2d-2f77-423f-b7d2-ef174dfb39cb": "What measures should be taken to ensure the protection of identifiable information (PII) in GAI applications?", "e0c2bb8a-3489-4ff3-b1de-6ece61e44439": "How can techniques like anonymization and differential privacy help minimize risks associated with AI-generated content?", "10574a1e-f5b0-4835-bca4-dc13e348fc1f": "What is the importance of domain expertise and socio-cultural awareness in AI red-teaming?", "0316425b-2470-4a9d-bedb-7a64c779bab1": "How can the involvement of general public users enhance the effectiveness of AI red-teaming exercises?", "b21b6a0f-80a2-43d5-b1d2-638addafff0d": "What is the purpose of making effectiveness public according to the context?", "3774f8b6-c21a-4871-a976-db872d3ac95a": "Where can definitions for key terms in The Blueprint for an AI Bill of Rights be found?", "8ba69c78-deb7-468f-b4aa-26b0c3fb82c3": "What are the main topics discussed by the panelists regarding the impact of new technologies on health disparities?", "8f4c3853-64c6-404d-9784-7aa0977d0ab1": "Which panelist is associated with the University of California, Berkeley School of Public Health?", "c85cc227-daee-4779-92d1-7238b73e4020": "What are the key considerations organizations must follow when collecting user feedback on AI systems in a production environment?", "d77d36c9-b575-4c29-956e-50675d1a38e7": "How does AI red-teaming contribute to identifying potential adverse impacts of AI models?", "ec7f30b1-d55f-4219-ba63-a9362817a8df": "What steps should be taken to ensure that automated systems are safe and effective before deployment?", "c5b93618-8d32-4bc3-8797-693a1ea428ae": "How can ongoing monitoring contribute to the safety and effectiveness of automated systems?", "1c15bc18-114e-4cef-9b1a-41f15432d1b5": "What are the main ethical tensions discussed in the context of human-AI companionship as explored in the work by Ciriello et al. (2024)?", "f5d17709-fd46-4c8d-afe1-1e308bde7767": "How do Dahl et al. (2024) characterize legal hallucinations in large language models in their research?", "81762243-deee-4d71-912b-9a60afd1f3a2": "What are the two criteria used in the framework to determine which automated systems are in scope?", "d4c8fccb-24fb-441d-82b2-7783b54328cf": "How does the framework propose to protect the rights and opportunities of the American public in relation to automated systems?", "0f1c739e-2351-4b3e-b4fe-62f86f3b8510": "What criteria should be used to establish the relevancy of data for automated systems?", "c155a798-bd7f-4005-8fe0-c9fc0f92560e": "Why is it important to measure and limit errors in data entry for high-quality data?", "51ba8904-daef-4609-9dbd-e1408437ddb8": "What is the purpose of ballot curing laws in the context of voter signatures?", "babfd68b-d3a0-4d85-9436-d461482d6272": "How do ballot curing processes differ among states according to the provided context?", "600dc924-8f05-4ca6-8b50-ca5b0f944844": "What is the purpose of creating measurement error models for pre-deployment metrics in the MEASURE function?", "d4216899-afe7-40cd-9e2e-8f54e4ea7f50": "In what scenarios are risk tracking approaches considered according to MEASURE 3.2?", "4466d45c-9f14-4ecd-b5ca-81962c9efdf7": "What is the main topic discussed in Mike Hughes' article regarding robots and humans?", "0c7aac7c-0034-4c9c-8dde-b07a7f82d393": "What information can be found in the National Conference of State Legislatures' table about signature cure processes?", "c3f2effc-cb10-467a-b3be-748b046f741c": "What are some practical ways proposed to reduce bias in healthcare algorithms that affect Black patients' access to medical care?", "6927d479-c3ff-4112-8ff8-1f179b7f0993": "How can employers ensure compliance with the ADA while addressing disparities in healthcare access for their employees?", "d9ca3425-4b19-4dbc-a096-a4153aba8907": "What are the potential risks associated with users disclosing mental health issues to chatbots?", "9468ccef-1177-4b3d-91fd-72d4ab95607a": "How can the generation of offensive or hateful language by AI contribute to downstream harm?", "d5693d4a-d7cf-4654-b81a-b136b8624417": "What is the purpose of using digital content transparency solutions in the context of content generation and modification?", "3e7f86f6-1eae-49ba-b788-de0a1138459f": "How can robust version control systems contribute to the AI lifecycle according to the provided context?", "5b9983db-c37b-4979-80d1-48fbef9b00db": "What are the short, mid, and long-term impacts of AI in cybersecurity as discussed by De Angelo (2024)?", "9c15bfa4-954e-4f02-868b-2da77664fb43": "How do chatbots relate to mental health and the safety of generative AI according to De Freitas et al. (2023)?", "033d7314-f204-413a-b364-7ad12390b48d": "What factors should be considered when updating or defining risk tiers for GAI according to the suggested action GV-1.3-001?", "d0fdb2b1-cae0-44f2-879a-1a43c7ff54e8": "How do the processes, procedures, and practices for risk management relate to an organization's risk tolerance?", "3fa6dc43-5ab1-4cee-a610-1c098ac967a9": "What is the focus of the Information Technology Industry Council's 2024 document on AI-generated content?", "025a0cc8-e961-425d-9ac4-febdd2c89ce8": "How do people typically react to AI failures according to the research by Jones-Jang et al. (2022)?", "b56ec30a-4970-44da-aa6e-e21943170956": "What policies and procedures should be established for the continuous monitoring of third-party GAI systems in deployment?", "b4110a18-401d-4709-b968-b2e9e7be81e9": "How can organizations address GAI data redundancy, including model weights and other system artifacts?", "268b0fc3-9c2c-442e-b365-326fd5ca484c": "What is the purpose of the handbook mentioned in the context?", "ce9b787e-bcd6-46a5-bae8-ccf22e53b93a": "How do the principles outlined in the handbook assist in the technological design process?", "ab83c0c5-fba9-418a-a647-f329cf3dd4ed": "What concerns did the National Disabled Law Students Association raise regarding remote proctoring AI systems and individuals with disabilities?", "8869dfc9-17a5-4f62-bb25-d85ed0ae4b25": "How did the algorithm designed to identify patients with high healthcare needs demonstrate racial bias in its scoring system?", "63dbe61f-0718-4430-8add-6a4d69fb9d7e": "What key needs did the panelists identify for the future design of critical AI systems?", "daa89275-3707-40b3-ae20-d5e0186a9622": "Who were the speakers at the event focused on social welfare and development?", "d1974f40-efcc-49eb-b4b4-dbdaaf3d293b": "What measures should be taken to ensure that notices and explanations are accessible to users with disabilities?", "c4c19fc6-8ddb-414d-9303-8461c9547bad": "How can user testing be utilized to assess the effectiveness of notices and explanations related to the automated system?", "2b71a6f3-26ea-4b9c-b435-806c35f201a6": "What are the potential risks associated with confabulated content in GAI when used in medical applications?", "da89376c-7d03-43cc-bc87-266199dc8ec1": "How might LLMs mislead humans into trusting their outputs despite providing incorrect answers?", "360b2a89-b15b-47fe-941c-33454971f842": "What are the potential benefits of expanding proactive protections for automated systems in relation to public safety?", "9e36f2bb-527c-4b6a-958f-836ed3bf69bf": "How can innovators be supported while ensuring the American public is protected from unsafe outcomes in automated systems?", "4b4c790f-7fd6-4bc8-acb7-66ca93683270": "What are the main techniques used in provenance data tracking for GAI systems?", "0e719164-dd48-4ede-9cbe-664758d5cefc": "How does provenance data tracking assist AI actors in managing the impacts of early-stage model decisions?", "8bdd4314-b88c-43ac-9b50-a34fae03ad0e": "What measures are suggested to protect intellectual property and trade secrets during legal discovery?", "d433a2fa-7dd1-4e29-b683-fb1c8ad9411b": "How should systems be designed to ensure meaningful access and understanding of their models?", "39bd73c8-02d6-4422-bbb6-1bfcb98507d6": "What are the potential risks associated with human-AI configuration in GAI systems?", "421a2779-0bd7-4e28-a463-41c310af9304": "How can human perspectives and experiences impact interactions with AI systems?", "7b3f26d5-401b-4037-9e27-cd3d7697b988": "What was the purpose of the increased funding by the Biden-Harris Administration in the 2022 plan year?", "00346eb9-c794-4549-aeb0-db6f7f10dcc5": "How do integrated human-AI systems benefit customer service in businesses?", "718c7753-081a-4355-b308-5e2868a8f1d4": "What techniques are mentioned for assessing and updating risk measurement approaches?", "5c155fcf-1033-4cb7-be48-deb1afe8619d": "How should policies and procedures for risk measurement be established according to the context?", "0ff00f53-6e1d-433a-90c8-b10442a8a14a": "What are the main goals of the Technical Companion in relation to automated systems?", "d25ef02e-13aa-489c-833e-d322cfffae45": "How does the Technical Companion aim to balance technological innovation with the protection of individuals?", "88f11d05-56c9-4887-932a-797ad8a08c91": "What are the key considerations for human oversight in automated systems used in sensitive domains?", "6fe88373-159e-4c71-ab48-51f00c4205e5": "How should reporting on human governance processes related to automated systems be handled according to the context provided?", "5e67df21-83ff-448b-95d8-92e942fabcef": "What are some of the key elements included in the framework developed by non-profit organizations and companies to enhance transparency in machine learning systems?", "6bab5e52-b192-4c14-9b32-9dc80a423a1c": "What federal laws require lenders to notify consumers about certain decisions made regarding their credit applications?", "6b54f21e-a6c4-45ec-ac8a-7800c122a0a5": "What are the best practices and policies suggested to reduce consumer harms related to algorithmic bias?", "794355a3-05e3-407a-9bc0-f7e91075ff28": "How does the Justice Department's new initiative aim to combat redlining?", "ff93a03a-7e66-42b4-9bb2-5e764d2d45ec": "What is the purpose of the email address ai-equity@ostp.eop.gov created by OSTP?", "bca1252b-2005-414f-9f4b-0f43ee2a60ff": "What information was OSTP seeking through the Request For Information (RFI) on biometric technologies?", "aaa5ea40-ac42-44b5-9a26-a2ad08d207da": "What mechanisms are suggested for verifying information sharing and feedback regarding the negative impacts of GAI systems?", "7839051d-ae5a-4791-a193-8f58ed300030": "How should organizations prioritize and integrate feedback from external sources concerning AI risks?", "a9718470-ee4e-4c30-8889-cf5557ea49ad": "What are some potential risks associated with the anthropomorphization of GAI systems?", "fcf7ca54-06f4-4644-a71a-67bfc287dea5": "How can lowered barriers for offensive cyber capabilities impact information security?", "dc197387-5ea8-4792-8f1c-22772cdc872e": "What challenges did the lawyer face in understanding the Medicaid eligibility decision for the older client with disabilities?", "2724db2c-fd79-4202-93c0-d8986ff7812e": "How does the lack of notice regarding data collection impact parents involved in algorithmic child maltreatment risk assessments?", "8a56ddf9-b6a5-4527-bd13-4cc719360e98": "What are the key components that should be included in the communication plans for informing AI stakeholders during the deactivation of a GAI system?", "ed34b7e1-8d2d-4e0e-a23e-ae03a34ed780": "Why is it important to establish communication plans for the disengagement process of a GAI system?", "0171f346-33ca-4599-82b2-143b29afd6a7": "What is the purpose of the framework mentioned in the context?", "765944ee-9475-48eb-a9a0-1bc5e5678f05": "How does the Blueprint for an AI Bill of Rights relate to existing U.S. laws and the Constitution?", "12016f36-54e1-4065-9e43-d54ca647a602": "What are the four ways the TSA is making flying easier for transgender people according to the ACLU article?", "438f08b0-de98-49d8-a90d-02f27d6f1c2d": "What concerns were raised by the National Disabled Law Students Association regarding the online administration of bar exams?", "f89e9f56-bc64-4d6b-856b-b43d26926a9b": "What are the potential risks associated with the eased access to CBRN information or capabilities?", "33d7a5b1-0834-48cc-888a-d483470132cd": "How does confabulation contribute to the spread of misinformation among users?", "ecc353c5-eb6d-41be-9ede-d37b88d3ee1c": "What are some of the problems that the principle of data privacy seeks to address and protect against?", "c0404ead-72de-4252-8975-f9a6dac2f11c": "How do surveillance and data collection practices impact the behavior of the American public according to the context?", "7b79f5a6-80f7-4904-84ea-94ce0db8e26a": "What types of systems are considered time-critical according to the context?", "40b9ec83-9319-470c-b864-e9d9c6c2d966": "How should the organizational structure be designed to ensure effective decision-making in time-critical systems?", "4270eb32-d316-43cb-9ca5-de94d7e28f5a": "What is the purpose of the ongoing monitoring and periodic review of the risk management process as outlined in the context?", "31ef5521-01c7-4e43-bbaf-f63f22749169": "What are the suggested actions related to organizational responsibilities for GAI systems incident monitoring?", "dd277aa4-fc66-4579-9a39-a8b87a982a8b": "What are some ways that data from one situation can be misapplied to limit people's opportunities, according to the panelists?", "8e523346-37b6-44df-9a1f-7f1948b88b0f": "How do the panelists suggest that technologies are shifting the burden of oversight from employers to workers?", "fa964056-e016-4755-b7c8-44ee0c0b70f6": "What should trigger the use of a human alternative in the attainment process when automated systems are involved?", "c136624c-1f08-4d24-bc68-764a85546000": "How should the process of opting out be designed to ensure it is timely and not burdensome?", "7c8702d8-d5d8-48db-a9ec-8f70ecf0fffb": "What mechanisms are included in the post-deployment AI system monitoring plans to capture and evaluate input from users and other relevant AI Actors?", "0257d74a-b4cb-4088-a864-e7a22e99121b": "How can organizations collaborate with external researchers and industry experts to manage identified risks associated with GAI systems?", "0ba1b306-09ff-4cdf-a516-497569d54122": "What are the main objectives outlined in the 2023 Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence?", "54ee7809-dd97-4779-963d-dfcf249c8403": "What issues related to information integrity are addressed in the 2022 Roadmap for Researchers?", "60bd4844-6f45-4dac-abcb-43cf16393bc6": "What groups are included in the equity definition for assessing potential discrimination resulting from the introduction of technology?", "024d4870-cbe7-4baf-8bc5-01a7fae5faa5": "How should the equity assessment be integrated into the consultation process for safety and efficacy reviews?", "32913b65-bce3-4020-8bb2-448bb7857e91": "What is the purpose of retaining the option to dial zero in automated call centers?", "904cf6cd-a189-44b6-8985-64e9407a4360": "How do human alternatives serve as a check on the power of automated systems in identity verification processes?", "27f95a72-bd6c-4653-be9a-a35c0b856939": "What are the key areas of concern when assessing harmful bias and content in GAI training data?", "7f0a9d6e-1327-496f-b342-414b47e4e040": "How should organizations respond when the negative risk of fine-tuned models exceeds their risk tolerance?", "2325fa4e-d3ed-45f4-9366-f234bf27982e": "What measures should be taken to review training data for CBRN information and intellectual property?", "d76931fa-dd36-4b7c-8a74-fb9d9433dc6b": "How can organizations prevent or respond to outputs that reproduce particular training data?", "b1b19fb1-16b5-429b-b720-fe132bc2bdea": "What are the suggested actions to manage GAI risks associated with third-party data and AI systems?", "e30678f3-a893-4f07-b91e-d1115a77fd76": "Why is it important to document incidents involving third-party GAI data and systems?", "09f5b3e6-e6cd-4d4a-b6e9-261db7f78c85": "What are the limitations of current pre-deployment testing approaches for GAI applications?", "08c26149-89f0-438b-82ef-340f6c667a83": "How do recommended pre-deployment testing practices aim to measure the performance and risks of GAI systems?", "2b981a06-7777-44e7-86d4-8bc31c75d12b": "What constitutes algorithmic discrimination according to the provided context?", "cf947e98-35b3-4edf-9a95-3248e2c073c8": "What measures should designers, developers, and deployers of automated systems take to prevent algorithmic discrimination?", "7043652d-6215-40bd-99c5-3617845ad562": "What is the focus of the study conducted by Andrew Wong et al. as mentioned in the context?", "5f2aba22-5e8c-46f0-8290-0f6c534c9e58": "How are Apple AirTags being misused according to the articles referenced in the context?", "0d1f52a2-1889-4456-873c-8e433b8103f0": "What measures should be taken to ensure that data collected for specific goals is not used in different contexts without assessing privacy risks?", "18c58c8f-d55a-4e1d-9c1e-83e62e62f489": "Why is it important to establish clear timelines for data retention, and what should happen to the data once those timelines are reached?", "de0e18fd-b3c9-4b2f-a1b1-890c971a7b64": "What is the main focus of the article by Yin et al. (2024) regarding OpenAI's GPT?", "7b54c6f2-f98b-44a1-9cb9-0006a6885dba": "What topic do Yu et al. (2024) explore in their research on large language models?", "7be29337-b304-4dc1-a82e-8006c5797000": "What role do strong transparency requirements play in reshaping power dynamics in smart city projects?", "58a1ac87-072b-457f-8d6d-9d54f99c9c72": "How can flexible and reactive policy development be compared to the continuous updates and deployments in software?", "19c9ab8d-afd0-4faa-b537-2d6c15274356": "What is the primary goal of NIST in relation to artificial intelligence?", "2011f606-5c9b-4f99-8885-3435f4852ab6": "How is NIST contributing to the 2023 Executive Order on Safe, Secure, and Trustworthy AI?", "d93b6b09-13e0-476a-8907-b0c440ebb99c": "What is the definition of \"dual-use foundation models\" as per EO 14110?", "2f4a4bd2-30b5-4219-a1d6-68bf9cefd1dd": "What directive does Section 4.1(a)(i)(A) of EO 14110 give to the Secretary of Commerce regarding generative AI?", "0ee2fb90-ef41-42f8-85b2-0bcee5bfbea5": "What is the title of the publication released by NIST in July 2024 regarding AI risk management?", "40d6c70c-4c9f-4583-a9a9-f42320653016": "Who is the Secretary of the U.S. Department of Commerce mentioned in the context?", "e0f4d002-0301-4c77-b674-0ead43241b67": "What is the primary purpose of AI red-teaming in the context of GAI models or systems?", "fe91e110-83ea-4d65-a8dd-16e0a93bbccd": "How does the diversity of an AI red team impact the quality of the red-teaming outputs?", "730a6ef9-4a5f-4454-82eb-a27b174bda51": "What are the potential impacts of data privacy violations related to biometric and personally identifiable information?", "e8388d13-748d-436a-9000-b567ba3c9b3d": "How can harmful bias or homogenization in AI models affect decision-making and performance across different sub-groups or languages?", "7fb6ba3f-35c1-46a3-8597-3065b2d96752": "Why is it important to document justification for each data attribute and source in a prediction process?", "640de923-46b2-4807-9146-eb66e270562a": "How can the quality and validity of predicted outcomes impact the goals of an automated system?", "338a2377-8d8e-4349-840f-aeae1851e834": "What are the key expectations for automated systems regarding data privacy?", "98dde9b5-9c32-42b9-b1bf-3a01fdbd0b4b": "Why are traditional terms of service considered inadequate for protecting privacy?", "ca898446-cd7f-4375-9d18-80b32ac42eb9": "What is the main focus of the paper by Shevlane et al. (2023) regarding model evaluation?", "950ea5ef-a7a4-4a1f-9223-ab697b034880": "How do the findings of Shumailov et al. (2023) relate to the impact of training on generated data?", "406e702a-3722-424b-8d99-4004460446dd": "What are the implications of large language models being able to strategically deceive their users under pressure, as discussed in the technical report by Scheurer et al. (2023)?", "9fb0651d-c7cc-4880-bd9e-6117dda72936": "How do the sociotechnical harms of algorithmic systems, as outlined by Shelby et al. (2023), contribute to the need for a taxonomy for harm reduction?", "9173450b-cf3b-4b56-ac43-aaa9c2366c16": "What is the purpose of the glossary of terms being developed for GAI risk management?", "745394f6-fa8f-4c55-abc8-5e766bfb30a1": "How does the AI RMF define risk in relation to GAI?", "c6ccbeba-18dc-45d0-b7b0-841f969ef981": "What methods are suggested for quantifying harms related to generated content exhibiting harmful bias?", "5dfa8b10-912f-467b-92e7-eba18b8cf7c7": "How can general fairness metrics be applied to ML pipelines or business processes that rely on GAI?", "9ff0b25a-3876-4b9a-b17f-1f13a6b4fc7b": "What are the potential consequences for individuals who are impacted by an automated system without human reconsideration?", "87292c75-3c40-4b64-b7cc-e8bc7442dbc2": "Why is it important for the American public to have the option to opt out of automated systems?", "013102ac-4d05-41ed-a13a-c82f083a386a": "What types of sensitive data are mentioned in the context that could expose individuals to meaningful harm?", "e1b7a480-d4c8-4c52-a42f-d49388d89030": "Why is data generated by or about individuals who are not yet legal adults considered sensitive?", "3738b51e-bb9e-4838-b4ba-1275e1570fb3": "What issues were identified with the proprietary model developed to predict the likelihood of sepsis in hospitalized patients?", "1f8fee84-c732-475d-a073-81cc21a2e5d6": "How have automated moderation systems on social media affected the speech of Black individuals who criticize racist messages?", "59272d70-39c4-4900-a2eb-75e9970fb604": "How does the fair use doctrine apply to GAI systems that utilize copyrighted material in their training data?", "c3fc9a5d-1321-4c56-8a01-a52be7843f14": "What are the legal implications of GAI outputs that resemble copyrighted work but do not strictly copy it?", "ad5807ff-957e-445a-8249-92db7baf8b3f": "What types of technologies are classified as \"infrastructure\" in the provided context?", "5a0d1277-601a-4883-b7c0-9e1d0bb0f703": "How are \"communities\" defined in the context of the framework?", "ef7cea96-5afd-45f0-91f8-cd310fe38363": "What are the key principles that systems should follow regarding user consent and data collection?", "fcc7b5a8-6029-4225-80df-fc86d114a9d7": "Why is it important to have enhanced protections for data related to sensitive domains such as health and education?", "2529a49e-08ba-43f4-83f7-f1a31c0f9ce6": "What role did the international community play in shaping the Blueprint for an AI Bill of Rights?", "0b8aee2e-c665-4bad-be3a-e3c0703f5d5c": "What are the core messages regarding the potential of AI technologies as discussed in the context?", "ab30f7f7-1da3-4996-981b-b334e624b75b": "How can users with disabilities access human consideration and fallback in the automated system?", "a9fbf68e-6b95-4a37-ba84-56de6733d04e": "What measures should be taken to ensure that the mechanisms for human consideration and fallback are not overly burdensome?", "992ca03b-c327-47fb-9976-af81917f46bb": "What organizations are associated with technology policy and advocacy in the provided context?", "629ec262-866a-44f1-aa1c-58412a55abf7": "Who are some of the individuals mentioned in the context, and what affiliations do they have?", "603bcf00-4dab-41d8-8ba0-b21805500b49": "What issue is highlighted in Samantha Cole's article regarding the use of Apple AirTags?", "e34f517b-451b-40ad-8048-5a38f554d5c3": "How do the findings from the articles by Kristian Lum and William Isaac relate to biases in crime prediction software?", "aae66143-1bc2-4594-acc8-7d3615728cd7": "What methods can be implemented to evaluate the decisions made by GAI systems for interpretability and explainability?", "9c4325b3-f45c-4e06-b40e-7f277038d3ae": "How can monitoring instances of human overrides on GAI decisions help in understanding content provenance issues?", "7ccb8d42-cafc-471e-a554-31afdacd3e6e": "What organizations are involved in criminal defense and risk consulting according to the context?", "b6fbb6df-5593-41d9-b6e2-7e48c0791453": "Which universities are mentioned in the context related to policy and labor studies?", "54f37cd5-2729-42c7-86ab-de93771d48de": "What are some examples of immediate risks associated with GAI systems mentioned in the context?", "2bfa8a4b-bb79-454d-8f54-9e99c6a7eeec": "How do the characteristics of a GAI model influence the presence and measurement of risks?", "06d62d50-a0ff-4cd6-b9d9-edae882b8083": "What mechanisms should be included to ensure timely and trusted access for system evaluation?", "f3fe5ae7-edb9-465c-8b8a-b2a1dcf9941b": "What information should be included in the regularly-updated reports provided by entities responsible for automated systems?", "783875d9-c94e-4a31-b999-ab6987247a13": "What are some examples of transparency artifacts that should be reviewed for third-party models?", "6c865737-4898-4396-a3aa-a6ec704beb61": "How can explainable AI (XAI) techniques contribute to the continuous improvement of generative AI systems?", "5b3a2c71-90ea-46f7-a15d-e725a8b03bc0": "What should be included in the results of any surveillance pre-deployment assessment?", "beaaba21-160a-4b49-a35f-ebd3649ae8da": "Why is it important to have an independent party assess the impact of surveillance or data collection?", "9cde1d56-efc1-4d6c-9e62-6d01b5206637": "What is the purpose of the National Artificial Intelligence Research Institutes as mentioned in the context?", "85d52f9b-8ff0-4b74-bf87-4c0eed65be82": "What funding opportunity is associated with Cyber-Physical Systems according to the National Science Foundation?", "bc161c5c-7664-4f95-a110-9e25fcc7efe7": "What were the main topics discussed in the panel on Artificial Intelligence and Democratic Values?", "2d8f4044-2f57-42e9-8a98-4cfef1cbfcf7": "Who were the key speakers and panelists involved in the event?", "43144843-20db-4591-817a-09d683920813": "What are some suggested ways to mitigate the harms caused by technology systems that reinforce inequality?", "118efe1b-1ac5-404d-85ab-62ca2ae14f2a": "How can technology be utilized to ensure that people receive benefits rather than having supports taken away from them?", "e80f1027-632e-442c-bb01-e1a660970059": "What are the key conditions under which consent should be re-acquired for data use?", "1869d7b8-9b26-48ea-ad95-0b6696fc3b5f": "How should consent requests be structured to ensure users understand their implications?", "194840b4-833a-4581-9fbd-12642995d7ec": "What types of rights and opportunities should be protected in relation to automated systems according to the framework?", "477b2c61-2654-4448-95b1-7ce704aa17a9": "How does the framework address the issue of access to critical resources or services in the context of automated systems?", "871e3096-818d-435e-849d-e814def3c940": "What types of information should be included in the reporting on human alternatives and fallback options?", "fe28741d-6a24-4790-9132-9e41245be3a2": "How often should the reporting on the accessibility and effectiveness of human consideration and fallback be made public?", "2a642e59-84aa-41ea-8d1b-6f91cdb97c7b": "What proactive measures should deployers of automated systems take to protect against algorithmic discrimination?", "86875e79-935d-4fbc-9d78-80ca2659c8af": "Why is it important to conduct independent evaluations and algorithmic impact assessments for automated systems?", "129a486e-775b-4c60-9f0f-34217cb72187": "What steps did the device manufacturer take to protect people from unwanted tracking after the release of the tracking device?", "ab21521c-24e1-452b-8189-e9e8f56ba818": "How did the algorithm used for police deployment contribute to incorrect crime predictions?", "09573468-3b35-4f5f-bc2a-9cea28494162": "What constitutes algorithmic discrimination according to the provided context?", "993d9e01-256c-4bc1-a11b-f2e3fc1d3656": "How is an \"automated system\" defined in the context of the AI Bill of Rights?", "63182b04-8156-4f2d-ae63-8d1f0aba552e": "What challenges are associated with vetting the training data for GAI systems?", "9ff6aaee-e3e0-4926-891f-169acf929d88": "How can errors in third-party GAI components affect the overall performance of a system?", "6b1bd526-09e6-426a-990c-c1f67eea2b5d": "What is the role of the Center for Democracy and Technology in relation to AI policy?", "1fac34b7-c9fd-41c3-9632-a5841230fd80": "How does the Innocence Project utilize technology in its mission to address wrongful convictions?", "1abaf8e7-7d19-400d-9e65-353f6f5a5e42": "What are some of the effects on victims of non-consensual intimate images as mentioned in the context?", "2ba0622c-0776-49d4-b1c8-12993bf8cedc": "How did the AI-powered cameras in delivery vans impact the drivers' eligibility for bonuses?", "6a1749b3-1471-410d-85f5-b919a3ec45fc": "What are the specific inventory exemptions for GAI systems as defined in organizational policies?", "6a156442-467d-4f55-b548-3d559e5680b3": "What items should be considered in GAI system inventory entries beyond general model, governance, and risk information?", "95add154-f6b3-4270-b9b5-93c6e6c025a9": "What impact does attending a Historically Black College or University (HBCU) have on the refinancing of student loans compared to those who did not attend an HBCU?", "b5d10495-d547-4e6c-8a5a-48c0e50411a2": "How did a hiring tool's design contribute to the rejection of women applicants based on their resumes?", "73967cf0-77f5-4d86-b72f-002c2699480a": "What concerns did the National Disabled Law Students Association express regarding individuals with disabilities?", "3ed7b45e-c09b-4207-acd7-920a29f99c56": "How do body scanners at airport checkpoints impact transgender travelers according to the context provided?", "4c3f4bc6-9d4d-4a62-9c7d-987fed8ee7dc": "What factors determine the level of safeguards required against human bias in automated systems?", "bcff172e-9865-4aee-8bf4-3cf5a241ed81": "How should mechanisms for human consideration and fallback be designed to ensure accessibility for users who struggle with automated systems?", "1d4ea51b-e977-42cb-a2dc-84844206879d": "What are some examples of participatory engagement methods that organizations can use to involve external stakeholders in product development?", "3e59da17-e66e-4cbe-9163-189421779b6a": "How does field testing differ from participatory engagement methods in terms of structure and focus?", "c253c1ad-6a1a-4c73-8958-45a249614d7c": "What steps are necessary to ensure that the new decision is effectively enacted in the automated system?", "f35e209c-cbef-427b-9073-b844904036eb": "How can training and assessment help combat automation bias in human-based components of a system?", "866d436b-1401-43b4-9d64-8625a39bac9d": "What action did the Biden-Harris Administration take regarding health care navigators ahead of the HealthCare.gov open enrollment period?", "510abedf-3fd8-4f17-983b-ccde09c18a4e": "What topics are covered in the sources cited in the endnotes, such as customer care and customer service solutions?", "485a46ca-2714-4a6a-9d24-984b44258ca1": "What are the key characteristics of AI risks and trustworthiness as outlined in Chapter 3 of the AI Risk Management Framework?", "8f2019c6-fc1b-472b-843f-c86026b19bf5": "How does the AI RMF Profiles section in Chapter 6 contribute to the overall understanding of AI risk management?", "9d55050e-c843-476b-a9f2-12149397ebff": "What are the key considerations for automated systems intended for use in sensitive domains?", "fbf5ea24-7d27-470b-abd5-46143e409ad6": "Why is it important to provide meaningful access for oversight in automated systems?", "2425065f-a26c-4106-98a4-208dfb25d49b": "What are the expectations for automated systems regarding data access and correction for individuals?", "ab47f30c-b75a-4dea-9425-b815d9105c7b": "How should entities handle consent withdrawal and data deletion according to the outlined expectations?", "3a0584f8-cfbf-4601-a0a8-879f58e50b51": "What is the main focus of the PAVE Interagency Task Force's Action Plan released in March 2022?", "2d40d0a7-93de-4e41-9079-675cc47f0e7a": "How do the guidelines from the U.S. Equal Employment Opportunity Commission address the use of software and algorithms in relation to the Americans with Disabilities Act?", "9421e1c5-cc74-4b09-8d4b-1e5799121bf7": "What are the main methods and considerations discussed in Solaiman et al. (2023) regarding the release of generative AI?", "3925f1c9-4080-4f81-9a7f-ab68bf56ccd1": "How does the 2023 Executive Order from The White House address the development and use of artificial intelligence?", "8e04aa74-8c04-4ebd-9e67-9792fa10f1db": "What are the potential impacts of AI-enabled \"nudification\" technology on victims, particularly women?", "093adf92-6ab8-4860-9869-42533af41d73": "How does the increasing sophistication of image-altering tools affect the detection of non-consensual intimate images?", "caeb4144-5378-4f31-b498-d4999251deb6": "What mechanisms are suggested to protect whistleblowers who report violations of laws or risks to public safety?", "ec89ef9d-7701-439f-8d61-d431c4c7347c": "What types of content are mentioned in relation to CBRN Information or Capabilities?", "17ebc621-2f67-4041-bd52-768f8b4f66d6": "What are the suggested actions for establishing interdisciplinary teams according to MAP 1.2?", "890ed485-3d96-439c-895e-3aa900d47217": "What risks are associated with human-AI configuration as mentioned in the context?", "6a98066b-ff2f-4948-90c8-ecf965491498": "What measures should be taken to ensure that surveillance is limited and proportionate to legitimate purposes?", "533d4921-9014-44eb-b07b-acbccd6947b8": "How can individuals be informed about the use of data gathered through surveillance?", "43233724-a8a7-44a5-a85a-79db8f519e33": "What methods are suggested for improving GAI system performance through human domain knowledge?", "a7c9e2be-1d95-42c2-83a3-e67495fec8bd": "How should instances of anthropomorphization in GAI system interfaces be tracked and documented?", "8f7c2b35-c50b-4431-9d29-aabf188b29c7": "What are the main purposes of AI RMF profiles in managing AI risks?", "fc978c62-3323-46fc-99ed-8b3418b5f1f9": "How does the cross-sectoral profile address risks associated with large language models (LLMs) and cloud-based services?", "b967db1b-5454-443e-a3e3-9485e602180d": "What are the key responsibilities of AI actors in relation to incident reporting for GAI systems?", "0991442d-0e56-4d93-b5ff-e7d130c5e99e": "Why is documentation and review of third-party inputs and plugins important for AI actors during incident disclosure?", "f37cbacb-d07e-40b2-9f5c-6e0e7142aa9a": "What are the suggested actions for measuring AI system performance or assurance criteria according to Measure 2.3?", "1aa61639-805d-4846-a43e-00410aaf458f": "How should claims of model capabilities be evaluated according to the provided context?", "8bf7412f-dd45-406e-96ae-2f7d59cc036b": "What measures should be determined to identify new impacts from the GAI system according to the suggested actions?", "0a464c13-6316-4cb7-a124-7f8f909fd2aa": "Who are the AI Actors that should be engaged regularly to review and evaluate unanticipated impacts of GAI systems?", "78e27949-270c-4bee-a334-9f164713113d": "What is the purpose of conducting adversarial testing in the context of GAI risks?", "3283564a-c46c-4980-a99c-8a2aac91b4f1": "How are measurement results regarding AI system trustworthiness documented and validated?", "4ea8f1cc-1867-4363-bd2a-35fe4288f9a1": "What are the specific tasks and methods defined for the AI system in MAP 2.1?", "05302944-44d3-4b1d-8720-90f856989ec2": "How does MAP 2.2 ensure that AI Actors have sufficient information to make decisions regarding the AI system's output?", "0495dadf-297c-4c56-a7d8-57e70b920e74": "What are the potential risks associated with using the same algorithm in consequential decision-making settings like employment and lending?", "6f05230b-a5bc-491a-a563-4a052b1f4b7d": "How have studies addressed the impact of AI on the workforce compared to the impact of GAI on the labor market?", "63219ac7-5825-4ff1-bae8-4f1f0e05d806": "What are the potential risks associated with confabulated outputs in statistical predictions?", "f528f43b-db33-4e30-aa53-1b7d87eb557b": "Why is it particularly important to monitor confabulated content in applications related to healthcare?", "fc494ba9-d40b-403e-a60a-0b98d99aa411": "Why is early-stage consultation important in the development of automated systems?", "7a34af57-294e-4ab2-a676-52f337759dfd": "What types of experts should be included in the consultation process for automated system development?", "e395eb6e-aba9-470a-a998-79addad23e83": "What legal relief can an individual seek if a federal agency does not comply with the Privacy Act\u2019s requirements?", "8f472cc2-6a57-4c2e-8f5d-03e53a677632": "How does NIST\u2019s Privacy Framework assist organizations in managing privacy risks?", "f72a4b0e-3705-4c45-84f4-579777e35f47": "What steps are companies and federal government agencies taking to protect the public from algorithmic discrimination?", "99c83083-2846-4f90-93a7-ce05694368c4": "How can bias testing impact the launch of products in relation to algorithmic discrimination?", "dee6c9c7-5653-4d75-a9dd-eee62dd6548b": "What are some of the principles proposed for the ethical use of AI and automated systems?", "2b1faff4-73b8-4f25-9093-ee21a49e65cd": "How does the Blueprint for an AI Bill of Rights relate to the principles outlined in Executive Order 13985?"}, "relevant_contexts": {"58c76623-8ba8-46fc-878c-ed0496e7e893": ["f4163331-41c3-44cc-8f37-215ef88b328b"], "c7640a6f-db1f-4caa-900e-8aa294e75af2": ["f4163331-41c3-44cc-8f37-215ef88b328b"], "e00e11fe-d95f-45ec-972e-c80d2194e65d": ["6d0e2aff-af8c-4231-a0d7-33e73767c307"], "1edbc8f1-9b78-401a-be35-5140318eea05": ["6d0e2aff-af8c-4231-a0d7-33e73767c307"], "05244258-5fd9-42e4-aeab-507827fd00e7": ["7bf222da-6351-416d-a0f2-32e276a0bb9d"], "3f9645e0-c1b7-4ccf-81a8-53e03d41ef71": ["7bf222da-6351-416d-a0f2-32e276a0bb9d"], "148984d4-e2db-4e3f-919f-8d52e009ea02": ["2fc822e4-1c4a-45b8-8cb6-f5101db0b817"], "e1048962-f7c9-4778-a2bc-9aac8b9946c3": ["2fc822e4-1c4a-45b8-8cb6-f5101db0b817"], "322d9471-bc69-4fb0-9155-4f718e7bc6ac": ["a5fbd50c-e73f-4de9-8e42-f6cf5c0935a3"], "166b630e-5e72-41fe-bc3d-997bc69f0b12": ["a5fbd50c-e73f-4de9-8e42-f6cf5c0935a3"], "7243b4af-7df9-4a27-b56f-ecf0f412ce0f": ["f24752e1-6838-46b4-a201-d6a184539b67"], "899ec171-2740-4987-9dfd-906596b43708": ["f24752e1-6838-46b4-a201-d6a184539b67"], "6848555a-d948-4f94-a35d-8fe31b7bc652": ["a193a6ca-e628-4aec-9a95-ebcd4b844fd7"], "179aab3b-cc49-4b57-8859-ac9a9784876a": ["a193a6ca-e628-4aec-9a95-ebcd4b844fd7"], "1b2e799e-d3cd-4cfe-bbd9-6b7a3ae5a421": ["6f993179-3a2c-4fac-ab5a-05785098b2c3"], "975957ba-b647-49af-8c50-9fc0572ac23a": ["6f993179-3a2c-4fac-ab5a-05785098b2c3"], "418326ff-7ae1-49da-a1a9-afe447cae08d": ["d3ccc1e5-5b64-4df4-b775-cc6a878f1a1d"], "5febb330-5ff8-47c7-a6c4-e8e331cef9a1": ["d3ccc1e5-5b64-4df4-b775-cc6a878f1a1d"], "2d5d25de-cde4-4af4-a95b-997a2f5b06fc": ["509a8a56-cbe6-4d60-bde5-4a450f56908e"], "0dd704b0-6435-4519-a5d4-77b1f0d0acad": ["509a8a56-cbe6-4d60-bde5-4a450f56908e"], "53123bce-0def-42f6-a795-c62e558f3e9d": ["ba7526ba-a852-4c81-8fed-f894ff1613b2"], "8bae5a6e-d677-4a1b-8a26-0ffdb1b6e5b1": ["ba7526ba-a852-4c81-8fed-f894ff1613b2"], "b5132632-a1bb-400f-9d0f-ad196d5d2734": ["cf7d8549-3bef-4142-a2f3-5e2be80c04b3"], "0dc8e7e3-7b4b-41ed-ba2d-0dcc505b8cd9": ["cf7d8549-3bef-4142-a2f3-5e2be80c04b3"], "a882b70f-5c06-4236-87af-2ed9711aa971": ["fbe7ad7a-0f08-49d7-b2e9-8c18909919c7"], "b4424cc2-8fe9-4dc5-a76a-b03569f236c1": ["fbe7ad7a-0f08-49d7-b2e9-8c18909919c7"], "465a58e2-ec3f-4315-ab1c-14a16dda898f": ["8dbd05ab-b5ee-4a14-b413-18f7553df4cd"], "103355d8-dadd-444c-8158-db504cd5fdfd": ["8dbd05ab-b5ee-4a14-b413-18f7553df4cd"], "4e43646b-423a-434d-a0dc-cef90edc01a5": ["43065ed7-0360-4abe-bba7-3ff7ae4728f5"], "60939948-850b-4e98-8208-6c72653b5b74": ["43065ed7-0360-4abe-bba7-3ff7ae4728f5"], "2747bcba-be96-4ebd-81ea-c37f7cfc643e": ["2537f8d4-61aa-4fb4-9e5c-f47a828400c9"], "d2d3d24a-e083-469b-a029-80b30f38cc7e": ["2537f8d4-61aa-4fb4-9e5c-f47a828400c9"], "cbb99eed-aac5-4b70-9a9b-26d93b160b98": ["a75501ab-ae94-4f46-85b4-0673035a7e87"], "8e93a701-7f97-4e9d-a4d2-e545b7d8b7a5": ["a75501ab-ae94-4f46-85b4-0673035a7e87"], "c6189ced-e849-4882-875d-2cca789dea92": ["ec4dabc9-7f02-476b-8b06-85be1e07e288"], "145b2152-9d2e-4c81-8e0c-dd263900ef09": ["ec4dabc9-7f02-476b-8b06-85be1e07e288"], "b44dcfe3-2cda-44a7-84d4-f9cf61c604be": ["605acb2c-1230-47de-a098-f285f493da9c"], "7b63c950-8284-49b9-b9c6-a5d2e4b43c8d": ["605acb2c-1230-47de-a098-f285f493da9c"], "54d4b06d-7336-4631-ad07-0340c4e0d204": ["ecf06de9-17a2-473c-84b0-b76e24ba17ce"], "ef5398dd-0ef5-4725-8378-c193878c1cb2": ["ecf06de9-17a2-473c-84b0-b76e24ba17ce"], "7e694c4f-06aa-428b-92c8-e4c611a32b55": ["293eaad4-2df1-447e-989b-a12cb708347e"], "d625930d-79a1-4a00-ba1c-67fe5dc422bb": ["293eaad4-2df1-447e-989b-a12cb708347e"], "e51f10b6-68cd-4011-8984-2f8ceabc9a76": ["a220b840-7e8f-43b5-b29f-16df49f5d190"], "2e8aba57-b12f-4d66-aa5c-a796c7df3479": ["a220b840-7e8f-43b5-b29f-16df49f5d190"], "4f3a89e2-f867-40d5-b21c-412aaf266134": ["6f2fae09-0b14-41a0-8329-61a0a2eba224"], "d6d15569-cee7-402d-8ae0-4977804adc3f": ["6f2fae09-0b14-41a0-8329-61a0a2eba224"], "390e2573-4af6-4ac6-9067-e6501204991a": ["afe23b73-3420-420c-b4d9-51724dff5c54"], "7b6b878e-b69d-4806-851d-4ac47088672c": ["afe23b73-3420-420c-b4d9-51724dff5c54"], "34db6c24-d439-4b7f-a742-05cc7ddb458f": ["36128849-b7d8-4307-b4ce-f2ad6fcb9c4b"], "cc0500a4-da2f-4e49-a869-721aa7b4fb07": ["36128849-b7d8-4307-b4ce-f2ad6fcb9c4b"], "db61bc02-b476-47a5-ad4c-748c351a1ea4": ["a90ea09f-0280-4ba7-95cd-9fe427439c64"], "e675f2c1-9abf-4114-8927-461efb0d83c6": ["a90ea09f-0280-4ba7-95cd-9fe427439c64"], "3c878a67-a2ed-4e71-8c82-ed8bd7dec17a": ["fe145076-3215-43ab-a02d-4712ea9ae537"], "240c3727-349b-4490-b20d-4bb1f29302d3": ["fe145076-3215-43ab-a02d-4712ea9ae537"], "d2cf9d7f-3fed-4f2a-bfd7-5da8f85528e8": ["e20b7b9a-c3e6-4c94-b713-aef9c060fb4e"], "f7d2495c-d9d7-44a8-a2fa-32c043b0d76a": ["e20b7b9a-c3e6-4c94-b713-aef9c060fb4e"], "532f8104-4969-444f-a709-9c477e5e6e7b": ["51060321-dc74-43e1-8438-075b797ee302"], "f9698d2c-fa9d-4e20-a9f9-7cf5fb74b33e": ["51060321-dc74-43e1-8438-075b797ee302"], "fd25c2a9-8fbe-44d3-b3f1-76993b2a8c76": ["39332746-bd1b-4ce7-a679-62cfe8f0c2cf"], "bdc5f7ce-15f6-4cf6-a383-b776c92f181b": ["39332746-bd1b-4ce7-a679-62cfe8f0c2cf"], "a3e02f6f-bdca-46cc-96af-44240f8ea623": ["58f5c280-0c19-4f0b-b82f-a2fccf2a0e08"], "82fd7534-ee80-4dba-8e40-ec68e9c9a31c": ["58f5c280-0c19-4f0b-b82f-a2fccf2a0e08"], "fcd9c1ad-6098-4720-9299-13cdd5ecf6d6": ["29f10452-8781-40b8-925a-745ff9910d6b"], "f3d6cd1f-f99f-43f2-9af6-3797cf7192d4": ["29f10452-8781-40b8-925a-745ff9910d6b"], "2425fd73-ed11-4b24-a0d3-0fe8f6cab09b": ["fa513ea9-7423-4147-ac3e-efb5094d1013"], "7615d378-05e1-4101-a7e9-aaad238e381b": ["fa513ea9-7423-4147-ac3e-efb5094d1013"], "03ec7d16-3bd8-452f-b830-a548005e2a88": ["5d3018d5-7978-45b3-b0de-bbbdc9038d05"], "0509969a-701e-4bda-bda9-ca1c8b0bf5c6": ["5d3018d5-7978-45b3-b0de-bbbdc9038d05"], "5a2df045-f605-4872-8b59-9da52f8a9dce": ["fa3d6a06-bc3a-47b3-b62f-53a4b6c477e3"], "0bc24585-8a72-4d34-8179-94706a4f78ac": ["fa3d6a06-bc3a-47b3-b62f-53a4b6c477e3"], "bf791b9b-f5d1-4510-8774-8681564639d0": ["bd23e28c-f261-4d6d-8327-4c8fd37ee81f"], "56d96e20-2cf1-4e9d-acc8-3f36333b0210": ["bd23e28c-f261-4d6d-8327-4c8fd37ee81f"], "90a1cf9c-813c-4c36-b7da-1e7268bcf76d": ["c787b59b-c156-428e-af9e-1f0f48a6964a"], "2e340ab9-7b43-4d91-b481-1f0a17de1c51": ["c787b59b-c156-428e-af9e-1f0f48a6964a"], "47c81ced-6a89-474b-8827-a44a4eab7148": ["a5edf076-3212-4576-86aa-7b9095a8149e"], "8ac7be36-a1a7-41a6-9554-57ad9f80883e": ["a5edf076-3212-4576-86aa-7b9095a8149e"], "3f8594b4-b817-4044-bc42-cb3bd2c8192f": ["586d6238-0439-4a27-911b-7eef2e2724cc"], "66672547-2527-4b16-be30-3b4c7bebefb9": ["586d6238-0439-4a27-911b-7eef2e2724cc"], "68984396-ec0e-471f-8dbc-780a8ed6a904": ["2e8724f0-c217-4ba0-bdd9-17f9392dc432"], "c49a28b6-5d7e-4fb8-ae2a-0d84c34198bf": ["2e8724f0-c217-4ba0-bdd9-17f9392dc432"], "ab07a6cb-ddf4-47b7-8db3-2c8fe1093b33": ["07427a9e-ff95-4acf-ad66-9634981a9d21"], "06fb33e0-2ae9-426c-86e3-c190a7ef0f94": ["07427a9e-ff95-4acf-ad66-9634981a9d21"], "360ac6ac-ec5f-4916-97ea-3096e8eb7c2e": ["e804dcc8-9c34-46e6-83c9-8689530d5606"], "c87e113d-e0ee-4f83-8a0d-baa17e1470fa": ["e804dcc8-9c34-46e6-83c9-8689530d5606"], "accdded6-19be-40d1-9e5d-adcd86e6f1d9": ["8d851dda-0d86-46f4-95d8-513aa368cc0a"], "2b69a321-b409-4b41-b718-88730665848f": ["8d851dda-0d86-46f4-95d8-513aa368cc0a"], "86e08664-f461-4df6-a45f-23107864f76f": ["c3a69573-fc01-4e87-a725-7ef714f24aff"], "c7cd6a4e-a23a-4e9c-9b58-4353e2ef9b78": ["c3a69573-fc01-4e87-a725-7ef714f24aff"], "81d4094c-9341-45c4-95a2-32de07502437": ["641ba4b1-7c45-4392-9b10-ef829ecad4fe"], "4e8ace9c-c5b3-4566-b51b-e9bce8b15d65": ["641ba4b1-7c45-4392-9b10-ef829ecad4fe"], "25f83546-38b4-4af2-96e3-34f1f52f5a52": ["280ff742-d637-40df-a320-bf44d0677965"], "b2129d81-8154-4946-88b3-7a1a69dfc18d": ["280ff742-d637-40df-a320-bf44d0677965"], "c33ab4c3-294b-43ea-bbfc-426bdfd13fd9": ["1bd2eda4-11e6-4ed2-bfb6-70b8bd2e7842"], "86bd4103-3920-4d7b-ac01-d2d368491c90": ["1bd2eda4-11e6-4ed2-bfb6-70b8bd2e7842"], "ef0a4d9c-2df2-4696-b46c-8fbea684db06": ["8d20989c-320f-40f3-bc75-fca5cad166ef"], "d30a855a-d172-4dfb-b787-de6a07527ea6": ["8d20989c-320f-40f3-bc75-fca5cad166ef"], "0b7c6267-868b-4446-a8ce-0ad9528b00a8": ["9a31121a-0079-4b8b-968b-b7706f5a5a79"], "5edbdbfd-66ac-41ac-bef4-cb16032eb2a7": ["9a31121a-0079-4b8b-968b-b7706f5a5a79"], "626ecdef-80f6-4c89-9733-40874385be2d": ["6e879ae1-5ff9-4c75-8003-126e9a5a3fd0"], "2e4c4478-fc82-4a9e-9610-ff68958b095d": ["6e879ae1-5ff9-4c75-8003-126e9a5a3fd0"], "070489d4-9073-48e8-af06-1d6ec028d89b": ["70bd0c1a-12d2-4365-b066-37eece26d188"], "fe23ff3b-6f44-4436-b5e5-e17d5925e37d": ["70bd0c1a-12d2-4365-b066-37eece26d188"], "6efd79e3-ed71-4919-a11a-a554a8d67f38": ["de8d503b-f8b1-403d-878d-16bd4c2bc55b"], "5e74474e-4d9a-4739-9017-d412b03f7989": ["de8d503b-f8b1-403d-878d-16bd4c2bc55b"], "322d9b7e-35d1-4227-9587-4ab7099c3f23": ["413255ec-385d-4c38-b4a0-a8cd7d03c126"], "63121d6f-b445-4ca2-abe8-7918bfeb7861": ["413255ec-385d-4c38-b4a0-a8cd7d03c126"], "79d55432-bc90-48a8-9a3d-c28085865968": ["207847f9-df7a-4af1-9aca-c965676a8034"], "cbce8e18-0678-4d2d-ba0e-abfa94f9390e": ["207847f9-df7a-4af1-9aca-c965676a8034"], "643db8cb-b1f6-4355-9219-66408cdcd50d": ["4ee8929e-d1fe-493c-a5ef-f2838fa36ea4"], "606e2c01-7e0d-4aab-a8f7-43eab2fd84b9": ["4ee8929e-d1fe-493c-a5ef-f2838fa36ea4"], "3c65e039-515f-4c33-8f58-03d07407cc01": ["fe11fcc6-d36c-42bb-9fce-8dc62dc6ad64"], "242b529d-42f6-4f19-bb64-45f310da9827": ["fe11fcc6-d36c-42bb-9fce-8dc62dc6ad64"], "0865c0c8-06cc-4f5d-97e9-4560cc11a303": ["fd75683b-70f4-45f4-bcce-64da93feced9"], "f9950a37-f05d-485c-893f-e4b91c1837b1": ["fd75683b-70f4-45f4-bcce-64da93feced9"], "aeb4b31c-7375-43cc-9188-445c9a7e68e2": ["cf235324-9f77-4de2-8ec4-d8423819499a"], "f8acdedd-0fc5-42db-8084-86eb1d972464": ["cf235324-9f77-4de2-8ec4-d8423819499a"], "fff04d51-127d-48c1-8d60-176c18e21dfb": ["346cc623-448b-40c8-aa73-657d6c75adfb"], "78494e2d-8233-45ba-a66f-bfeabb92a133": ["346cc623-448b-40c8-aa73-657d6c75adfb"], "dd3421c7-9a95-40a7-aa07-9702f1711f7c": ["97935876-96f4-41ea-80fc-8a7a9c2ac7b5"], "5b3e0c0a-9992-455d-a546-7ddcb6dac7d0": ["97935876-96f4-41ea-80fc-8a7a9c2ac7b5"], "636fbb2c-fa62-4e61-b933-e24eb0b1e2c4": ["387ad6e8-23b6-4ce6-9cde-cb2521d5aaf2"], "23810807-92c8-41ce-b833-3f3fbc027c21": ["387ad6e8-23b6-4ce6-9cde-cb2521d5aaf2"], "d56ee088-a4d2-4933-9492-3b456b436c50": ["154dc934-1306-4889-9480-007b039c9eea"], "e30780d0-d2e1-45fb-9e7f-3a5069cc629e": ["154dc934-1306-4889-9480-007b039c9eea"], "f348356f-6b44-449b-9211-7311d1213435": ["caff7dcf-9dab-4f12-bd2e-9b5d2ae3b3e4"], "89fc08e0-939d-46d3-b388-69633a2dc19f": ["caff7dcf-9dab-4f12-bd2e-9b5d2ae3b3e4"], "b5c121c1-7072-4302-a8a4-c00cec23c32d": ["9ba2e19d-4746-40b3-99f5-3f19866e4ea9"], "265c0aa5-7461-4065-b2a8-6709856625ab": ["9ba2e19d-4746-40b3-99f5-3f19866e4ea9"], "a7c358eb-bc29-4e4e-ad16-6593e289bfc5": ["3391ae93-5428-4257-b288-9618026301c1"], "a6174aa2-bed5-4d94-81e1-b50443684d3a": ["3391ae93-5428-4257-b288-9618026301c1"], "0f8d8717-721d-4c55-835f-8bf932a4745a": ["df0b5bde-fa5f-481b-b78b-39b3278d9b8c"], "580448d0-2c0e-409a-9fa2-32c506098182": ["df0b5bde-fa5f-481b-b78b-39b3278d9b8c"], "f05f94de-4e7b-4e17-b240-1d07d779e460": ["f545635d-4965-40fc-a76b-eeffe6071572"], "0afd2875-0efe-490c-b523-44f96de9beb0": ["f545635d-4965-40fc-a76b-eeffe6071572"], "0dc7a56b-2500-4ac2-91ae-948083c29907": ["6aceaff5-a98f-4f75-a77d-31db510abc00"], "59348a9f-a961-44eb-9ee4-803216da6593": ["6aceaff5-a98f-4f75-a77d-31db510abc00"], "e3c157e9-9f49-48bb-b0ae-5fb93eae30aa": ["5900ea84-9228-4be0-84d2-e6db99c066ca"], "d5b13bdd-4e05-4785-8ad0-73e64336dc3e": ["5900ea84-9228-4be0-84d2-e6db99c066ca"], "cd9ef25c-3705-4ebd-ae33-a316e012cafe": ["f8ca5d13-11ae-4365-addf-efc69e24cf1e"], "941c1458-5747-406f-bb85-ce3590ea8029": ["f8ca5d13-11ae-4365-addf-efc69e24cf1e"], "333cb273-9e63-471c-b44f-5b8d89bc2960": ["0a7cd993-1129-47e1-ad83-0e5f80519812"], "c90ef8c2-d458-4b20-ade3-edfe25677f16": ["0a7cd993-1129-47e1-ad83-0e5f80519812"], "561a4cfb-5f0a-4b8d-bebe-e517b35fbedf": ["a24a5ca9-e065-430e-9dae-cedb96e6df30"], "723e1cd2-226b-4cb9-9e41-6a55f98b1e3d": ["a24a5ca9-e065-430e-9dae-cedb96e6df30"], "44cd2a42-7f57-49b7-8cf2-916427f2186e": ["06d411ca-fb6d-4e32-9320-9bc6d2c06048"], "0ecb1d45-9171-439d-a2e8-31364a9f6eb7": ["06d411ca-fb6d-4e32-9320-9bc6d2c06048"], "72d2e627-641b-4c3c-97f8-70c0b5f45bab": ["bc07b9e9-3f25-4539-a7da-a7839e0f48d3"], "cee11532-6b13-4f41-80f6-fbd7f21e0bbc": ["bc07b9e9-3f25-4539-a7da-a7839e0f48d3"], "b3d14d1a-ad78-44f2-964b-064180e1dacd": ["0f74ef24-2e50-40b4-a6ba-574538c53e40"], "becad471-a231-48c6-b9ad-2a101e584ab7": ["0f74ef24-2e50-40b4-a6ba-574538c53e40"], "c5ca2388-8103-4d55-9204-f62ca362ff44": ["b003ea70-13e5-4690-8143-f4789ceb0753"], "6f69b0b4-bdde-4ce2-be33-a5e5c74b0e60": ["b003ea70-13e5-4690-8143-f4789ceb0753"], "ba1576f8-0aad-4af1-83dd-990d4a898657": ["166837f6-7c33-474c-b181-1eacd0e2b333"], "7e8568cc-5542-4cbd-98f4-36c80c65c040": ["166837f6-7c33-474c-b181-1eacd0e2b333"], "a06277a5-208d-4696-93c5-cafd09116312": ["b4319c4f-c8cb-4cf8-b82a-ce1dc3b6674f"], "b7815adf-bff4-4b72-ab7c-8979bba8c46d": ["b4319c4f-c8cb-4cf8-b82a-ce1dc3b6674f"], "0c682729-dd6d-4004-932d-c4ba79b32ba0": ["76cc6c42-482a-400c-b774-5f3e35cb6e43"], "d8cdaf5c-3dbf-460a-99f9-ff38278b0419": ["76cc6c42-482a-400c-b774-5f3e35cb6e43"], "b2f06f2d-2f77-423f-b7d2-ef174dfb39cb": ["5d4fc094-05d4-43a8-8d0c-69c5d75def5f"], "e0c2bb8a-3489-4ff3-b1de-6ece61e44439": ["5d4fc094-05d4-43a8-8d0c-69c5d75def5f"], "10574a1e-f5b0-4835-bca4-dc13e348fc1f": ["7206bfee-b9f3-4fa8-aadc-6a42946235aa"], "0316425b-2470-4a9d-bedb-7a64c779bab1": ["7206bfee-b9f3-4fa8-aadc-6a42946235aa"], "b21b6a0f-80a2-43d5-b1d2-638addafff0d": ["360643f0-1054-4e87-b8e6-16eaec2c4656"], "3774f8b6-c21a-4871-a976-db872d3ac95a": ["360643f0-1054-4e87-b8e6-16eaec2c4656"], "8ba69c78-deb7-468f-b4aa-26b0c3fb82c3": ["d7e77db6-97e6-4bac-a8b6-f8cd9b34e4c3"], "8f4c3853-64c6-404d-9784-7aa0977d0ab1": ["d7e77db6-97e6-4bac-a8b6-f8cd9b34e4c3"], "c85cc227-daee-4779-92d1-7238b73e4020": ["c3fa4d00-1477-4d58-ba2a-aa4d63da2874"], "d77d36c9-b575-4c29-956e-50675d1a38e7": ["c3fa4d00-1477-4d58-ba2a-aa4d63da2874"], "ec7f30b1-d55f-4219-ba63-a9362817a8df": ["2e893474-517e-42f5-9396-c5fe13350e3c"], "c5b93618-8d32-4bc3-8797-693a1ea428ae": ["2e893474-517e-42f5-9396-c5fe13350e3c"], "1c15bc18-114e-4cef-9b1a-41f15432d1b5": ["fee1b245-5983-4196-9032-5872c7f1aa1b"], "f5d17709-fd46-4c8d-afe1-1e308bde7767": ["fee1b245-5983-4196-9032-5872c7f1aa1b"], "81762243-deee-4d71-912b-9a60afd1f3a2": ["6a55e406-8570-4cc4-b9aa-cdc3cb8582a7"], "d4c8fccb-24fb-441d-82b2-7783b54328cf": ["6a55e406-8570-4cc4-b9aa-cdc3cb8582a7"], "0f1c739e-2351-4b3e-b4fe-62f86f3b8510": ["750f05fd-2595-4d49-8575-a01f481a154a"], "c155a798-bd7f-4005-8fe0-c9fc0f92560e": ["750f05fd-2595-4d49-8575-a01f481a154a"], "51ba8904-daef-4609-9dbd-e1408437ddb8": ["6858f000-a5e0-4dde-a4ad-cee9e9542fac"], "babfd68b-d3a0-4d85-9436-d461482d6272": ["6858f000-a5e0-4dde-a4ad-cee9e9542fac"], "600dc924-8f05-4ca6-8b50-ca5b0f944844": ["502bb66e-388d-47a7-91e9-46b3e2a55821"], "d4216899-afe7-40cd-9e2e-8f54e4ea7f50": ["502bb66e-388d-47a7-91e9-46b3e2a55821"], "4466d45c-9f14-4ecd-b5ca-81962c9efdf7": ["021aca5f-0cb1-467f-9604-2f8f144b92f6"], "0c7aac7c-0034-4c9c-8dde-b07a7f82d393": ["021aca5f-0cb1-467f-9604-2f8f144b92f6"], "c3f2effc-cb10-467a-b3be-748b046f741c": ["c0da6c6b-360a-4eaf-8328-b44ac0df7031"], "6927d479-c3ff-4112-8ff8-1f179b7f0993": ["c0da6c6b-360a-4eaf-8328-b44ac0df7031"], "d9ca3425-4b19-4dbc-a096-a4153aba8907": ["1d4207c7-9112-42df-b75b-4d3d1d4c7ebf"], "9468ccef-1177-4b3d-91fd-72d4ab95607a": ["1d4207c7-9112-42df-b75b-4d3d1d4c7ebf"], "d5693d4a-d7cf-4654-b81a-b136b8624417": ["6357e678-7a6c-4626-9522-1afff995587c"], "3e7f86f6-1eae-49ba-b788-de0a1138459f": ["6357e678-7a6c-4626-9522-1afff995587c"], "5b9983db-c37b-4979-80d1-48fbef9b00db": ["d69a269b-4ac6-4c34-9206-6f0c8687f8da"], "9c15bfa4-954e-4f02-868b-2da77664fb43": ["d69a269b-4ac6-4c34-9206-6f0c8687f8da"], "033d7314-f204-413a-b364-7ad12390b48d": ["025b0f84-4bda-439a-9539-cc2b7bbb3dc0"], "d0fdb2b1-cae0-44f2-879a-1a43c7ff54e8": ["025b0f84-4bda-439a-9539-cc2b7bbb3dc0"], "3fa6dc43-5ab1-4cee-a610-1c098ac967a9": ["4a35a5e6-0a1d-4d9c-9842-2e5577501b3f"], "025a0cc8-e961-425d-9ac4-febdd2c89ce8": ["4a35a5e6-0a1d-4d9c-9842-2e5577501b3f"], "b56ec30a-4970-44da-aa6e-e21943170956": ["76618734-7341-481d-89aa-5ee17efbdffc"], "b4110a18-401d-4709-b968-b2e9e7be81e9": ["76618734-7341-481d-89aa-5ee17efbdffc"], "268b0fc3-9c2c-442e-b365-326fd5ca484c": ["04f91fb1-a85c-4185-8d50-4d112ba275ba"], "ce9b787e-bcd6-46a5-bae8-ccf22e53b93a": ["04f91fb1-a85c-4185-8d50-4d112ba275ba"], "ab83c0c5-fba9-418a-a647-f329cf3dd4ed": ["886d66dd-2bea-4c11-b7e7-38e5dd45eff1"], "8869dfc9-17a5-4f62-bb25-d85ed0ae4b25": ["886d66dd-2bea-4c11-b7e7-38e5dd45eff1"], "63dbe61f-0718-4430-8add-6a4d69fb9d7e": ["04b0d553-c4a7-4ecc-b58d-39204c37f0bb"], "daa89275-3707-40b3-ae20-d5e0186a9622": ["04b0d553-c4a7-4ecc-b58d-39204c37f0bb"], "d1974f40-efcc-49eb-b4b4-dbdaaf3d293b": ["868564eb-83b7-4d7a-92f8-c81a68e3e492"], "c4c19fc6-8ddb-414d-9303-8461c9547bad": ["868564eb-83b7-4d7a-92f8-c81a68e3e492"], "2b71a6f3-26ea-4b9c-b435-806c35f201a6": ["a92b242a-d93e-402a-8a75-98d71fff252e"], "da89376c-7d03-43cc-bc87-266199dc8ec1": ["a92b242a-d93e-402a-8a75-98d71fff252e"], "360b2a89-b15b-47fe-941c-33454971f842": ["5d049b4e-1464-406c-bb1f-9f703f4b2a15"], "9e36f2bb-527c-4b6a-958f-836ed3bf69bf": ["5d049b4e-1464-406c-bb1f-9f703f4b2a15"], "4b4c790f-7fd6-4bc8-acb7-66ca93683270": ["04ae0eca-8e5c-4ff0-b76e-cc2fd828b0d8"], "0e719164-dd48-4ede-9cbe-664758d5cefc": ["04ae0eca-8e5c-4ff0-b76e-cc2fd828b0d8"], "8bdd4314-b88c-43ac-9b50-a34fae03ad0e": ["91078e11-ca6a-448f-82cd-bf6d8aa29a22"], "d433a2fa-7dd1-4e29-b683-fb1c8ad9411b": ["91078e11-ca6a-448f-82cd-bf6d8aa29a22"], "39bd73c8-02d6-4422-bbb6-1bfcb98507d6": ["48632257-c296-4742-8721-b02ea821234f"], "421a2779-0bd7-4e28-a463-41c310af9304": ["48632257-c296-4742-8721-b02ea821234f"], "7b3f26d5-401b-4037-9e27-cd3d7697b988": ["016a132a-10f7-42d3-ae2b-1d54ff9a949a"], "00346eb9-c794-4549-aeb0-db6f7f10dcc5": ["016a132a-10f7-42d3-ae2b-1d54ff9a949a"], "718c7753-081a-4355-b308-5e2868a8f1d4": ["b0332b22-b3f6-4b6a-84ac-da01dc01e83a"], "5c155fcf-1033-4cb7-be48-deb1afe8619d": ["b0332b22-b3f6-4b6a-84ac-da01dc01e83a"], "0ff00f53-6e1d-433a-90c8-b10442a8a14a": ["b61b34cd-005d-42c3-9759-133ef46ad79d"], "d25ef02e-13aa-489c-833e-d322cfffae45": ["b61b34cd-005d-42c3-9759-133ef46ad79d"], "88f11d05-56c9-4887-932a-797ad8a08c91": ["c7db751b-67d3-43c5-8a49-8889cdb2f77d"], "6fe88373-159e-4c71-ab48-51f00c4205e5": ["c7db751b-67d3-43c5-8a49-8889cdb2f77d"], "5e67df21-83ff-448b-95d8-92e942fabcef": ["7bc11834-b5d6-4fbf-9ea1-0de1c6240a04"], "6bab5e52-b192-4c14-9b32-9dc80a423a1c": ["7bc11834-b5d6-4fbf-9ea1-0de1c6240a04"], "6b54f21e-a6c4-45ec-ac8a-7800c122a0a5": ["85bdf9b8-2dff-42b0-8d54-c3607ecce62e"], "794355a3-05e3-407a-9bc0-f7e91075ff28": ["85bdf9b8-2dff-42b0-8d54-c3607ecce62e"], "ff93a03a-7e66-42b4-9bb2-5e764d2d45ec": ["111970aa-c1b6-4e0a-8c55-eda7b544b561"], "bca1252b-2005-414f-9f4b-0f43ee2a60ff": ["111970aa-c1b6-4e0a-8c55-eda7b544b561"], "aaa5ea40-ac42-44b5-9a26-a2ad08d207da": ["431d173e-91a7-4b23-b7e8-914341ca20bc"], "7839051d-ae5a-4791-a193-8f58ed300030": ["431d173e-91a7-4b23-b7e8-914341ca20bc"], "a9718470-ee4e-4c30-8889-cf5557ea49ad": ["6649095b-32ea-40e3-b58e-4eb92e592eac"], "fcf7ca54-06f4-4644-a71a-67bfc287dea5": ["6649095b-32ea-40e3-b58e-4eb92e592eac"], "dc197387-5ea8-4792-8f1c-22772cdc872e": ["cf3b84ab-8a50-4440-afe0-965069050695"], "2724db2c-fd79-4202-93c0-d8986ff7812e": ["cf3b84ab-8a50-4440-afe0-965069050695"], "8a56ddf9-b6a5-4527-bd13-4cc719360e98": ["59d73582-e851-4755-9688-b42d10c56fb9"], "ed34b7e1-8d2d-4e0e-a23e-ae03a34ed780": ["59d73582-e851-4755-9688-b42d10c56fb9"], "0171f346-33ca-4599-82b2-143b29afd6a7": ["c0ae6e03-665d-4091-9523-1239ee306932"], "765944ee-9475-48eb-a9a0-1bc5e5678f05": ["c0ae6e03-665d-4091-9523-1239ee306932"], "12016f36-54e1-4065-9e43-d54ca647a602": ["fe0c0e69-a7e4-44a4-9f1b-3fe04099a55d"], "438f08b0-de98-49d8-a90d-02f27d6f1c2d": ["fe0c0e69-a7e4-44a4-9f1b-3fe04099a55d"], "f89e9f56-bc64-4d6b-856b-b43d26926a9b": ["10f2bffa-8222-4ffb-816e-1e66d4e6795d"], "33d7a5b1-0834-48cc-888a-d483470132cd": ["10f2bffa-8222-4ffb-816e-1e66d4e6795d"], "ecc353c5-eb6d-41be-9ede-d37b88d3ee1c": ["4dcaa294-ebe7-4fc2-8e41-df0ee1110653"], "c0404ead-72de-4252-8975-f9a6dac2f11c": ["4dcaa294-ebe7-4fc2-8e41-df0ee1110653"], "7b79f5a6-80f7-4904-84ea-94ce0db8e26a": ["4efd92be-ce2a-43e7-a249-4cd926123b60"], "40b9ec83-9319-470c-b864-e9d9c6c2d966": ["4efd92be-ce2a-43e7-a249-4cd926123b60"], "4270eb32-d316-43cb-9ca5-de94d7e28f5a": ["e185fa01-5783-411d-a733-7fdbbe1cac24"], "31ef5521-01c7-4e43-bbaf-f63f22749169": ["e185fa01-5783-411d-a733-7fdbbe1cac24"], "dd277aa4-fc66-4579-9a39-a8b87a982a8b": ["91713c6c-1e14-429f-8ac4-27231f0954e4"], "8e523346-37b6-44df-9a1f-7f1948b88b0f": ["91713c6c-1e14-429f-8ac4-27231f0954e4"], "fa964056-e016-4755-b7c8-44ee0c0b70f6": ["9fe3429b-bc37-43a6-b8e0-fbd4697e2491"], "c136624c-1f08-4d24-bc68-764a85546000": ["9fe3429b-bc37-43a6-b8e0-fbd4697e2491"], "7c8702d8-d5d8-48db-a9ec-8f70ecf0fffb": ["110af4fc-3653-4469-ae10-bde7df1cb21b"], "0257d74a-b4cb-4088-a864-e7a22e99121b": ["110af4fc-3653-4469-ae10-bde7df1cb21b"], "0ba1b306-09ff-4cdf-a516-497569d54122": ["d9385984-3bf2-4263-95d8-2435b19faa5a"], "54ee7809-dd97-4779-963d-dfcf249c8403": ["d9385984-3bf2-4263-95d8-2435b19faa5a"], "60bd4844-6f45-4dac-abcb-43cf16393bc6": ["81e66c8f-0bc8-45fb-8372-ef88bfceedb0"], "024d4870-cbe7-4baf-8bc5-01a7fae5faa5": ["81e66c8f-0bc8-45fb-8372-ef88bfceedb0"], "32913b65-bce3-4020-8bb2-448bb7857e91": ["1233349c-2328-4900-baac-ba5f4f6609d7"], "904cf6cd-a189-44b6-8985-64e9407a4360": ["1233349c-2328-4900-baac-ba5f4f6609d7"], "27f95a72-bd6c-4653-be9a-a35c0b856939": ["a6cd5869-184b-41d9-877f-e38268815402"], "7f0a9d6e-1327-496f-b342-414b47e4e040": ["a6cd5869-184b-41d9-877f-e38268815402"], "2325fa4e-d3ed-45f4-9366-f234bf27982e": ["682826c7-6b85-41f2-a7a6-92d00cb43934"], "d76931fa-dd36-4b7c-8a74-fb9d9433dc6b": ["682826c7-6b85-41f2-a7a6-92d00cb43934"], "b1b19fb1-16b5-429b-b720-fe132bc2bdea": ["d86bc87b-31bc-4c02-9bb6-459a5abae8a8"], "e30678f3-a893-4f07-b91e-d1115a77fd76": ["d86bc87b-31bc-4c02-9bb6-459a5abae8a8"], "09f5b3e6-e6cd-4d4a-b6e9-261db7f78c85": ["f665614c-4198-4864-8530-d72d7e97d50e"], "08c26149-89f0-438b-82ef-340f6c667a83": ["f665614c-4198-4864-8530-d72d7e97d50e"], "2b981a06-7777-44e7-86d4-8bc31c75d12b": ["b168e895-45c3-49d6-b925-ee2b05acdd5e"], "cf947e98-35b3-4edf-9a95-3248e2c073c8": ["b168e895-45c3-49d6-b925-ee2b05acdd5e"], "7043652d-6215-40bd-99c5-3617845ad562": ["45e161fa-0aea-477c-9e2b-d72c58d0b8ae"], "5f2aba22-5e8c-46f0-8290-0f6c534c9e58": ["45e161fa-0aea-477c-9e2b-d72c58d0b8ae"], "0d1f52a2-1889-4456-873c-8e433b8103f0": ["4d4009c8-827f-49e6-86fe-9cd3de962e18"], "18c58c8f-d55a-4e1d-9c1e-83e62e62f489": ["4d4009c8-827f-49e6-86fe-9cd3de962e18"], "de0e18fd-b3c9-4b2f-a1b1-890c971a7b64": ["24e7b8e5-4c92-4469-8ba9-afc9b858a8d0"], "7b54c6f2-f98b-44a1-9cb9-0006a6885dba": ["24e7b8e5-4c92-4469-8ba9-afc9b858a8d0"], "7be29337-b304-4dc1-a82e-8006c5797000": ["398bfad3-816a-4e6b-bd54-087e787ca143"], "58a1ac87-072b-457f-8d6d-9d54f99c9c72": ["398bfad3-816a-4e6b-bd54-087e787ca143"], "19c9ab8d-afd0-4faa-b537-2d6c15274356": ["c06b3759-7d1b-4e0a-983c-fcbb7e7b0b8c"], "2011f606-5c9b-4f99-8885-3435f4852ab6": ["c06b3759-7d1b-4e0a-983c-fcbb7e7b0b8c"], "d93b6b09-13e0-476a-8907-b0c440ebb99c": ["e7ca6b6b-c04d-4d3c-bd64-4b77a1ef31bc"], "2f4a4bd2-30b5-4219-a1d6-68bf9cefd1dd": ["e7ca6b6b-c04d-4d3c-bd64-4b77a1ef31bc"], "0ee2fb90-ef41-42f8-85b2-0bcee5bfbea5": ["ebd87564-c837-44c5-97f3-1e653bf21e50"], "40d6c70c-4c9f-4583-a9a9-f42320653016": ["ebd87564-c837-44c5-97f3-1e653bf21e50"], "e0f4d002-0301-4c77-b674-0ead43241b67": ["4eddb830-5ece-4763-bf97-69e9f24b61cb"], "fe91e110-83ea-4d65-a8dd-16e0a93bbccd": ["4eddb830-5ece-4763-bf97-69e9f24b61cb"], "730a6ef9-4a5f-4454-82eb-a27b174bda51": ["c6b21473-17b3-4411-a801-ff40b859a3ba"], "e8388d13-748d-436a-9000-b567ba3c9b3d": ["c6b21473-17b3-4411-a801-ff40b859a3ba"], "7fb6ba3f-35c1-46a3-8597-3065b2d96752": ["12ead7ff-d728-404a-9342-1331972fe95a"], "640de923-46b2-4807-9146-eb66e270562a": ["12ead7ff-d728-404a-9342-1331972fe95a"], "338a2377-8d8e-4349-840f-aeae1851e834": ["fb87ecf3-4b7f-403c-83e3-8df71ec844cc"], "98dde9b5-9c32-42b9-b1bf-3a01fdbd0b4b": ["fb87ecf3-4b7f-403c-83e3-8df71ec844cc"], "ca898446-cd7f-4375-9d18-80b32ac42eb9": ["cac20d2e-a376-4dbd-a593-6f518f9b3173"], "950ea5ef-a7a4-4a1f-9223-ab697b034880": ["cac20d2e-a376-4dbd-a593-6f518f9b3173"], "406e702a-3722-424b-8d99-4004460446dd": ["d1511faa-d630-4d98-a77c-ed3b7a850b43"], "9fb0651d-c7cc-4880-bd9e-6117dda72936": ["d1511faa-d630-4d98-a77c-ed3b7a850b43"], "9173450b-cf3b-4b56-ac43-aaa9c2366c16": ["6c36f7a4-8b20-4e1b-a85d-d8fc8b91c791"], "745394f6-fa8f-4c55-abc8-5e766bfb30a1": ["6c36f7a4-8b20-4e1b-a85d-d8fc8b91c791"], "c6ccbeba-18dc-45d0-b7b0-841f969ef981": ["4ceeb692-63b5-452b-8f9a-fa3268c1d60a"], "5dfa8b10-912f-467b-92e7-eba18b8cf7c7": ["4ceeb692-63b5-452b-8f9a-fa3268c1d60a"], "9ff0b25a-3876-4b9a-b17f-1f13a6b4fc7b": ["c0408cd6-041b-43c9-ad5e-684889bf6f95"], "87292c75-3c40-4b64-b7cc-e8bc7442dbc2": ["c0408cd6-041b-43c9-ad5e-684889bf6f95"], "013102ac-4d05-41ed-a13a-c82f083a386a": ["17a5f8ab-6ccd-4a9c-8912-f29e87222bca"], "e1b7a480-d4c8-4c52-a42f-d49388d89030": ["17a5f8ab-6ccd-4a9c-8912-f29e87222bca"], "3738b51e-bb9e-4838-b4ba-1275e1570fb3": ["8cca91f3-a696-4087-92c9-7800cd79a85f"], "1f8fee84-c732-475d-a073-81cc21a2e5d6": ["8cca91f3-a696-4087-92c9-7800cd79a85f"], "59272d70-39c4-4900-a2eb-75e9970fb604": ["3f8e1815-4f3e-4174-a2a4-e9f13ef07c96"], "c3fc9a5d-1321-4c56-8a01-a52be7843f14": ["3f8e1815-4f3e-4174-a2a4-e9f13ef07c96"], "ad5807ff-957e-445a-8249-92db7baf8b3f": ["fbab112e-6429-4ac4-bc7a-ff1623906f3a"], "5a0d1277-601a-4883-b7c0-9e1d0bb0f703": ["fbab112e-6429-4ac4-bc7a-ff1623906f3a"], "ef7cea96-5afd-45f0-91f8-cd310fe38363": ["084954bd-6218-42f6-9aca-f30820c5110a"], "fcc7b5a8-6029-4225-80df-fc86d114a9d7": ["084954bd-6218-42f6-9aca-f30820c5110a"], "2529a49e-08ba-43f4-83f7-f1a31c0f9ce6": ["a8df712a-6706-492f-9518-a6262542ae49"], "0b8aee2e-c665-4bad-be3a-e3c0703f5d5c": ["a8df712a-6706-492f-9518-a6262542ae49"], "ab30f7f7-1da3-4996-981b-b334e624b75b": ["8e8dd93e-de2e-417e-a03f-27007f1db51a"], "a9fbf68e-6b95-4a37-ba84-56de6733d04e": ["8e8dd93e-de2e-417e-a03f-27007f1db51a"], "992ca03b-c327-47fb-9976-af81917f46bb": ["427aa6e7-aabe-492c-ac98-bd8c53a3021c"], "629ec262-866a-44f1-aa1c-58412a55abf7": ["427aa6e7-aabe-492c-ac98-bd8c53a3021c"], "603bcf00-4dab-41d8-8ba0-b21805500b49": ["33f85d63-d76b-429f-baf4-146cf3f0cd92"], "e34f517b-451b-40ad-8048-5a38f554d5c3": ["33f85d63-d76b-429f-baf4-146cf3f0cd92"], "aae66143-1bc2-4594-acc8-7d3615728cd7": ["ccd503f4-3054-4865-84ae-52140797e686"], "9c4325b3-f45c-4e06-b40e-7f277038d3ae": ["ccd503f4-3054-4865-84ae-52140797e686"], "7ccb8d42-cafc-471e-a554-31afdacd3e6e": ["f6345ae8-5537-4545-805a-d419ccbd7b58"], "b6fbb6df-5593-41d9-b6e2-7e48c0791453": ["f6345ae8-5537-4545-805a-d419ccbd7b58"], "54f37cd5-2729-42c7-86ab-de93771d48de": ["e93a97bd-f7f2-494f-815a-6c2483c7f929"], "2bfa8a4b-bb79-454d-8f54-9e99c6a7eeec": ["e93a97bd-f7f2-494f-815a-6c2483c7f929"], "06d62d50-a0ff-4cd6-b9d9-edae882b8083": ["55a82d79-3651-4c07-9925-501bd29b486b"], "f3fe5ae7-edb9-465c-8b8a-b2a1dcf9941b": ["55a82d79-3651-4c07-9925-501bd29b486b"], "783875d9-c94e-4a31-b999-ab6987247a13": ["6bd50fba-21b5-4c8d-9a33-7e31811f4688"], "6c865737-4898-4396-a3aa-a6ec704beb61": ["6bd50fba-21b5-4c8d-9a33-7e31811f4688"], "5b3a2c71-90ea-46f7-a15d-e725a8b03bc0": ["495a1d1a-9cc6-41a4-9e4c-e23a3383de45"], "beaaba21-160a-4b49-a35f-ebd3649ae8da": ["495a1d1a-9cc6-41a4-9e4c-e23a3383de45"], "9cde1d56-efc1-4d6c-9e62-6d01b5206637": ["67f861ac-d5cf-471f-9f1d-2de2d9b2605c"], "85d52f9b-8ff0-4b74-bf87-4c0eed65be82": ["67f861ac-d5cf-471f-9f1d-2de2d9b2605c"], "bc161c5c-7664-4f95-a110-9e25fcc7efe7": ["a923ff4d-0a75-48f6-9690-5debb7aea714"], "2d8f4044-2f57-42e9-8a98-4cfef1cbfcf7": ["a923ff4d-0a75-48f6-9690-5debb7aea714"], "43144843-20db-4591-817a-09d683920813": ["2b61969b-a4d8-4840-9763-2a8aab744de4"], "118efe1b-1ac5-404d-85ab-62ca2ae14f2a": ["2b61969b-a4d8-4840-9763-2a8aab744de4"], "e80f1027-632e-442c-bb01-e1a660970059": ["70d41c71-c073-4d34-9f2d-1e5d9bec9f7f"], "1869d7b8-9b26-48ea-ad95-0b6696fc3b5f": ["70d41c71-c073-4d34-9f2d-1e5d9bec9f7f"], "194840b4-833a-4581-9fbd-12642995d7ec": ["0bf3e239-cb0c-4e32-8add-9935c1c54207"], "477b2c61-2654-4448-95b1-7ce704aa17a9": ["0bf3e239-cb0c-4e32-8add-9935c1c54207"], "871e3096-818d-435e-849d-e814def3c940": ["82872239-471a-4c64-a6f1-656a7a72f779"], "fe28741d-6a24-4790-9132-9e41245be3a2": ["82872239-471a-4c64-a6f1-656a7a72f779"], "2a642e59-84aa-41ea-8d1b-6f91cdb97c7b": ["1d4dbe76-6294-4c51-bd9b-b22f92caf958"], "86875e79-935d-4fbc-9d78-80ca2659c8af": ["1d4dbe76-6294-4c51-bd9b-b22f92caf958"], "129a486e-775b-4c60-9f0f-34217cb72187": ["d46813ed-51ab-4c0a-b988-dee378421920"], "ab21521c-24e1-452b-8189-e9e8f56ba818": ["d46813ed-51ab-4c0a-b988-dee378421920"], "09573468-3b35-4f5f-bc2a-9cea28494162": ["204e22f0-74a0-479d-8ecf-3221e37181c2"], "993d9e01-256c-4bc1-a11b-f2e3fc1d3656": ["204e22f0-74a0-479d-8ecf-3221e37181c2"], "63182b04-8156-4f2d-ae63-8d1f0aba552e": ["5c8efab7-5f74-4a2c-b30b-6009f3cb9f05"], "9ff6aaee-e3e0-4926-891f-169acf929d88": ["5c8efab7-5f74-4a2c-b30b-6009f3cb9f05"], "6b1bd526-09e6-426a-990c-c1f67eea2b5d": ["917b2edd-2501-4d54-9cc2-3c49c582396b"], "1fac34b7-c9fd-41c3-9632-a5841230fd80": ["917b2edd-2501-4d54-9cc2-3c49c582396b"], "1abaf8e7-7d19-400d-9e65-353f6f5a5e42": ["5cd28879-abe6-4109-a650-321a9334313b"], "2ba0622c-0776-49d4-b1c8-12993bf8cedc": ["5cd28879-abe6-4109-a650-321a9334313b"], "6a1749b3-1471-410d-85f5-b919a3ec45fc": ["b0312159-0b91-4089-8744-dd57e4b8672f"], "6a156442-467d-4f55-b548-3d559e5680b3": ["b0312159-0b91-4089-8744-dd57e4b8672f"], "95add154-f6b3-4270-b9b5-93c6e6c025a9": ["e6b2fc26-b433-4d4e-8e62-a69a52ed465c"], "b5d10495-d547-4e6c-8a5a-48c0e50411a2": ["e6b2fc26-b433-4d4e-8e62-a69a52ed465c"], "73967cf0-77f5-4d86-b72f-002c2699480a": ["971f64d8-51aa-4637-a438-a6f001878bf1"], "3ed7b45e-c09b-4207-acd7-920a29f99c56": ["971f64d8-51aa-4637-a438-a6f001878bf1"], "4c3f4bc6-9d4d-4a62-9c7d-987fed8ee7dc": ["4a84ea14-0f78-46ed-81a3-b80591dae0e4"], "bcff172e-9865-4aee-8bf4-3cf5a241ed81": ["4a84ea14-0f78-46ed-81a3-b80591dae0e4"], "1d4ea51b-e977-42cb-a2dc-84844206879d": ["73792944-31ae-40b5-9693-e7e5e7b816c3"], "3e59da17-e66e-4cbe-9163-189421779b6a": ["73792944-31ae-40b5-9693-e7e5e7b816c3"], "c253c1ad-6a1a-4c73-8958-45a249614d7c": ["d75cb414-2cab-4fcf-9e3b-e79c52f40541"], "f35e209c-cbef-427b-9073-b844904036eb": ["d75cb414-2cab-4fcf-9e3b-e79c52f40541"], "866d436b-1401-43b4-9d64-8625a39bac9d": ["e6de1e79-112d-41df-bb0f-ed5f0973ae4f"], "510abedf-3fd8-4f17-983b-ccde09c18a4e": ["e6de1e79-112d-41df-bb0f-ed5f0973ae4f"], "485a46ca-2714-4a6a-9d24-984b44258ca1": ["0d78c95c-b646-4b52-8878-773e2425c443"], "8f2019c6-fc1b-472b-843f-c86026b19bf5": ["0d78c95c-b646-4b52-8878-773e2425c443"], "9d55050e-c843-476b-a9f2-12149397ebff": ["ad832177-3f64-4cce-9bfb-43a601c01c55"], "fbf5ea24-7d27-470b-abd5-46143e409ad6": ["ad832177-3f64-4cce-9bfb-43a601c01c55"], "2425065f-a26c-4106-98a4-208dfb25d49b": ["7acd026f-2afd-41ca-9d24-66c6d7b4e20d"], "ab47f30c-b75a-4dea-9425-b815d9105c7b": ["7acd026f-2afd-41ca-9d24-66c6d7b4e20d"], "3a0584f8-cfbf-4601-a0a8-879f58e50b51": ["343696f3-0d02-443f-ac4c-ed3e2871430d"], "2d40d0a7-93de-4e41-9079-675cc47f0e7a": ["343696f3-0d02-443f-ac4c-ed3e2871430d"], "9421e1c5-cc74-4b09-8d4b-1e5799121bf7": ["974b2f2d-fc44-4056-83c8-1c1bd4129f43"], "3925f1c9-4080-4f81-9a7f-ab68bf56ccd1": ["974b2f2d-fc44-4056-83c8-1c1bd4129f43"], "8e04aa74-8c04-4ebd-9e67-9792fa10f1db": ["de313654-8f9f-427c-a57e-9f8d1a65342a"], "093adf92-6ab8-4860-9869-42533af41d73": ["de313654-8f9f-427c-a57e-9f8d1a65342a"], "caeb4144-5378-4f31-b498-d4999251deb6": ["be0e1082-2a53-4197-82a9-a27e52afe77c"], "ec89ef9d-7701-439f-8d61-d431c4c7347c": ["be0e1082-2a53-4197-82a9-a27e52afe77c"], "17ebc621-2f67-4041-bd52-768f8b4f66d6": ["7618ed6c-bb6e-48be-9c01-0277f256a286"], "890ed485-3d96-439c-895e-3aa900d47217": ["7618ed6c-bb6e-48be-9c01-0277f256a286"], "6a98066b-ff2f-4948-90c8-ecf965491498": ["236709d1-4c94-4e47-8c2f-9b1935ed3101"], "533d4921-9014-44eb-b07b-acbccd6947b8": ["236709d1-4c94-4e47-8c2f-9b1935ed3101"], "43233724-a8a7-44a5-a85a-79db8f519e33": ["56844b37-a0b4-46b0-ab49-2e91ba5cc3ed"], "a7c9e2be-1d95-42c2-83a3-e67495fec8bd": ["56844b37-a0b4-46b0-ab49-2e91ba5cc3ed"], "8f7c2b35-c50b-4431-9d29-aabf188b29c7": ["2637f07f-0e18-4e00-95a1-017302c1eebf"], "fc978c62-3323-46fc-99ed-8b3418b5f1f9": ["2637f07f-0e18-4e00-95a1-017302c1eebf"], "b967db1b-5454-443e-a3e3-9485e602180d": ["e5a60d6b-9c57-480a-9340-dfc5264e7bf6"], "0991442d-0e56-4d93-b5ff-e7d130c5e99e": ["e5a60d6b-9c57-480a-9340-dfc5264e7bf6"], "f37cbacb-d07e-40b2-9f5c-6e0e7142aa9a": ["a263bf7c-1285-4818-b081-b61114301e97"], "1aa61639-805d-4846-a43e-00410aaf458f": ["a263bf7c-1285-4818-b081-b61114301e97"], "8bf7412f-dd45-406e-96ae-2f7d59cc036b": ["c9cd577c-1f3d-4db8-927a-f495c3f274be"], "0a464c13-6316-4cb7-a124-7f8f909fd2aa": ["c9cd577c-1f3d-4db8-927a-f495c3f274be"], "78e27949-270c-4bee-a334-9f164713113d": ["c282a38e-4939-4b5c-ab4b-7db57caa6092"], "3283564a-c46c-4980-a99c-8a2aac91b4f1": ["c282a38e-4939-4b5c-ab4b-7db57caa6092"], "4ea8f1cc-1867-4363-bd2a-35fe4288f9a1": ["aa70db96-141a-48b6-b812-bc2d7cbd4f2d"], "05302944-44d3-4b1d-8720-90f856989ec2": ["aa70db96-141a-48b6-b812-bc2d7cbd4f2d"], "0495dadf-297c-4c56-a7d8-57e70b920e74": ["67a07331-d1a7-44fe-bac2-0b7a08a0336b"], "6f05230b-a5bc-491a-a563-4a052b1f4b7d": ["67a07331-d1a7-44fe-bac2-0b7a08a0336b"], "63219ac7-5825-4ff1-bae8-4f1f0e05d806": ["11142b8e-6ed2-4e1e-97b6-f5aa0ae91259"], "f528f43b-db33-4e30-aa53-1b7d87eb557b": ["11142b8e-6ed2-4e1e-97b6-f5aa0ae91259"], "fc494ba9-d40b-403e-a60a-0b98d99aa411": ["e6ef7c2b-18a3-4c51-b0a8-68e65f03912c"], "7a34af57-294e-4ab2-a676-52f337759dfd": ["e6ef7c2b-18a3-4c51-b0a8-68e65f03912c"], "e395eb6e-aba9-470a-a998-79addad23e83": ["7d17f7f1-2c22-4077-9a47-d1a94a8e4666"], "8f472cc2-6a57-4c2e-8f5d-03e53a677632": ["7d17f7f1-2c22-4077-9a47-d1a94a8e4666"], "f72a4b0e-3705-4c45-84f4-579777e35f47": ["c018662e-8711-4e1e-b1c2-996ba8b8ac4b"], "99c83083-2846-4f90-93a7-ce05694368c4": ["c018662e-8711-4e1e-b1c2-996ba8b8ac4b"], "dee6c9c7-5653-4d75-a9dd-eee62dd6548b": ["3711e070-6ed6-47f5-be5e-46ab64735ea7"], "2b1faff4-73b8-4f25-9093-ee21a49e65cd": ["3711e070-6ed6-47f5-be5e-46ab64735ea7"]}, "corpus": {"f4163331-41c3-44cc-8f37-215ef88b328b": "context in which automated systems are being utilized. In some circumstances, application of these principles \nin whole or in part may not be appropriate given the intended use of automated systems to achieve government \nagency missions. Future sector-specific guidance will likely be necessary and important for guiding the use of \nautomated systems in certain settings such as AI systems used as part of school building security or automated \nhealth diagnostic systems. \nThe Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of \nequities, for example, between the protection of sensitive law enforcement information and the principle of \nnotice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and \nother law enforcement equities. Even in contexts where these principles may not apply in whole or in part,", "6d0e2aff-af8c-4231-a0d7-33e73767c307": "di\ufb03cult to estimate given the wide range of GAI stakeholders, uses, inputs, and outputs. Challenges with \nrisk estimation are aggravated by a lack of visibility into GAI training data, and the generally immature \nstate of the science of AI measurement and safety today. This document focuses on risks for which there \nis an existing empirical evidence base at the time this pro\ufb01le was written; for example, speculative risks \nthat may potentially arise in more advanced, future GAI systems are not considered. Future updates may \nincorporate additional risks or provide further details on the risks identi\ufb01ed below. \nTo guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by \nthe development and use of GAI are de\ufb01ned below.5 Each risk is labeled according to the outcome, \nobject, or source of the risk (i.e., some are risks \u201cto\u201d a subject or domain and others are risks \u201cof\u201d or", "7bf222da-6351-416d-a0f2-32e276a0bb9d": "defer liability in unexpected ways and/or contribute to unauthorized data \ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \nassignment of liability and responsibility for incidents, GAI system changes over \ntime (e.g., \ufb01ne-tuning, drift, decay); Request: Noti\ufb01cation and disclosure for \nserious incidents arising from third-party data and systems; Service Level \nAgreements (SLAs) in vendor contracts that address incident response, response \ntimes, and availability of critical support. \nHuman-AI Con\ufb01guration; \nInformation Security; Value Chain \nand Component Integration \nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third-party entities \n \nMAP 1.1: Intended purposes, potentially bene\ufb01cial uses, context speci\ufb01c laws, norms and expectations, and prospective settings in \nwhich the AI system will be deployed are understood and documented. Considerations include: the speci\ufb01c set or types of users", "2fc822e4-1c4a-45b8-8cb6-f5101db0b817": "belief that people can change for the better, system use can lead to the loss of jobs and custody of children, and \nsurveillance can lead to chilling effects for communities and sends negative signals to community members \nabout how they're viewed. \nIn discussion of technical and governance interventions that that are needed to protect against the harms of \nthese technologies, various panelists emphasized that transparency is important but is not enough to achieve \naccountability. Some panelists discussed their individual views on additional system needs for validity, and \nagreed upon the importance of advisory boards and compensated community input early in the design process \n(before the technology is built and instituted). Various panelists also emphasized the importance of regulation \nthat includes limits to the type and cost of such technologies. \n56", "a5fbd50c-e73f-4de9-8e42-f6cf5c0935a3": "breeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; and \nthreatening our democratic process.63 The American public should be protected from these growing risks. \nIncreasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer \nprivacy into their products by design and by default, including by minimizing the data they collect, communicating \ncollection and use clearly, and improving security practices. Federal government surveillance and other collection and \nuse of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention \nin some cases. Many states have also enacted consumer data privacy protection regimes to address some of these \nharms. \nHowever, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory", "f24752e1-6838-46b4-a201-d6a184539b67": "program on Explainable Artificial Intelligence that aims to create a suite of machine learning techniques that \nproduce more explainable models, while maintaining a high level of learning performance (prediction \naccuracy), and enable human users to understand, appropriately trust, and effectively manage the emerging \ngeneration of artificially intelligent partners.95 The National Science Foundation\u2019s program on Fairness in \nArtificial Intelligence also includes a specific interest in research foundations for explainable AI.96\n45", "a193a6ca-e628-4aec-9a95-ebcd4b844fd7": "DATA PRIVACY \nEXTRA PROTECTIONS FOR DATA RELATED TO SENSITIVE\nDOMAINS\n\u2022\nContinuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleep\napnea, and send usage data to a patient\u2019s insurance company, which may subsequently deny coverage for the\ndevice based on usage data. Patients were not aware that the data would be used in this way or monitored\nby anyone other than their doctor.70 \n\u2022\nA department store company used predictive analytics applied to collected consumer data to determine that a\nteenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to her\nhouse, revealing to her father that she was pregnant.71\n\u2022\nSchool audio surveillance systems monitor student conversations to detect potential \"stress indicators\" as\na warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an", "6f993179-3a2c-4fac-ab5a-05785098b2c3": "33 \nMEASURE 2.7: AI system security and resilience \u2013 as identi\ufb01ed in the MAP function \u2013 are evaluated and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.7-001 \nApply established security measures to: Assess likelihood and magnitude of \nvulnerabilities and threats such as backdoors, compromised dependencies, data \nbreaches, eavesdropping, man-in-the-middle attacks, reverse engineering, \nautonomous agents, model theft or exposure of model weights, AI inference, \nbypass, extraction, and other baseline security concerns. \nData Privacy; Information Integrity; \nInformation Security; Value Chain \nand Component Integration \nMS-2.7-002 \nBenchmark GAI system security and resilience related to content provenance \nagainst industry standards and best practices. Compare GAI system security \nfeatures and content provenance methods against industry state-of-the-art. \nInformation Integrity; Information \nSecurity \nMS-2.7-003", "d3ccc1e5-5b64-4df4-b775-cc6a878f1a1d": "Intelligence, and Disability Discrimination in Hiring. May 12, 2022. https://beta.ada.gov/resources/ai\u00ad\nguidance/\n54. Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in\nan algorithm used to manage the health of populations. Science. Vol. 366, No. 6464. Oct. 25, 2019. https://\nwww.science.org/doi/10.1126/science.aax2342\n55. Data & Trust Alliance. Algorithmic Bias Safeguards for Workforce: Overview. Jan. 2022. https://\ndataandtrustalliance.org/Algorithmic_Bias_Safeguards_for_Workforce_Overview.pdf\n56. Section 508.gov. IT Accessibility Laws and Policies. Access Board. https://www.section508.gov/\nmanage/laws-and-policies/\n67", "509a8a56-cbe6-4d60-bde5-4a450f56908e": "11 \nvalue chain (e.g., data inputs, processing, GAI training, or deployment environments), conventional \ncybersecurity practices may need to adapt or evolve. \nFor instance, prompt injection involves modifying what input is provided to a GAI system so that it \nbehaves in unintended ways. In direct prompt injections, attackers might craft malicious prompts and \ninput them directly to a GAI system, with a variety of downstream negative consequences to \ninterconnected systems. Indirect prompt injection attacks occur when adversaries remotely (i.e., without \na direct interface) exploit LLM-integrated applications by injecting prompts into data likely to be \nretrieved. Security researchers have already demonstrated how indirect prompt injections can exploit \nvulnerabilities by stealing proprietary data or running malicious code remotely on a machine. Merely \nquerying a closed production model can elicit previously undisclosed information about that model.", "ba7526ba-a852-4c81-8fed-f894ff1613b2": "\u2022 An automated signature matching system is used as part of the voting process in many parts of the country to\ndetermine whether the signature on a mail-in ballot matches the signature on file. These signature matching\nsystems are less likely to work correctly for some voters, including voters with mental or physical\ndisabilities, voters with shorter or hyphenated names, and voters who have changed their name.97 A human\ncuring process,98 which helps voters to confirm their signatures and correct other voting mistakes, is\nimportant to ensure all votes are counted,99 and it is already standard practice in much of the country for\nboth an election official and the voter to have the opportunity to review and correct any such issues.100 \n47", "cf7d8549-3bef-4142-a2f3-5e2be80c04b3": "18. Office of Management and Budget. Study to Identify Methods to Assess Equity: Report to the President.\nAug. 2021. https://www.whitehouse.gov/wp-content/uploads/2021/08/OMB-Report-on-E013985\u00ad\nImplementation_508-Compliant-Secure-v1.1.pdf\n19. National Institute of Standards and Technology. AI Risk Management Framework. Accessed May 23,\n2022. https://www.nist.gov/itl/ai-risk-management-framework\n20. U.S. Department of Energy. U.S. Department of Energy Establishes Artificial Intelligence Advancement\nCouncil. U.S. Department of Energy Artificial Intelligence and Technology Office. April 18, 2022. https://\nwww.energy.gov/ai/articles/us-department-energy-establishes-artificial-intelligence-advancement-council\n21. Department of Defense. U.S Department of Defense Responsible Artificial Intelligence Strategy and\nImplementation Pathway. Jun. 2022. https://media.defense.gov/2022/Jun/22/2003022604/-1/-1/0/\nDepartment-of-Defense-Responsible-Artificial-Intelligence-Strategy-and-Implementation\u00ad", "fbe7ad7a-0f08-49d7-b2e9-8c18909919c7": "tion from past decisions to infect decision-making in unrelated situations.  In some cases, technologies are purposeful\u00ad\nly designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intended \nor unintended uses lead to unintended harms. \nMany of the harms resulting from these technologies are preventable, and actions are already being taken to protect \nthe public. Some companies have put in place safeguards that have prevented harm from occurring by ensuring that \nkey development decisions are vetted by an ethics review; others have identified and mitigated harms found through \npre-deployment testing and ongoing monitoring processes. Governments at all levels have existing public consulta\u00ad\ntion processes that may be applied when considering the use of new automated systems, and existing product develop\u00ad\nment and testing practices already protect the American public from many potential harms.", "8dbd05ab-b5ee-4a14-b413-18f7553df4cd": "notice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and \nother law enforcement equities. Even in contexts where these principles may not apply in whole or in part, \nfederal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well as \nexisting policies and safeguards that govern automated systems, including, for example, Executive Order 13960, \nPromoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020). \nThis white paper recognizes that national security (which includes certain law enforcement and \nhomeland security activities) and defense activities are of increased sensitivity and interest to our nation\u2019s \nadversaries and are often subject to special requirements, such as those governing classified information and \nother protected data. Such activities require alternative, compatible safeguards through existing policies that", "43065ed7-0360-4abe-bba7-3ff7ae4728f5": "construed broadly. An explanation need not be a plain-language statement about causality but could consist of \nany mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the \nstated purpose. Tailoring should be assessed (e.g., via user experience research). \nTailored to the target of the explanation. Explanations should be targeted to specific audiences and \nclearly state that audience. An explanation provided to the subject of a decision might differ from one provided \nto an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience \nresearch). \n43", "2537f8d4-61aa-4fb4-9e5c-f47a828400c9": "35 \nMEASURE 2.9: The AI model is explained, validated, and documented, and AI system output is interpreted within its context \u2013 as \nidenti\ufb01ed in the MAP function \u2013 to inform responsible use and governance. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.9-001 \nApply and document ML explanation results such as: Analysis of embeddings, \nCounterfactual prompts, Gradient-based attributions, Model \ncompression/surrogate models, Occlusion/term reduction. \nConfabulation \nMS-2.9-002 \nDocument GAI model details including: Proposed use and organizational value; \nAssumptions and limitations, Data collection methodologies; Data provenance; \nData quality; Model architecture (e.g., convolutional neural network, \ntransformers, etc.); Optimization objectives; Training algorithms; RLHF \napproaches; Fine-tuning or retrieval-augmented generation approaches; \nEvaluation data; Ethical considerations; Legal and regulatory requirements. \nInformation Integrity; Harmful Bias \nand Homogenization", "a75501ab-ae94-4f46-85b4-0673035a7e87": "data. \u201cSensitive domains\u201d are those in which activities being conducted can cause material harms, including signifi\u00ad\ncant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains \nthat have historically been singled out as deserving of enhanced data protections or where such enhanced protections \nare reasonably expected by the public include, but are not limited to, health, family planning and care, employment, \neducation, criminal justice, and personal finance. In the context of this framework, such domains are considered \nsensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains \nand data that are considered sensitive are understood to change over time based on societal norms and context. \n36", "ec4dabc9-7f02-476b-8b06-85be1e07e288": "domain where previous assumptions (relating to context of use or mapped risks \nsuch as security, and safety) may no longer hold.  \nCBRN Information or Capabilities; \nIntellectual Property; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content; Data \nPrivacy \nMP-4.1-009 Leverage approaches to detect the presence of PII or sensitive data in generated \noutput text, image, video, or audio. \nData Privacy", "605acb2c-1230-47de-a098-f285f493da9c": "illustrative examples. \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS: \n\u2022 The expectations for automated systems are meant to serve as a blueprint for the development of additional technical\nstandards and practices that should be tailored for particular sectors and contexts.\n\u2022 This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. The \nexpectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoing \nmonitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offer \nconcrete directions for how those changes can be made. \n\u2022 Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can \nbe provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should", "ecf06de9-17a2-473c-84b0-b76e24ba17ce": "part of deployment approval (\u201cgo/\u201dno-go\u201d) policies, procedures, and processes, \nwith reviewed processes and approval thresholds re\ufb02ecting measurement of GAI \ncapabilities and risks. \nCBRN Information or Capabilities; \nConfabulation; Dangerous, \nViolent, or Hateful Content \nGV-1.3-003 \nEstablish a test plan and response policy, before developing highly capable models, \nto periodically evaluate whether the model may misuse CBRN information or \ncapabilities and/or o\ufb00ensive cyber capabilities. \nCBRN Information or Capabilities; \nInformation Security", "293eaad4-2df1-447e-989b-a12cb708347e": "Policy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office \nof the President with advice on the scientific, engineering, and technological aspects of the economy, national \nsecurity, health, foreign relations, the environment, and the technological recovery and use of resources, among \nother topics. OSTP leads interagency science and technology policy coordination efforts, assists the Office of \nManagement and Budget (OMB) with an annual review and analysis of Federal research and development in \nbudgets, and serves as a source of scientific and technological analysis and judgment for the President with \nrespect to major policies, plans, and programs of the Federal Government. \nLegal Disclaimer \nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper \npublished by the White House Office of Science and Technology Policy. It is intended to support the", "a220b840-7e8f-43b5-b29f-16df49f5d190": "25 \nMP-2.3-002 Review and document accuracy, representativeness, relevance, suitability of data \nused at di\ufb00erent stages of AI life cycle. \nHarmful Bias and Homogenization; \nIntellectual Property \nMP-2.3-003 \nDeploy and document fact-checking techniques to verify the accuracy and \nveracity of information generated by GAI systems, especially when the \ninformation comes from multiple (or unknown) sources. \nInformation Integrity  \nMP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., \nsynthetic media) that might be indistinguishable from human-generated content. Information Integrity \nMP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify \nvulnerabilities and potential manipulation or misuse. \nInformation Security \nAI Actor Tasks: AI Development, Domain Experts, TEVV \n \nMAP 3.4: Processes for operator and practitioner pro\ufb01ciency with AI system performance and trustworthiness \u2013 and relevant", "6f2fae09-0b14-41a0-8329-61a0a2eba224": "Glazunov, S. et al. (2024) Project Naptime: Evaluating O\ufb00ensive Security Capabilities of Large Language \nModels. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \npeople\u2019s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv.", "afe23b73-3420-420c-b4d9-51724dff5c54": "ing level appropriate for the audience. Notices and explanations may need to be available in multiple forms, \n(e.g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the \nAmerican public. \nProvide explanations as to how and why a decision was made or an action was taken by an \nautomated system \nTailored to the purpose. Explanations should be tailored to the specific purpose for which the user is \nexpected to use the explanation, and should clearly state that purpose. An informational explanation might \ndiffer from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the \ncontext of a dispute or contestation process. For the purposes of this framework, 'explanation' should be \nconstrued broadly. An explanation need not be a plain-language statement about causality but could consist of \nany mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the", "36128849-b7d8-4307-b4ce-f2ad6fcb9c4b": "53 \nDocumenting, reporting, and sharing information about GAI incidents can help mitigate and prevent \nharmful outcomes by assisting relevant AI Actors in tracing impacts to their source. Greater awareness \nand standardization of GAI incident reporting could promote this transparency and improve GAI risk \nmanagement across the AI ecosystem.  \nDocumentation and Involvement of AI Actors \nAI Actors should be aware of their roles in reporting AI incidents. To better understand previous incidents \nand implement measures to prevent similar ones in the future, organizations could consider developing \nguidelines for publicly available incident reporting which include information about AI actor \nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and", "a90ea09f-0280-4ba7-95cd-9fe427439c64": "adverse effects, to the greatest extent possible based on the needs of the use case. \nBrief and direct consent requests. When seeking consent from users short, plain language consent \nrequests should be used so that users understand for what use contexts, time span, and entities they are \nproviding data and metadata consent. User experience research should be performed to ensure these consent \nrequests meet performance standards for readability and comprehension. This includes ensuring that consent \nrequests are accessible to users with disabilities and are available in the language(s) and reading level appro\u00ad\npriate for the audience.  User experience design choices that intentionally obfuscate or manipulate user \nchoice (i.e., \u201cdark patterns\u201d) should be not be used. \n34", "fe145076-3215-43ab-a02d-4712ea9ae537": "\u2022 Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can \nbe provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should \nbe made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, law \nenforcement, or national security considerations may prevent public release. Where public reports are not possible, the \ninformation should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard \ning individuals\u2019 rights. These reporting expectations are important for transparency, so the American people can have\nconfidence that their rights, opportunities, and access as well as their expectations about technologies are respected. \n3\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE:", "e20b7b9a-c3e6-4c94-b713-aef9c060fb4e": "content detection mechanisms provide information about the origin and history of content to assist in \nGAI risk management e\ufb00orts. \nProvenance metadata can include information about GAI model developers or creators of GAI content, \ndate/time of creation, location, modi\ufb01cations, and sources. Metadata can be tracked for text, images, \nvideos, audio, and underlying datasets. The implementation of provenance data tracking techniques can \nhelp assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital \ncontent. Some well-known techniques for provenance data tracking include digital watermarking, \nmetadata recording, digital \ufb01ngerprinting, and human authentication, among others. \nProvenance Data Tracking Approaches \nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for", "51060321-dc74-43e1-8438-075b797ee302": "inferred data, purposeful privacy violations, or community surveillance or other community harms. Data \ncollection should be minimized and clearly communicated to the people whose data is collected. Data should \nonly be collected or used for the purposes of training or testing machine learning models if such collection and \nuse is legal and consistent with the expectations of the people whose data is collected. User experience \nresearch should be conducted to confirm that people understand what data is being collected about them and \nhow it will be used, and that this collection matches their expectations and desires. \nData collection and use-case scope limits. Data collection should be limited in scope, with specific, \nnarrow identified goals, to avoid \"mission creep.\"  Anticipated data collection should be determined to be \nstrictly necessary to the identified goals and should be minimized as much as possible. Data collected based on", "39332746-bd1b-4ce7-a679-62cfe8f0c2cf": "subject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as \ncivil rights, civil liberties, and privacy experts. For private sector applications, consultations before product \nlaunch may need to be confidential. Government applications, particularly law enforcement applications or \napplications that raise national security considerations, may require confidential or limited engagement based \non system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultation \nshould be documented, and the automated system developers were proposing to create, use, or deploy should \nbe reconsidered based on this feedback. \nTesting. Systems should undergo extensive testing before deployment. This testing should follow \ndomain-specific best practices, when available, for ensuring the technology will work in its real-world", "58f5c280-0c19-4f0b-b82f-a2fccf2a0e08": "models to \ufb02ag problematic inputs and outputs. \nInformation Integrity; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMG-3.2-006 \nImplement real-time monitoring processes for analyzing generated content \nperformance and trustworthiness characteristics related to content provenance \nto identify deviations from the desired standards and trigger alerts for human \nintervention. \nInformation Integrity", "29f10452-8781-40b8-925a-745ff9910d6b": "National Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \nBias in Arti\ufb01cial Intelligence https://www.nist.gov/publications/towards-standard-identifying-and-\nmanaging-bias-arti\ufb01cial-intelligence \nNorthcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. \narXiv. https://arxiv.org/pdf/2103.14749 \nOECD (2023) \"Advancing accountability in AI: Governing and managing risks throughout the lifecycle for \ntrustworthy AI\", OECD Digital Economy Papers, No. 349, OECD Publishing, Paris. \nhttps://doi.org/10.1787/2448f04b-en \nOECD (2024) \"De\ufb01ning AI incidents and related terms\" OECD Arti\ufb01cial Intelligence Papers, No. 16, OECD \nPublishing, Paris. https://doi.org/10.1787/d1a8d965-en \nOpenAI (2023) GPT-4 System Card. https://cdn.openai.com/papers/gpt-4-system-card.pdf \nOpenAI (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774", "fa513ea9-7423-4147-ac3e-efb5094d1013": "21 \nGV-6.1-005 \nImplement a use-cased based supplier risk assessment framework to evaluate and \nmonitor third-party entities\u2019 performance and adherence to content provenance \nstandards and technologies to detect anomalies and unauthorized changes; \nservices acquisition and value chain risk management; and legal compliance. \nData Privacy; Information \nIntegrity; Information Security; \nIntellectual Property; Value Chain \nand Component Integration \nGV-6.1-006 Include clauses in contracts which allow an organization to evaluate third-party \nGAI processes and standards.  \nInformation Integrity \nGV-6.1-007 Inventory all third-party entities with access to organizational content and \nestablish approved GAI technology and service provider lists. \nValue Chain and Component \nIntegration \nGV-6.1-008 Maintain records of changes to content made by third parties to promote content \nprovenance, including sources, timestamps, metadata. \nInformation Integrity; Value Chain \nand Component Integration;", "5d3018d5-7978-45b3-b0de-bbbdc9038d05": "Homogenization below).  \nTrustworthy AI Characteristics: Safe, Secure and Resilient \n2.4. Data Privacy \nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \naccepted privacy principles, including to transparency, individual participation (including consent), and \npurpose speci\ufb01cation. For example, most model developers do not disclose speci\ufb01c data sources on \nwhich models were trained, limiting user awareness of whether personally identi\ufb01ably information (PII) \nwas trained on and, if so, how it was collected.  \nModels may leak, generate, or correctly infer sensitive information about individuals. For example, \nduring adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \nincluded in their training data. This problem has been referred to as data memorization, and may pose", "fa3d6a06-bc3a-47b3-b62f-53a4b6c477e3": "Legal Disclaimer \nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper \npublished by the White House Office of Science and Technology Policy. It is intended to support the \ndevelopment of policies and practices that protect civil rights and promote democratic values in the building, \ndeployment, and governance of automated systems. \nThe Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy. It \ndoes not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or \ninternational instrument. It does not constitute binding guidance for the public or Federal agencies and \ntherefore does not require compliance with the principles described herein. It also is not determinative of what \nthe U.S. government\u2019s position will be in any international negotiation. Adoption of these principles may not", "bd23e28c-f261-4d6d-8327-4c8fd37ee81f": "tity-related information includes group characteristics or affiliations, geographic designations, location-based \nand association-based inferences, social networks, and biometrics. Continuous surveillance and monitoring \nsystems should not be used in physical or digital workplaces (regardless of employment status), public educa\u00ad\ntional institutions, and public accommodations. Continuous surveillance and monitoring systems should not \nbe used in a way that has the effect of limiting access to critical resources or services or suppressing the exer\u00ad\ncise of rights, even where the organization is not under a particular duty to protect those rights. \nProvide the public with mechanisms for appropriate and meaningful consent, access, and \ncontrol over their data \nUse-specific consent. Consent practices should not allow for abusive surveillance practices. Where data \ncollectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specif\u00ad", "c787b59b-c156-428e-af9e-1f0f48a6964a": "Applying The Blueprint for an AI Bill of Rights \nSENSITIVE DATA: Data and metadata are sensitive if they pertain to an individual in a sensitive domain \n(defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a \nsensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric \ndata, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship \nhistory and legal status such as custody and divorce information, and home, work, or school environmental \ndata); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful \nharm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about \nthose who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes,", "a5edf076-3212-4576-86aa-7b9095a8149e": "fully validated against the risk of collateral consequences. \nData reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result \nin the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi\u00ad\ncating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in \nsome cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure \nsafety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal \nmatters or private sector use) should only occur where use of such data is legally authorized and, after examina\u00ad\ntion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason\u00ad\nable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to", "586d6238-0439-4a27-911b-7eef2e2724cc": "should be kept up-to-date and people impacted by the system should be notified of significant use case or key \nfunctionality changes. You should know how and why an outcome impacting you was determined by an \nautomated system, including when the automated system is not the sole input determining the outcome. \nAutomated systems should provide explanations that are technically valid, meaningful and useful to you and to \nany operators or others who need to understand the system, and calibrated to the level of risk based on the \ncontext. Reporting that includes summary information about these automated systems in plain language and \nassessments of the clarity and quality of the notice and explanations should be made public whenever possible. \n6", "2e8724f0-c217-4ba0-bdd9-17f9392dc432": "consultation, design stage equity assessments (potentially including qualitative analysis), accessibility \ndesigns and testing, disparity testing, document any remaining disparities, and detail any mitigation \nimplementation and assessments. This algorithmic impact assessment should be made public whenever \npossible. Reporting should be provided in a clear and machine-readable manner using plain language to \nallow for more straightforward public accountability. \n28\nAlgorithmic \nDiscrimination \nProtections", "07427a9e-ff95-4acf-ad66-9634981a9d21": "18 \nGOVERN 3.2: Policies and procedures are in place to de\ufb01ne and di\ufb00erentiate roles and responsibilities for human-AI con\ufb01gurations \nand oversight of AI systems. \nAction ID \nSuggested Action \nGAI Risks \nGV-3.2-001 \nPolicies are in place to bolster oversight of GAI systems with independent \nevaluations or assessments of GAI models or systems where the type and \nrobustness of evaluations are proportional to the identi\ufb01ed risks. \nCBRN Information or Capabilities; \nHarmful Bias and Homogenization \nGV-3.2-002 \nConsider adjustment of organizational roles and components across lifecycle \nstages of large or complex GAI systems, including: Test and evaluation, validation, \nand red-teaming of GAI systems; GAI content moderation; GAI system \ndevelopment and engineering; Increased accessibility of GAI tools, interfaces, and \nsystems, Incident response and containment. \nHuman-AI Con\ufb01guration; \nInformation Security; Harmful Bias \nand Homogenization \nGV-3.2-003", "e804dcc8-9c34-46e6-83c9-8689530d5606": "Communities Through the Federal Government:\nhttps://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive-order\u00ad\nadvancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\n106. HealthCare.gov. Navigator - HealthCare.gov Glossary. Accessed May 2, 2022.\nhttps://www.healthcare.gov/glossary/navigator/\n72", "8d851dda-0d86-46f4-95d8-513aa368cc0a": "You should be protected from abusive data practices via built-in \nprotections and you should have agency over how data about \nyou is used. You should be protected from violations of privacy through \ndesign choices that ensure such protections are included by default, including \nensuring that data collection conforms to reasonable expectations and that \nonly data strictly necessary for the specific context is collected. Designers, de\u00ad\nvelopers, and deployers of automated systems should seek your permission \nand respect your decisions regarding collection, use, access, transfer, and de\u00ad\nletion of your data in appropriate ways and to the greatest extent possible; \nwhere not possible, alternative privacy by design safeguards should be used. \nSystems should not employ user experience and design decisions that obfus\u00ad\ncate user choice or burden users with defaults that are privacy invasive. Con\u00ad\nsent should only be used to justify collection of data in cases where it can be", "c3a69573-fc01-4e87-a725-7ef714f24aff": "used to provide a positive outcome; technology shouldn't be used to take supports away from people who need \nthem. \nPanel 6: The Healthcare System. This event explored current and emerging uses of technology in the \nhealthcare system and consumer products related to health. \nWelcome:\n\u2022\nAlondra Nelson, Deputy Director for Science and Society, White House Office of Science and Technology\nPolicy\n\u2022\nPatrick Gaspard, President and CEO, Center for American Progress\nModerator: Micky Tripathi, National Coordinator for Health Information Technology, U.S Department of \nHealth and Human Services. \nPanelists: \n\u2022\nMark Schneider, Health Innovation Advisor, ChristianaCare\n\u2022\nZiad Obermeyer, Blue Cross of California Distinguished Associate Professor of Policy and Management,\nUniversity of California, Berkeley School of Public Health\n\u2022\nDorothy Roberts, George A. Weiss University Professor of Law and Sociology and the Raymond Pace and", "641ba4b1-7c45-4392-9b10-ef829ecad4fe": "required to provide employees with a written description of each quota that applies to the employee, including \n\u201cquantified number of tasks to be performed or materials to be produced or handled, within the defined \ntime period, and any potential adverse employment action that could result from failure to meet the quota.\u201d93\nAcross the federal government, agencies are conducting and supporting research on explain-\nable AI systems. The NIST is conducting fundamental research on the explainability of AI systems. A multidis-\nciplinary team of researchers aims to develop measurement methods and best practices to support the \nimplementation of core tenets of explainable AI.94 The Defense Advanced Research Projects Agency has a \nprogram on Explainable Artificial Intelligence that aims to create a suite of machine learning techniques that \nproduce more explainable models, while maintaining a high level of learning performance (prediction", "280ff742-d637-40df-a320-bf44d0677965": "bene\ufb01cial for GAI risk measurement and management based on the context of \nuse. \nHarmful Bias and \nHomogenization; CBRN \nInformation or Capabilities \nMS-1.1-009 \nTrack and document risks or opportunities related to all GAI risks that cannot be \nmeasured quantitatively, including explanations as to why some risks cannot be \nmeasured (e.g., due to technological limitations, resource constraints, or \ntrustworthy considerations). Include unmeasured risks in marginal risks. \nInformation Integrity \nAI Actor Tasks: AI Development, Domain Experts, TEVV \n \nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are \ninvolved in regular assessments and updates. Domain experts, users, AI Actors external to the team that developed or deployed the \nAI system, and a\ufb00ected communities are consulted in support of assessments as necessary per organizational risk tolerance. \nAction ID \nSuggested Action \nGAI Risks \nMS-1.3-001", "1bd2eda4-11e6-4ed2-bfb6-70b8bd2e7842": "CBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content \nMG-2.2-002 \nDocument training data sources to trace the origin and provenance of AI-\ngenerated content. \nInformation Integrity \nMG-2.2-003 \nEvaluate feedback loops between GAI system content provenance and human \nreviewers, and update where needed. Implement real-time monitoring systems \nto a\ufb03rm that content provenance protocols remain e\ufb00ective.  \nInformation Integrity \nMG-2.2-004 \nEvaluate GAI content and data for representational biases and employ \ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \nbiases in the generated content. \nInformation Security; Harmful Bias \nand Homogenization \nMG-2.2-005 \nEngage in due diligence to analyze GAI output for harmful content, potential \nmisinformation, and CBRN-related or NCII content. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or", "8d20989c-320f-40f3-bc75-fca5cad166ef": "labor organization in connection with a labor dispute\" must report expenditures relating to this surveillance to \nthe Department of Labor Office of Labor-Management Standards, and consultants who employers retain for \nthese purposes must also file reports regarding their activities.81\nPrivacy choices on smartphones show that when technologies are well designed, privacy \nand data agency can be meaningful and not overwhelming. These choices\u2014such as contextual, timely \nalerts about location tracking\u2014are brief, direct, and use-specific. Many of the expectations listed here for \nprivacy by design and use-specific consent mirror those distributed to developers as best practices when \ndeveloping for smart phone devices,82 such as being transparent about how user data will be used, asking for app \npermissions during their use so that the use-context will be clear to users, and ensuring that the app will still \nwork if users deny (or later revoke) some permissions. \n39", "9a31121a-0079-4b8b-968b-b7706f5a5a79": "8 \nTrustworthy AI Characteristics: Accountable and Transparent, Privacy Enhanced, Safe, Secure and \nResilient \n2.5. Environmental Impacts \nTraining, maintaining, and operating (running inference on) GAI systems are resource-intensive activities, \nwith potentially large energy and environmental footprints. Energy and carbon emissions vary based on \nwhat is being done with the GAI model (i.e., pre-training, \ufb01ne-tuning, inference), the modality of the \ncontent, hardware used, and type of task or application. \nCurrent estimates suggest that training a single transformer LLM can emit as much carbon as 300 round-\ntrip \ufb02ights between San Francisco and New York. In a study comparing energy consumption and carbon \nemissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \nand carbon-intensive than discriminative or non-generative tasks (e.g., text classi\ufb01cation).", "6e879ae1-5ff9-4c75-8003-126e9a5a3fd0": "materials (SBOMs), application of service level agreements (SLAs), and statement on standards for \nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \nGAI systems. \nA.1.4. Pre-Deployment Testing \nOverview \nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \ncomplicates risk mapping and pre-deployment measurement e\ufb00orts. Robust test, evaluation, validation, \nand veri\ufb01cation (TEVV) processes can be iteratively applied \u2013 and documented \u2013 in early stages of the AI \nlifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous", "70bd0c1a-12d2-4365-b066-37eece26d188": "2.6. Harmful Bias and Homogenization \nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI \nsystems, can increase the speed and scale at which harmful biases manifest and are acted upon, \npotentially perpetuating and amplifying harms to individuals, groups, communities, organizations, and \nsociety. For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current \ntext-to-image models underrepresent women and/or racial minorities, and people with disabilities. \nImage generator models have also produced biased or stereotyped output for various demographic \ngroups and have di\ufb03culty producing non-stereotyped content even when the prompt speci\ufb01cally \nrequests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which \nmay stem from their training data, can also cause representational harms or perpetuate or exacerbate", "de8d503b-f8b1-403d-878d-16bd4c2bc55b": "60. See, e.g., the 2014 Federal Trade Commission report \u201cData Brokers A Call for Transparency and\nAccountability\u201d. https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency\u00ad\naccountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf\n61. See, e.g., Nir Kshetri. School surveillance of students via laptops may do more harm than good. The\nConversation. Jan. 21, 2022.\nhttps://theconversation.com/school-surveillance-of-students-via-laptops-may-do-more-harm-than\u00ad\ngood-170983; Matt Scherer. Warning: Bossware May be Hazardous to Your Health. Center for Democracy\n& Technology Report.\nhttps://cdt.org/wp-content/uploads/2021/07/2021-07-29-Warning-Bossware-May-Be-Hazardous-To\u00ad\nYour-Health-Final.pdf; Human Impact Partners and WWRC. The Public Health Crisis Hidden in Amazon\nWarehouses. HIP and WWRC report. Jan. 2021.\nhttps://humanimpact.org/wp-content/uploads/2021/01/The-Public-Health-Crisis-Hidden-In-Amazon\u00ad", "413255ec-385d-4c38-b4a0-a8cd7d03c126": "39 \nMS-3.3-004 \nProvide input for training materials about the capabilities and limitations of GAI \nsystems related to digital content transparency for AI Actors, other \nprofessionals, and the public about the societal impacts of AI and the role of \ndiverse and inclusive content generation. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Harmful Bias \nand Homogenization \nMS-3.3-005 \nRecord and integrate structured feedback about content provenance from \noperators, users, and potentially impacted communities through the use of \nmethods such as user research studies, focus groups, or community forums. \nActively seek feedback on generated content quality and potential biases. \nAssess the general awareness among end users and impacted communities \nabout the availability of these feedback channels. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, End-Users, Operation and Monitoring, TEVV", "207847f9-df7a-4af1-9aca-c965676a8034": "ENDNOTES\n12. Expectations about reporting are intended for the entity developing or using the automated system. The\nresulting reports can be provided to the public, regulators, auditors, industry standards groups, or others\nengaged in independent review, and should be made public as much as possible consistent with law,\nregulation, and policy, and noting that intellectual property or law enforcement considerations may prevent\npublic release. These reporting expectations are important for transparency, so the American people can\nhave confidence that their rights, opportunities, and access as well as their expectations around\ntechnologies are respected.\n13. National Artificial Intelligence Initiative Office. Agency Inventories of AI Use Cases. Accessed Sept. 8,\n2022. https://www.ai.gov/ai-use-case-inventories/\n14. National Highway Traffic Safety Administration. https://www.nhtsa.gov/", "4ee8929e-d1fe-493c-a5ef-f2838fa36ea4": "be reconsidered based on this feedback. \nTesting. Systems should undergo extensive testing before deployment. This testing should follow \ndomain-specific best practices, when available, for ensuring the technology will work in its real-world \ncontext. Such testing should take into account both the specific technology used and the roles of any human \noperators or reviewers who impact system outcomes or effectiveness; testing should include both automated \nsystems testing and human-led (manual) testing. Testing conditions should mirror as closely as possible the \nconditions in which the system will be deployed, and new testing may be required for each deployment to \naccount for material differences in conditions from one deployment to another. Following testing, system \nperformance should be compared with the in-place, potentially human-driven, status quo procedures, with \nexisting human performance considered as a performance baseline for the algorithm to meet pre-deployment,", "fe11fcc6-d36c-42bb-9fce-8dc62dc6ad64": "Establish terms of use and terms of service for GAI systems. \nIntellectual Property; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nGV-4.2-002 \nInclude relevant AI Actors in the GAI system risk identi\ufb01cation process. \nHuman-AI Con\ufb01guration \nGV-4.2-003 \nVerify that downstream GAI system impacts (such as the use of third-party \nplugins) are included in the impact documentation process. \nValue Chain and Component \nIntegration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring \n \nGOVERN 4.3: Organizational practices are in place to enable AI testing, identi\ufb01cation of incidents, and information sharing. \nAction ID \nSuggested Action \nGAI Risks \nGV4.3--001 \nEstablish policies for measuring the e\ufb00ectiveness of employed content \nprovenance methodologies (e.g., cryptography, watermarking, steganography, \netc.) \nInformation Integrity \nGV-4.3-002 \nEstablish organizational practices to identify the minimum set of criteria", "fd75683b-70f4-45f4-bcce-64da93feced9": "\u2022\nJennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio\nState University\n\u2022\nCarl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet\n\u2022\nSurya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup\n\u2022\nMariah Montgomery, National Campaign Director, Partnership for Working Families\n55", "cf235324-9f77-4de2-8ec4-d8423819499a": "14 \nGOVERN 1.2: The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.2-001 \nEstablish transparency policies and processes for documenting the origin and \nhistory of training data and generated data for GAI applications to advance digital \ncontent transparency, while balancing the proprietary nature of training \napproaches. \nData Privacy; Information \nIntegrity; Intellectual Property \nGV-1.2-002 \nEstablish policies to evaluate risk-relevant capabilities of GAI and robustness of \nsafety measures, both prior to deployment and on an ongoing basis, through \ninternal and external evaluations. \nCBRN Information or Capabilities; \nInformation Security \nAI Actor Tasks: Governance and Oversight \n \nGOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based \non the organization\u2019s risk tolerance. \nAction ID", "346cc623-448b-40c8-aa73-657d6c75adfb": "trip \ufb02ights between San Francisco and New York. In a study comparing energy consumption and carbon \nemissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \nand carbon-intensive than discriminative or non-generative tasks (e.g., text classi\ufb01cation).  \nMethods for creating smaller versions of trained models, such as model distillation or compression, \ncould reduce environmental impacts at inference time, but training and tuning such models may still \ncontribute to their environmental impacts. Currently there is no agreed upon method to estimate \nenvironmental impacts from GAI.  \nTrustworthy AI Characteristics: Accountable and Transparent, Safe \n2.6. Harmful Bias and Homogenization \nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI \nsystems, can increase the speed and scale at which harmful biases manifest and are acted upon,", "97935876-96f4-41ea-80fc-8a7a9c2ac7b5": "29 \nMS-1.1-006 \nImplement continuous monitoring of GAI system impacts to identify whether GAI \noutputs are equitable across various sub-populations. Seek active and direct \nfeedback from a\ufb00ected communities via structured feedback mechanisms or red-\nteaming to monitor and improve outputs.  \nHarmful Bias and Homogenization \nMS-1.1-007 \nEvaluate the quality and integrity of data used in training and the provenance of \nAI-generated content, for example by employing techniques like chaos \nengineering and seeking stakeholder feedback. \nInformation Integrity \nMS-1.1-008 \nDe\ufb01ne use cases, contexts of use, capabilities, and negative impacts where \nstructured human feedback exercises, e.g., GAI red-teaming, would be most \nbene\ufb01cial for GAI risk measurement and management based on the context of \nuse. \nHarmful Bias and \nHomogenization; CBRN \nInformation or Capabilities \nMS-1.1-009 \nTrack and document risks or opportunities related to all GAI risks that cannot be", "387ad6e8-23b6-4ce6-9cde-cb2521d5aaf2": "those laws beyond providing them as examples, where appropriate, of existing protective measures. This \nframework instead shares a broad, forward-leaning vision of recommended principles for automated system \ndevelopment and use to inform private and public involvement with these systems where they have the poten\u00ad\ntial to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze or \ntake a position on legislative and regulatory proposals in municipal, state, and federal government, or those in \nother countries. \nWe have seen modest progress in recent years, with some state and local governments responding to these prob\u00ad\nlems with legislation, and some courts extending longstanding statutory protections to new and emerging tech\u00ad\nnologies. There are companies working to incorporate additional protections in their design and use of auto\u00ad\nmated systems, and researchers developing innovative guardrails. Advocates, researchers, and government", "154dc934-1306-4889-9480-007b039c9eea": "harm, such as the ideation and design of novel harmful chemical or biological agents.  \nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing \nassessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN \nweapons planning and GAI systems\u2019 connection or access to relevant data and tools. \nTrustworthy AI Characteristic: Safe, Explainable and Interpretable", "caff7dcf-9dab-4f12-bd2e-9b5d2ae3b3e4": "system development. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nGV-5.1-002 \nDocument interactions with GAI systems to users prior to interactive activities, \nparticularly in contexts involving more signi\ufb01cant risks.  \nHuman-AI Con\ufb01guration; \nConfabulation \nAI Actor Tasks: AI Design, AI Impact Assessment, A\ufb00ected Individuals and Communities, Governance and Oversight \n \nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of \ninfringement of a third-party\u2019s intellectual property or other rights. \nAction ID \nSuggested Action \nGAI Risks \nGV-6.1-001 Categorize di\ufb00erent types of GAI content with associated third-party rights (e.g., \ncopyright, intellectual property, data privacy). \nData Privacy; Intellectual \nProperty; Value Chain and \nComponent Integration \nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \nto promote best practices for managing GAI risks.", "9ba2e19d-4746-40b3-99f5-3f19866e4ea9": "programs; or, \nAccess to critical resources or services, such as healthcare, financial services, safety, social services, \nnon-deceptive information about goods and services, and government benefits. \nA list of examples of automated systems for which these principles should be considered is provided in the \nAppendix. The Technical Companion, which follows, offers supportive guidance for any person or entity that \ncreates, deploys, or oversees automated systems. \nConsidered together, the five principles and associated practices of the Blueprint for an AI Bill of \nRights form an overlapping set of backstops against potential harms. This purposefully overlapping \nframework, when taken as a whole, forms a blueprint to help protect the public from harm. \nThe measures taken to realize the vision set forward in this framework should be proportionate \nwith the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and \naccess.", "3391ae93-5428-4257-b288-9618026301c1": "secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\n34. Todd Feathers. Major Universities Are Using Race as a \u201cHigh Impact Predictor\u201d of Student Success:\nStudents, professors, and education experts worry that that\u2019s pushing Black students in particular out of math\nand science. The Markup. Mar. 2, 2021. https://themarkup.org/machine-learning/2021/03/02/major\u00ad\nuniversities-are-using-race-as-a-high-impact-predictor-of-student-success\n65", "df0b5bde-fa5f-481b-b78b-39b3278d9b8c": "GAI Risks \nGV-1.1-001 Align GAI development and use with applicable laws and regulations, including \nthose related to data privacy, copyright and intellectual property law. \nData Privacy; Harmful Bias and \nHomogenization; Intellectual \nProperty \nAI Actor Tasks: Governance and Oversight \n \n \n \n14 AI Actors are de\ufb01ned by the OECD as \u201cthose who play an active role in the AI system lifecycle, including \norganizations and individuals that deploy or operate AI.\u201d See Appendix A of the AI RMF for additional descriptions \nof AI Actors and AI Actor Tasks.", "f545635d-4965-40fc-a76b-eeffe6071572": "error rates (overall and per demographic group), and comparisons to previously deployed systems; \nongoing monitoring procedures and regular performance testing reports, including monitoring frequency, \nresults, and actions taken; and the procedures for and results from independent evaluations. Reporting \nshould be provided in a plain language and machine-readable manner. \n20", "6aceaff5-a98f-4f75-a77d-31db510abc00": "MP = Map; MS = Measure; MG = Manage. \n\u2022 \nSuggested Action: Steps an organization or AI actor can take to manage GAI risks.  \n\u2022 \nGAI Risks: Tags linking suggested actions with relevant GAI risks.  \n\u2022 \nAI Actor Tasks: Pertinent AI Actor Tasks for each subcategory. Not every AI Actor Task listed will \napply to every suggested action in the subcategory (i.e., some apply to AI development and \nothers apply to AI deployment).  \nThe tables below begin with the AI RMF subcategory, shaded in blue, followed by suggested actions.  \n \nGOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.  \nAction ID \nSuggested Action \nGAI Risks \nGV-1.1-001 Align GAI development and use with applicable laws and regulations, including \nthose related to data privacy, copyright and intellectual property law. \nData Privacy; Harmful Bias and \nHomogenization; Intellectual \nProperty \nAI Actor Tasks: Governance and Oversight", "5900ea84-9228-4be0-84d2-e6db99c066ca": "AI system, and a\ufb00ected communities are consulted in support of assessments as necessary per organizational risk tolerance. \nAction ID \nSuggested Action \nGAI Risks \nMS-1.3-001 \nDe\ufb01ne relevant groups of interest (e.g., demographic groups, subject matter \nexperts, experience with GAI technology) within the context of use as part of \nplans for gathering structured public feedback. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization; CBRN \nInformation or Capabilities \nMS-1.3-002 \nEngage in internal and external evaluations, GAI red-teaming, impact \nassessments, or other structured human feedback exercises in consultation \nwith representative AI Actors with expertise and familiarity in the context of \nuse, and/or who are representative of the populations associated with the \ncontext of use. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization; CBRN \nInformation or Capabilities \nMS-1.3-003 \nVerify those conducting structured human feedback exercises are not directly", "f8ca5d13-11ae-4365-addf-efc69e24cf1e": "47 \nAppendix A. Primary GAI Considerations \nThe following primary considerations were derived as overarching themes from the GAI PWG \nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \ncontributions from the community and NIST sta\ufb00 GAI PWG leads: George Awad, Luca Belli, Harold Booth, \nMat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \nA.1. Governance \nA.1.1. Overview \nLike any other technology system, governance principles and techniques can be used to manage risks", "0a7cd993-1129-47e1-ad83-0e5f80519812": "legal protections. Throughout this framework the term \u201calgorithmic discrimination\u201d takes this meaning (and \nnot a technical understanding of discrimination as distinguishing between items). \nAUTOMATED SYSTEM: An \"automated system\" is any system, software, or process that uses computation as \nwhole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect \ndata or observations, or otherwise interact with individuals and/or communities. Automated systems \ninclude, but are not limited to, systems derived from machine learning, statistics, or other data processing \nor artificial intelligence techniques, and exclude passive computing infrastructure. \u201cPassive computing \ninfrastructure\u201d is any intermediary technology that does not influence or determine the outcome of decision, \nmake or aid in decisions, inform policy implementation, or collect data or observations, including web", "a24a5ca9-e065-430e-9dae-cedb96e6df30": "termined by an automated system, including when the automated \nsystem is not the sole input determining the outcome. Automated \nsystems should provide explanations that are technically valid, \nmeaningful and useful to you and to any operators or others who \nneed to understand the system, and calibrated to the level of risk \nbased on the context. Reporting that includes summary information \nabout these automated systems in plain language and assessments of \nthe clarity and quality of the notice and explanations should be made \npublic whenever possible.   \nNOTICE AND EXPLANATION\n40", "06d411ca-fb6d-4e32-9320-9bc6d2c06048": "19 \nGV-4.1-003 \nEstablish policies, procedures, and processes for oversight functions (e.g., senior \nleadership, legal, compliance, including internal evaluation) across the GAI \nlifecycle, from problem formulation and supply chains to system decommission. \nValue Chain and Component \nIntegration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring \n \nGOVERN 4.2: Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, \nevaluate, and use, and they communicate about the impacts more broadly. \nAction ID \nSuggested Action \nGAI Risks \nGV-4.2-001 \nEstablish terms of use and terms of service for GAI systems. \nIntellectual Property; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nGV-4.2-002 \nInclude relevant AI Actors in the GAI system risk identi\ufb01cation process. \nHuman-AI Con\ufb01guration \nGV-4.2-003", "bc07b9e9-3f25-4539-a7da-a7839e0f48d3": "27. National Science Foundation. Designing Accountable Software Systems. Accessed Sept. 12, 2022.\nhttps://beta.nsf.gov/funding/opportunities/designing-accountable-software-systems-dass\n28. The Leadership Conference Education Fund. The Use Of Pretrial \u201cRisk Assessment\u201d Instruments: A\nShared Statement Of Civil Rights Concerns. Jul. 30, 2018. http://civilrightsdocs.info/pdf/criminal-justice/\nPretrial-Risk-Assessment-Short.pdf; https://civilrights.org/edfund/pretrial-risk-assessments/\n29. Idaho Legislature. House Bill 118. Jul. 1, 2019. https://legislature.idaho.gov/sessioninfo/2019/\nlegislation/H0118/\n30. See, e.g., Executive Office of the President. Big Data: A Report on Algorithmic Systems, Opportunity, and\nCivil Rights. May, 2016. https://obamawhitehouse.archives.gov/sites/default/files/microsites/\nostp/2016_0504_data_discrimination.pdf; Cathy O\u2019Neil. Weapons of Math Destruction. Penguin Books.\n2017. https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction; Ruha Benjamin. Race After", "0f74ef24-2e50-40b4-a6ba-574538c53e40": "exchange, and consumption in society.\u201d High-integrity information can be trusted; \u201cdistinguishes fact \nfrom \ufb01ction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of \nvetting. This information can be linked to the original source(s) with appropriate evidence. High-integrity \ninformation is also accurate and reliable, can be veri\ufb01ed and authenticated, has a clear chain of custody, \nand creates reasonable expectations about when its validity may expire.\u201d11 \n \n \n11 This de\ufb01nition of information integrity is derived from the 2022 White House Roadmap for Researchers on \nPriorities Related to Information Integrity Research and Development.", "b003ea70-13e5-4690-8143-f4789ceb0753": "under-ecoa-fcra/\n91. Federal Trade Commission. Using Consumer Reports for Credit Decisions: What to Know About\nAdverse Action and Risk-Based Pricing Notices. Accessed May 2, 2022.\nhttps://www.ftc.gov/business-guidance/resources/using-consumer-reports-credit-decisions-what\u00ad\nknow-about-adverse-action-risk-based-pricing-notices#risk\n92. Consumer Financial Protection Bureau. CFPB Acts to Protect the Public from Black-Box Credit\nModels Using Complex Algorithms. May 26, 2022.\nhttps://www.consumerfinance.gov/about-us/newsroom/cfpb-acts-to-protect-the-public-from-black\u00ad\nbox-credit-models-using-complex-algorithms/\n93. Anthony Zaller. California Passes Law Regulating Quotas In Warehouses \u2013 What Employers Need to\nKnow About AB 701. Zaller Law Group California Employment Law Report. Sept. 24, 2021.\nhttps://www.californiaemploymentlawreport.com/2021/09/california-passes-law-regulating-quotas\u00ad\nin-warehouses-what-employers-need-to-know-about-ab-701/", "166837f6-7c33-474c-b181-1eacd0e2b333": "ensure that all people are treated fairly when automated systems are used. This includes all dimensions of their \nlives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal \njustice system. Ensuring equity should also go beyond existing guardrails to consider the holistic impact that \nautomated systems make on underserved communities and to institute proactive protections that support these \ncommunities. \n\u2022\nAn automated system using nontraditional factors such as educational attainment and employment history as\npart of its loan underwriting and pricing model was found to be much more likely to charge an applicant who\nattended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loan\nthan an applicant who did not attend an HBCU. This was found to be true even when controlling for\nother credit-related factors.32\n\u2022", "b4319c4f-c8cb-4cf8-b82a-ce1dc3b6674f": "65. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info\nAppears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://\nwww.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles\u00ad\nin-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server. WIRED,\nNov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/\n66. Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash. New York Times.\nSept. 24, 2019.\nhttps://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html\n67. Jo Constantz. \u2018They Were Spying On Us\u2019: Amazon, Walmart, Use Surveillance Technology to Bust\nUnions. Newsweek. Dec. 13, 2021.\nhttps://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust\u00ad\nunions-1658603\n68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum", "76cc6c42-482a-400c-b774-5f3e35cb6e43": "Speech-related systems such as automated content moderation tools; \nSurveillance and criminal justice system algorithms such as risk assessments, predictive  \n    policing, automated license plate readers, real-time facial recognition systems (especially  \n    those used in public places or during protected activities like peaceful protests), social media  \n    monitoring, and ankle monitoring devices; \nVoting-related systems such as signature matching tools; \nSystems with a potential privacy impact such as smart home systems and associated data,  \n    systems that use or collect health-related data, systems that use or collect education-related  \n    data, criminal justice system data, ad-targeting systems, and systems that perform big data  \n    analytics in order to build profiles or infer personal information about individuals; and \nAny system that has the meaningful potential to lead to algorithmic discrimination. \n\u2022 Equal opportunities, including but not limited to:", "5d4fc094-05d4-43a8-8d0c-69c5d75def5f": "identi\ufb01able information (PII) to prevent potential harm or misuse. \nData Privacy; Human AI \nCon\ufb01guration; Information \nIntegrity; Information Security; \nDangerous, Violent, or Hateful \nContent \nMS-2.2-003 Provide human subjects with options to withdraw participation or revoke their \nconsent for present or future use of their data in GAI applications.  \nData Privacy; Human-AI \nCon\ufb01guration; Information \nIntegrity \nMS-2.2-004 \nUse techniques such as anonymization, di\ufb00erential privacy or other privacy-\nenhancing technologies to minimize the risks associated with linking AI-generated \ncontent back to individual human subjects. \nData Privacy; Human-AI \nCon\ufb01guration \nAI Actor Tasks: AI Development, Human Factors, TEVV \n \nMEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for \nconditions similar to deployment setting(s). Measures are documented. \nAction ID \nSuggested Action \nGAI Risks", "7206bfee-b9f3-4fa8-aadc-6a42946235aa": "varying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management e\ufb00orts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n\u2022 \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been \nprovided instructions and material to complete tasks which may elicit harmful model behaviors. \nThis type of exercise can be more e\ufb00ective with large groups of AI red-teamers. \n\u2022", "360643f0-1054-4e87-b8e6-16eaec2c4656": "and effectiveness should be made public whenever possible. \nDefinitions for key terms in The Blueprint for an AI Bill of Rights can be found in Applying the Blueprint for an AI Bill of Rights. \nAccompanying analysis and tools for actualizing each principle can be found in the Technical Companion. \n7", "d7e77db6-97e6-4bac-a8b6-f8cd9b34e4c3": "\u2022\nZiad Obermeyer, Blue Cross of California Distinguished Associate Professor of Policy and Management,\nUniversity of California, Berkeley School of Public Health\n\u2022\nDorothy Roberts, George A. Weiss University Professor of Law and Sociology and the Raymond Pace and\nSadie Tanner Mossell Alexander Professor of Civil Rights, University of Pennsylvania\n\u2022\nDavid Jones, A. Bernard Ackerman Professor of the Culture of Medicine, Harvard University\n\u2022\nJamila Michener, Associate Professor of Government, Cornell University; Co-Director, Cornell Center for\nHealth Equity\u00ad\nPanelists discussed the impact of new technologies on health disparities; healthcare access, delivery, and \noutcomes; and areas ripe for research and policymaking. Panelists discussed the increasing importance of tech-\nnology as both a vehicle to deliver healthcare and a tool to enhance the quality of care. On the issue of \ndelivery, various panelists pointed to a number of concerns including access to and expense of broadband", "c3fa4d00-1477-4d58-ba2a-aa4d63da2874": "under which the GAI system will be deployed. Field style tests can be adapted from a focus on user \npreferences and experiences towards AI risks and impacts \u2013 both negative and positive. When carried \nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \nin real world interactions. \nOrganizations may also collect feedback on outcomes, harms, and user experience directly from users in \nthe production environment after a model has been released, in accordance with human subject \nstandards such as informed consent and compensation. Organizations should follow applicable human \nsubjects research requirements, and best practices such as informed consent and subject compensation, \nwhen implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse", "2e893474-517e-42f5-9396-c5fe13350e3c": "SAFE AND EFFECTIVE SYSTEMS \nYou should be protected from unsafe or ineffective sys\u00ad\ntems. Automated systems should be developed with consultation \nfrom diverse communities, stakeholders, and domain experts to iden\u00ad\ntify concerns, risks, and potential impacts of the system. Systems \nshould undergo pre-deployment testing, risk identification and miti\u00ad\ngation, and ongoing monitoring that demonstrate they are safe and \neffective based on their intended use, mitigation of unsafe outcomes \nincluding those beyond the intended use, and adherence to do\u00ad\nmain-specific standards. Outcomes of these protective measures \nshould include the possibility of not deploying the system or remov\u00ad\ning a system from use. Automated systems should not be designed \nwith an intent or reasonably foreseeable possibility of endangering \nyour safety or the safety of your community. They should be designed \nto proactively protect you from harms stemming from unintended,", "fee1b245-5983-4196-9032-5872c7f1aa1b": "RAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\nchinese.html \nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika. \nResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\nAI_Companionship_A_Dialectical_Inquiry_into_Replika \nDahl, M. et al. (2024) Large Legal Fictions: Pro\ufb01ling Legal Hallucinations in Large Language Models. arXiv. \nhttps://arxiv.org/abs/2401.01301", "6a55e406-8570-4cc4-b9aa-cdc3cb8582a7": "SECTION TITLE\nApplying The Blueprint for an AI Bill of Rights \nWhile many of the concerns addressed in this framework derive from the use of AI, the technical \ncapabilities and specific definitions of such systems change with the speed of innovation, and the potential \nharms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a two-\npart test to determine what systems are in scope. This framework applies to (1) automated systems that (2) \nhave the potential to meaningfully impact the American public\u2019s rights, opportunities, or access to \ncritical resources or services. These rights, opportunities, and access to critical resources of services should \nbe enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in \nour lives. \nThis framework describes protections that should be applied with respect to all automated systems that", "750f05fd-2595-4d49-8575-a01f481a154a": "reuse \nRelevant and high-quality data. Data used as part of any automated system\u2019s creation, evaluation, or \ndeployment should be relevant, of high quality, and tailored to the task at hand. Relevancy should be \nestablished based on research-backed demonstration of the causal influence of the data to the specific use case \nor justified more generally based on a reasonable expectation of usefulness in the domain and/or for the \nsystem design or ongoing development. Relevance of data should not be established solely by appealing to \nits historical connection to the outcome. High quality and tailored data should be representative of the task at \nhand and errors from data entry or other sources should be measured and limited. Any data used as the target \nof a prediction process should receive particular attention to the quality and validity of the predicted outcome \nor label to ensure the goal of the automated system is appropriately identified and measured. Additionally,", "6858f000-a5e0-4dde-a4ad-cee9e9542fac": "calls or otherwise respond to complicated requests. Using both AI and human agents is viewed as key to \nsuccessful customer service.109\nBallot curing laws in at least 24 states require a fallback system that allows voters to \ncorrect their ballot and have it counted in the case that a voter signature matching \nalgorithm incorrectly flags their ballot as invalid or there is another issue with their \nballot, and review by an election official does not rectify the problem. Some federal \ncourts have found that such cure procedures are constitutionally required.110 \nBallot \ncuring processes vary among states, and include direct phone calls, emails, or mail contact by election \nofficials.111 Voters are asked to provide alternative information or a new signature to verify the validity of their \nballot. \n52", "502bb66e-388d-47a7-91e9-46b3e2a55821": "38 \nMEASURE 2.13: E\ufb00ectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.13-001 \nCreate measurement error models for pre-deployment metrics to demonstrate \nconstruct validity for each metric (i.e., does the metric e\ufb00ectively operationalize \nthe desired concept): Measure or estimate, and document, biases or statistical \nvariance in applied metrics or structured human feedback processes; Leverage \ndomain expertise when modeling complex societal constructs such as hateful \ncontent. \nConfabulation; Information \nIntegrity; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV \n \nMEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are di\ufb03cult to assess using currently available \nmeasurement techniques or where metrics are not yet available. \nAction ID \nSuggested Action \nGAI Risks \nMS-3.2-001", "021aca5f-0cb1-467f-9604-2f8f144b92f6": "109. Mike Hughes. Are We Getting The Best Out Of Our Bots? Co-Intelligence Between Robots &\nHumans. Forbes. Jul. 14, 2022.\nhttps://www.forbes.com/sites/mikehughes1/2022/07/14/are-we-getting-the-best-out-of-our-bots-co\u00ad\nintelligence-between-robots--humans/?sh=16a2bd207395\n110. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020. https://\nbipartisanpolicy.org/blog/the-low-down-on-ballot-curing/; Zahavah Levine and Thea Raymond-\nSeidel. Mail Voting Litigation in 2020, Part IV: Verifying Mail Ballots. Oct. 29, 2020.\nhttps://www.lawfareblog.com/mail-voting-litigation-2020-part-iv-verifying-mail-ballots\n111. National Conference of State Legislatures. Table 15: States With Signature Cure Processes. Jan. 18,\n2022.\nhttps://www.ncsl.org/research/elections-and-campaigns/vopp-table-15-states-that-permit-voters-to\u00ad\ncorrect-signature-discrepancies.aspx\n112. White House Office of Science and Technology Policy. Join the Effort to Create A Bill of Rights for", "c0da6c6b-360a-4eaf-8328-b44ac0df7031": "tips to employers on how to comply with the ADA, and to job applicants and employees who think that their \nrights may have been violated. \nDisparity assessments identified harms to Black patients' healthcare access. A widely \nused healthcare algorithm relied on the cost of each patient\u2019s past medical care to predict future medical needs, \nrecommending early interventions for the patients deemed most at risk. This process discriminated \nagainst Black patients, who generally have less access to medical care and therefore have generated less cost \nthan white patients with similar illness and need. A landmark study documented this pattern and proposed \npractical ways that were shown to reduce this bias, such as focusing specifically on active chronic health \nconditions or avoidable future costs related to emergency visits and hospitalization.54 \nLarge employers have developed best practices to scrutinize the data and models used", "1d4207c7-9112-42df-b75b-4d3d1d4c7ebf": "\u201cjailbreaking,\u201d or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be \nharmful or dangerous in certain contexts. Studies have observed that users may disclose mental health \nissues in conversations with chatbots \u2013 and that users exhibit negative reactions to unhelpful responses \nfrom these chatbots during situations of distress. \nThis risk encompasses di\ufb03culty controlling creation of and public exposure to o\ufb00ensive or hateful \nlanguage, and denigrating or stereotypical content generated by AI. This kind of speech may contribute \nto downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or \nstereotypical content can also further exacerbate representational harms (see Harmful Bias and \nHomogenization below).  \nTrustworthy AI Characteristics: Safe, Secure and Resilient \n2.4. Data Privacy \nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in", "6357e678-7a6c-4626-9522-1afff995587c": "and Homogenization \nMS-2.8-002 Document the instructions given to data annotators or AI red-teamers. \nHuman-AI Con\ufb01guration \nMS-2.8-003 \nUse digital content transparency solutions to enable the documentation of each \ninstance where content is generated, modi\ufb01ed, or shared to provide a tamper-\nproof history of the content, promote transparency, and enable traceability. \nRobust version control systems can also be applied to track changes across the AI \nlifecycle over time. \nInformation Integrity \nMS-2.8-004 Verify adequacy of GAI system user instructions through user testing. \nHuman-AI Con\ufb01guration \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV", "d69a269b-4ac6-4c34-9206-6f0c8687f8da": "55 \nDe Angelo, D. (2024) Short, Mid and Long-Term Impacts of AI in Cybersecurity. Palo Alto Networks. \nhttps://www.paloaltonetworks.com/blog/2024/02/impacts-of-ai-in-cybersecurity/ \nDe Freitas, J. et al. (2023) Chatbots and Mental Health: Insights into the Safety of Generative AI. Harvard \nBusiness School. https://www.hbs.edu/ris/Publication%20Files/23-011_c1bdd417-f717-47b6-bccb-\n5438c6e65c1a_f6fd9798-3c2d-4932-b222-056231fe69d7.pdf \nDietvorst, B. et al. (2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them \nErr. Journal of Experimental Psychology. https://marketing.wharton.upenn.edu/wp-\ncontent/uploads/2016/10/Dietvorst-Simmons-Massey-2014.pdf \nDuhigg, C. (2012) How Companies Learn Your Secrets. New York Times. \nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html \nElsayed, G. et al. (2024) Images altered to trick machine vision can in\ufb02uence humans too. Google", "025b0f84-4bda-439a-9539-cc2b7bbb3dc0": "CBRN Information or Capabilities; \nInformation Security \nAI Actor Tasks: Governance and Oversight \n \nGOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based \non the organization\u2019s risk tolerance. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.3-001 \nConsider the following factors when updating or de\ufb01ning risk tiers for GAI: Abuses \nand impacts to information integrity; Dependencies between GAI and other IT or \ndata systems; Harm to fundamental rights or public safety; Presentation of \nobscene, objectionable, o\ufb00ensive, discriminatory, invalid or untruthful output; \nPsychological impacts to humans (e.g., anthropomorphization, algorithmic \naversion, emotional entanglement); Possibility for malicious use; Whether the \nsystem introduces signi\ufb01cant new security vulnerabilities; Anticipated system \nimpact on some groups compared to others; Unreliable decision making", "4a35a5e6-0a1d-4d9c-9842-2e5577501b3f": "Information Technology Industry Council (2024) Authenticating AI-Generated Content. \nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \nhttps://arxiv.org/pdf/2305.08157 \nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \nArticle 248. https://doi.org/10.1145/3571730 \nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/ \nKalai, A., et al. (2024) Calibrated Language Models Must Hallucinate. arXiv. \nhttps://arxiv.org/pdf/2311.14648", "76618734-7341-481d-89aa-5ee17efbdffc": "GV-6.2-004 \nEstablish policies and procedures for continuous monitoring of third-party GAI \nsystems in deployment. \nValue Chain and Component \nIntegration \nGV-6.2-005 \nEstablish policies and procedures that address GAI data redundancy, including \nmodel weights and other system artifacts. \nHarmful Bias and Homogenization \nGV-6.2-006 \nEstablish policies and procedures to test and manage risks related to rollover and \nfallback technologies for GAI systems, acknowledging that rollover and fallback \nmay include manual processing. \nInformation Integrity \nGV-6.2-007 \nReview vendor contracts and avoid arbitrary or capricious termination of critical \nGAI technologies or vendor services and non-standard terms that may amplify or \ndefer liability in unexpected ways and/or contribute to unauthorized data \ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \nassignment of liability and responsibility for incidents, GAI system changes over", "04f91fb1-a85c-4185-8d50-4d112ba275ba": "companion\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including \ndetailed steps toward actualizing these principles in the technological design process. These principles help \nprovide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, \nor access to critical needs. \n3", "886d66dd-2bea-4c11-b7e7-38e5dd45eff1": "while simultaneously enhancing the security effectiveness capabilities of the existing technology. \n\u2022\nThe National Disabled Law Students Association expressed concerns that individuals with disabilities were\nmore likely to be flagged as potentially suspicious by remote proctoring AI systems because of their disabili-\nty-specific access needs such as needing longer breaks or using screen readers or dictation software.45 \n\u2022\nAn algorithm designed to identify patients with high needs for healthcare systematically assigned lower\nscores (indicating that they were not as high need) to Black patients than to those of white patients, even\nwhen those patients had similar numbers of chronic conditions and other markers of health.46 In addition,\nhealthcare clinical algorithms that are used by physicians to guide clinical decisions may include\nsociodemographic variables that adjust or \u201ccorrect\u201d the algorithm\u2019s output on the basis of a patient\u2019s race or", "04b0d553-c4a7-4ecc-b58d-39204c37f0bb": "understanding of the algorithmic systems. The interventions and key needs various panelists put forward as \nnecessary to the future design of critical AI systems included ongoing transparency, value sensitive and \nparticipatory design, explanations designed for relevant stakeholders, and public consultation. \nVarious \npanelists emphasized the importance of placing trust in people, not technologies, and in engaging with \nimpacted communities to understand the potential harms of technologies and build protection by design into \nfuture systems. \nPanel 5: Social Welfare and Development. This event explored current and emerging uses of technology to \nimplement or improve social welfare systems, social development programs, and other systems that can impact \nlife chances. \nWelcome:\n\u2022\nSuresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\nand Technology Policy\n\u2022\nAnne-Marie Slaughter, CEO, New America", "868564eb-83b7-4d7a-92f8-c81a68e3e492": "while being impacted by the technology. An explanation should be available with the decision itself, or soon \nthereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use case \nor key functionality changes. \nBrief and clear. Notices and explanations should be assessed, such as by research on users\u2019 experiences, \nincluding user testing, to ensure that the people using or impacted by the automated system are able to easily \nfind notices and explanations, read them quickly, and understand and act on them. This includes ensuring that \nnotices and explanations are accessible to users with disabilities and are available in the language(s) and read-\ning level appropriate for the audience. Notices and explanations may need to be available in multiple forms, \n(e.g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the \nAmerican public.", "a92b242a-d93e-402a-8a75-98d71fff252e": "information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \ninto applications involving consequential decision making. \nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \nsystem\u2019s answer, which may further mislead humans into inappropriately trusting the system\u2019s output. \nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \npotentially deceiving humans into believing they are speaking with another human. \nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide", "5d049b4e-1464-406c-bb1f-9f703f4b2a15": "tion processes that may be applied when considering the use of new automated systems, and existing product develop\u00ad\nment and testing practices already protect the American public from many potential harms. \nStill, these kinds of practices are deployed too rarely and unevenly. Expanded, proactive protections could build on \nthese existing practices, increase confidence in the use of automated systems, and protect the American public. Inno\u00ad\nvators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protections \nfrom unsafe outcomes. All can benefit from assurances that automated systems will be designed, tested, and consis\u00ad\ntently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harm\u00ad\nful outcomes. \n\u2022\nA proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was imple\u00ad", "04ae0eca-8e5c-4ff0-b76e-cc2fd828b0d8": "Provenance Data Tracking Approaches \nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for \ndigital content, allowing its authenticity to be determined. It consists of techniques to record metadata \nas well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin \nand history of input data through metadata and digital watermarking techniques. Provenance data \ntracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or \ncontrol over the various trade-o\ufb00s and cascading impacts of early-stage model decisions on downstream \nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational", "91078e11-ca6a-448f-82cd-bf6d8aa29a22": "necessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating mea\u00ad\nsures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. This \nincludes (potentially private and protected) meaningful access to source code, documentation, and related \ndata during any associated legal discovery, subject to effective confidentiality or court orders. Such meaning\u00ad\nful access should include (but is not limited to) adhering to the principle on Notice and Explanation using the \nhighest level of risk so the system is designed with built-in explanations; such systems should use fully-trans\u00ad\nparent models where the model itself can be understood by people needing to directly examine it. \nDemonstrate access to human alternatives, consideration, and fallback \nReporting. Reporting should include an assessment of timeliness and the extent of additional burden for", "48632257-c296-4742-8721-b02ea821234f": "generate the synthetic training data. \nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Valid and Reliable \n2.7. Human-AI Con\ufb01guration \nGAI system use can involve varying risks of miscon\ufb01gurations and poor interactions between a system \nand a human who is interacting with it. Humans bring their unique perspectives, experiences, or domain-\nspeci\ufb01c expertise to interactions with AI systems but may not have detailed knowledge of AI systems and \nhow they work. As a result, human experts may be unnecessarily \u201caverse\u201d to GAI systems, and thus \ndeprive themselves or others of GAI\u2019s bene\ufb01cial uses.  \nConversely, due to the complexity and increasing reliability of GAI technology, over time, humans may \nover-rely on GAI systems or may unjusti\ufb01ably perceive GAI content to be of higher quality than that \nproduced by other sources. This phenomenon is an example of automation bias, or excessive deference", "016a132a-10f7-42d3-ae2b-1d54ff9a949a": "the 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could \n\u201ctrain and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensive \nhealth coverage.\u201d107\nThe customer service industry has successfully integrated automated services such as \nchat-bots and AI-driven call response systems with escalation to a human support \nteam.108 Many businesses now use partially automated customer service platforms that help answer customer \nquestions and compile common problems for human agents to review. These integrated human-AI \nsystems allow companies to provide faster customer care while maintaining human agents to answer \ncalls or otherwise respond to complicated requests. Using both AI and human agents is viewed as key to \nsuccessful customer service.109\nBallot curing laws in at least 24 states require a fallback system that allows voters to", "b0332b22-b3f6-4b6a-84ac-da01dc01e83a": "techniques such as: application of gradient-based attributions, occlusion/term \nreduction, counterfactual prompts and prompt engineering, and analysis of \nembeddings; Assess and update risk measurement approaches at regular \ncadences. \nConfabulation \nGV-4.1-002 \nEstablish policies, procedures, and processes detailing risk measurement in \ncontext of use with standardized measurement protocols and structured public \nfeedback exercises such as AI red-teaming or independent external evaluations. \nCBRN Information and Capability; \nValue Chain and Component \nIntegration", "b61b34cd-005d-42c3-9759-133ef46ad79d": "particularly relevant to automated systems, without articulating a specific set of FIPPs or scoping \napplicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, \nethics, or risk management. The Technical Companion builds on this prior work to provide practical next \nsteps to move these principles into practice and promote common approaches that allow technological \ninnovation to flourish while protecting people from harm. \n9", "c7db751b-67d3-43c5-8a49-8889cdb2f77d": "fails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and \nfallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and \nshould not impose an unreasonable burden on the public. Automated systems with an intended use within sensi\u00ad\ntive domains, including, but not limited to, criminal justice, employment, education, and health, should additional\u00ad\nly be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting \nwith the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes \na description of these human governance processes and assessment of their timeliness, accessibility, outcomes, \nand effectiveness should be made public whenever possible. \nDefinitions for key terms in The Blueprint for an AI Bill of Rights can be found in Applying the Blueprint for an AI Bill of Rights.", "7bc11834-b5d6-4fbf-9ea1-0de1c6240a04": "individual, unless written notice is provided to that individual or their legally appointed representative. 87\nMajor technology companies are piloting new ways to communicate with the public about \ntheir automated technologies. For example, a collection of non-profit organizations and companies have \nworked together to develop a framework that defines operational approaches to transparency for machine \nlearning systems.88 This framework, and others like it,89 inform the public about the use of these tools, going \nbeyond simple notice to include reporting elements such as safety evaluations, disparity assessments, and \nexplanations of how the systems work. \nLenders are required by federal law to notify consumers about certain decisions made about \nthem. Both the Fair Credit Reporting Act and the Equal Credit Opportunity Act require in certain circumstances \nthat consumers who are denied credit receive \"adverse action\" notices. Anyone who relies on the information in a", "85bdf9b8-2dff-42b0-8d54-c3607ecce62e": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and\u00ad\npolicies-to-reduce-consumer-harms/; Andrew D. Selbst. An Institutional View Of Algorithmic Impact\nAssessments. Harvard Journal of Law & Technology. June 15, 2021. https://ssrn.com/abstract=3867634;\nDillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker. Algorithmic Impact\nAssessments: A Practical Framework for Public Agency Accountability. AI Now Institute Report. April\n2018. https://ainowinstitute.org/aiareport2018.pdf\n51. Department of Justice. Justice Department Announces New Initiative to Combat Redlining. Oct. 22,\n2021. https://www.justice.gov/opa/pr/justice-department-announces-new-initiative-combat-redlining\n52. PAVE Interagency Task Force on Property Appraisal and Valuation Equity. Action Plan to Advance\nProperty Appraisal and Valuation Equity: Closing the Racial Wealth Gap by Addressing Mis-valuations for", "111970aa-c1b6-4e0a-8c55-eda7b544b561": "APPENDIX\nSummaries of Additional Engagements: \n\u2022 OSTP created an email address (ai-equity@ostp.eop.gov) to solicit comments from the public on the use of\nartificial intelligence and other data-driven technologies in their lives.\n\u2022 OSTP issued a Request For Information (RFI) on the use and governance of biometric technologies.113 The\npurpose of this RFI was to understand the extent and variety of biometric technologies in past, current, or\nplanned use; the domains in which these technologies are being used; the entities making use of them; current\nprinciples, practices, or policies governing their use; and the stakeholders that are, or may be, impacted by their\nuse or regulation. The 130 responses to this RFI are available in full online114 and were submitted by the below\nlisted organizations and individuals:\nAccenture \nAccess Now \nACT | The App Association \nAHIP \nAIethicist.org \nAirlines for America \nAlliance for Automotive Innovation \nAmelia Winger-Bearskin", "431d173e-91a7-4b23-b7e8-914341ca20bc": "20 \nGV-4.3-003 \nVerify information sharing and feedback mechanisms among individuals and \norganizations regarding any negative impact from GAI systems. \nInformation Integrity; Data \nPrivacy \nAI Actor Tasks: AI Impact Assessment, A\ufb00ected Individuals and Communities, Governance and Oversight \n \nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those \nexternal to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI \nrisks. \nAction ID \nSuggested Action \nGAI Risks \nGV-5.1-001 \nAllocate time and resources for outreach, feedback, and recourse processes in GAI \nsystem development. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nGV-5.1-002 \nDocument interactions with GAI systems to users prior to interactive activities, \nparticularly in contexts involving more signi\ufb01cant risks.  \nHuman-AI Con\ufb01guration; \nConfabulation", "6649095b-32ea-40e3-b58e-4eb92e592eac": "uncertainties, or could be leveraged for large-scale dis- and mis-information campaigns. \n9. Information Security: Lowered barriers for o\ufb00ensive cyber capabilities, including via automated \ndiscovery and exploitation of vulnerabilities to ease hacking, malware, phishing, o\ufb00ensive cyber \n \n \n6 Some commenters have noted that the terms \u201challucination\u201d and \u201cfabrication\u201d anthropomorphize GAI, which \nitself is a risk related to GAI systems as it can inappropriately attribute human characteristics to non-human \nentities.  \n7 What is categorized as sensitive data or sensitive PII can be highly contextual based on the nature of the \ninformation, but examples of sensitive information include information that relates to an information subject\u2019s \nmost intimate sphere, including political opinions, sex life, or criminal convictions.  \n8 The notion of harm presumes some baseline scenario that the harmful factor (e.g., a GAI model) makes worse.", "cf3b84ab-8a50-4440-afe0-965069050695": "opportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in the \nvalidity and reasonable use of automated systems. \n\u2022\nA lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home\nhealth-care assistance couldn't determine why, especially since the decision went against historical access\npractices. In a court hearing, the lawyer learned from a witness that the state in which the older client\nlived had recently adopted a new algorithm to determine eligibility.83 The lack of a timely explanation made it\nharder to understand and contest the decision.\n\u2022\nA formal child welfare investigation is opened against a parent based on an algorithm and without the parent\never being notified that data was being collected and used as part of an algorithmic child maltreatment\nrisk assessment.84 The lack of notice or an explanation makes it harder for those performing child", "59d73582-e851-4755-9688-b42d10c56fb9": "Action ID \nSuggested Action \nGAI Risks \nMG-2.4-001 \nEstablish and maintain communication plans to inform AI stakeholders as part of \nthe deactivation or disengagement process of a speci\ufb01c GAI system (including for \nopen-source models) or context of use, including reasons, workarounds, user \naccess removal, alternative processes, contact information, etc. \nHuman-AI Con\ufb01guration", "c0ae6e03-665d-4091-9523-1239ee306932": "framework, when taken as a whole, forms a blueprint to help protect the public from harm. \nThe measures taken to realize the vision set forward in this framework should be proportionate \nwith the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and \naccess. \nRELATIONSHIP TO EXISTING LAW AND POLICY\nThe Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is \nprotected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes princi\u00ad\nples that can help ensure these protections. Some of these protections are already required by the U.S. Constitu\u00ad\ntion or implemented under existing U.S. laws. For example, government surveillance, and data search and \nseizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for \nhuman review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws", "fe0c0e69-a7e4-44a4-9f1b-3fe04099a55d": "43. Arli Christian. Four Ways the TSA Is Making Flying Easier for Transgender People. American Civil\nLiberties Union. Apr. 5, 2022. https://www.aclu.org/news/lgbtq-rights/four-ways-the-tsa-is-making\u00ad\nflying-easier-for-transgender-people\n44. U.S. Transportation Security Administration. Transgender/ Non Binary / Gender Nonconforming\nPassengers. TSA. Accessed Apr. 21, 2022. https://www.tsa.gov/transgender-passengers\n45. See, e.g., National Disabled Law Students Association. Report on Concerns Regarding Online\nAdministration of Bar Exams. Jul. 29, 2020. https://ndlsa.org/wp-content/uploads/2020/08/\nNDLSA_Online-Exam-Concerns-Report1.pdf; Lydia X. Z. Brown. How Automated Test Proctoring\nSoftware Discriminates Against Disabled Students. Center for Democracy and Technology. Nov. 16, 2020.\nhttps://cdt.org/insights/how-automated-test-proctoring-software-discriminates-against-disabled\u00ad\nstudents/\n46. Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of", "10f2bffa-8222-4ffb-816e-1e66d4e6795d": "4 \n1. CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious \ninformation or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) \nweapons or other dangerous materials or agents. \n2. Confabulation: The production of con\ufb01dently stated but erroneous or false content (known \ncolloquially as \u201challucinations\u201d or \u201cfabrications\u201d) by which users may be misled or deceived.6 \n3. Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, \nradicalizing, or threatening content as well as recommendations to carry out self-harm or \nconduct illegal activities. Includes di\ufb03culty controlling public exposure to hateful and disparaging \nor stereotyping content. \n4. Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of \nbiometric, health, location, or other personally identi\ufb01able information or sensitive data.7", "4dcaa294-ebe7-4fc2-8e41-df0ee1110653": "DATA PRIVACY \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \nData privacy is a foundational and cross-cutting principle required for achieving all others in this framework. Surveil\u00ad\nlance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries, \nwith more and more companies tracking the behavior of the American public, building individual profiles based on \nthis data, and using this granular-level information as input into automated systems that further track, profile, and \nimpact the American public. Government agencies, particularly law enforcement agencies, also use and help develop \na variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input", "4efd92be-ce2a-43e7-a249-4cd926123b60": "system, and the review system should be staffed and regularly assessed to ensure it is providing timely \nconsideration and fallback. In time-critical systems, this mechanism should be immediately available or, \nwhere possible, available before the harm occurs. Time-critical systems include, but are not limited to, \nvoting-related systems, automated building access and other access systems, systems that form a critical \ncomponent of healthcare, and systems that have the ability to withhold wages or otherwise cause \nimmediate financial penalties. \nEffective. The organizational structure surrounding processes for consideration and fallback should \nbe designed so that if the human decision-maker charged with reassessing a decision determines that it \nshould be overruled, the new decision will be effectively enacted. This includes ensuring that the new \ndecision is entered into the automated system throughout its components, any previous repercussions from", "e185fa01-5783-411d-a733-7fdbbe1cac24": "16 \nGOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned, and \norganizational roles and responsibilities are clearly de\ufb01ned, including determining the frequency of periodic review. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.5-001 De\ufb01ne organizational responsibilities for periodic review of content provenance \nand incident monitoring for GAI systems. \nInformation Integrity \nGV-1.5-002 \nEstablish organizational policies and procedures for after action reviews of GAI \nsystem incident response and incident disclosures, to identify gaps; Update \nincident response and incident disclosure processes as required. \nHuman-AI Con\ufb01guration; \nInformation Security \nGV-1.5-003 \nMaintain a document retention policy to keep history for test, evaluation, \nvalidation, and veri\ufb01cation (TEVV), and digital content transparency methods for \nGAI. \nInformation Integrity; Intellectual \nProperty", "91713c6c-1e14-429f-8ac4-27231f0954e4": "and life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists \npointed out ways that data from one situation was misapplied in another in a way that limited people's \nopportunities, for example data from criminal justice settings or previous evictions being used to block further \naccess to housing. Throughout, various panelists emphasized that these technologies are being used to shift the \nburden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in \nways that diminish and encroach on equality of opportunity; assessment of these technologies should include \nwhether they are genuinely helpful in solving an identified problem. \nIn discussion of technical and governance interventions that that are needed to protect against the harms of \nthese technologies, panelists individually described the importance of: receiving community input into the", "9fe3429b-bc37-43a6-b8e0-fbd4697e2491": "of human involvement in attaining rights, opportunities, or access. When automated systems make up part of \nthe attainment process, alternative timely human-driven processes should be provided. The use of a human \nalternative should be triggered by an opt-out process. \nTimely and not burdensome human alternative. Opting out should be timely and not unreasonably \nburdensome in both the process of requesting to opt-out and the human-driven alternative provided. \nProvide timely human consideration and remedy by a fallback and escalation system in the \nevent that an automated system fails, produces error, or you would like to appeal or con\u00ad\ntest its impacts on you \nProportionate. The availability of human consideration and fallback, along with associated training and \nsafeguards against human bias, should be proportionate to the potential of the automated system to meaning\u00ad\nfully impact rights, opportunities, or access. Automated systems that have greater control over outcomes,", "110af4fc-3653-4469-ae10-bde7df1cb21b": "metrics and decommission or retrain pre-trained models that perform outside of \nde\ufb01ned limits. \nCBRN Information or Capabilities; \nConfabulation \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \n \nMANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating \ninput from users and other relevant AI Actors, appeal and override, decommissioning, incident response, recovery, and change \nmanagement. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.1-001 \nCollaborate with external researchers, industry experts, and community \nrepresentatives to maintain awareness of emerging best practices and \ntechnologies in measuring and managing identi\ufb01ed risks. \nInformation Integrity; Harmful Bias \nand Homogenization \nMG-4.1-002 \nEstablish, maintain, and evaluate e\ufb00ectiveness of organizational processes and \nprocedures for post-deployment monitoring of GAI systems, particularly for", "d9385984-3bf2-4263-95d8-2435b19faa5a": "The White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of \nArti\ufb01cial Intelligence. https://www.whitehouse.gov/brie\ufb01ng-room/presidential-\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-\narti\ufb01cial-intelligence/ \nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity \nResearch and Development. https://www.whitehouse.gov/wp-content/uploads/2022/12/Roadmap-\nInformation-Integrity-RD-2022.pdf? \nThiel, D. (2023) Investigation Finds AI Image Generation Models Trained on Child Abuse. Stanford Cyber \nPolicy Center. https://cyber.fsi.stanford.edu/news/investigation-\ufb01nds-ai-image-generation-models-\ntrained-child-abuse", "81e66c8f-0bc8-45fb-8372-ef88bfceedb0": "context, accessibility for people with disabilities, and societal goals to identify potential discrimination and \neffects on equity resulting from the introduction of the technology. The assessed groups should be as inclusive \nas possible of the underserved communities mentioned in the equity definition:  Black, Latino, and Indigenous \nand Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of \nreligious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and inter-\nsex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons \notherwise adversely affected by persistent poverty or inequality. Assessment could include both qualitative \nand quantitative evaluations of the system. This equity assessment should also be considered a core part of the \ngoals of the consultation conducted as part of the safety and efficacy review.", "1233349c-2328-4900-baac-ba5f4f6609d7": "in place, providing an important alternative to ensure access. Companies that have introduced automated call centers \noften retain the option of dialing zero to reach an operator. When automated identity controls are in place to board an \nairplane or enter the country, there is a person supervising the systems who can be turned to for help or to appeal a \nmisidentification. \nThe American people deserve the reassurance that such procedures are in place to protect their rights, opportunities, \nand access. People make mistakes, and a human alternative or fallback mechanism will not always have the right \nanswer, but they serve as an important check on the power and validity of automated systems. \n\u2022 An automated signature matching system is used as part of the voting process in many parts of the country to\ndetermine whether the signature on a mail-in ballot matches the signature on file. These signature matching", "a6cd5869-184b-41d9-877f-e38268815402": "information during GAI training and maintenance. \nHuman-AI Con\ufb01guration; Obscene, \nDegrading, and/or Abusive \nContent; Value Chain and \nComponent Integration; \nDangerous, Violent, or Hateful \nContent \nMS-2.6-002 \nAssess existence or levels of harmful bias, intellectual property infringement, \ndata privacy violations, obscenity, extremism, violence, or CBRN information in \nsystem training data. \nData Privacy; Intellectual Property; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content; CBRN \nInformation or Capabilities \nMS-2.6-003 Re-evaluate safety features of \ufb01ne-tuned models when the negative risk exceeds \norganizational risk tolerance. \nDangerous, Violent, or Hateful \nContent \nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \nassess risks that may arise from unreliable downstream decision-making. \nValue Chain and Component \nIntegration; Dangerous, Violent, or \nHateful Content", "682826c7-6b85-41f2-a7a6-92d00cb43934": "and/or use cases that were not evaluated in initial testing. \nValue Chain and Component \nIntegration \nMG-3.1-004 \nTake reasonable measures to review training data for CBRN information, and \nintellectual property, and where appropriate, remove it. Implement reasonable \nmeasures to prevent, \ufb02ag, or take other action in response to outputs that \nreproduce particular training data (e.g., plagiarized, trademarked, patented, \nlicensed content or trade secret material). \nIntellectual Property; CBRN \nInformation or Capabilities", "d86bc87b-31bc-4c02-9bb6-459a5abae8a8": "GOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be \nhigh-risk. \nAction ID \nSuggested Action \nGAI Risks \nGV-6.2-001 \nDocument GAI risks associated with system value chain to identify over-reliance \non third-party data and to identify fallbacks. \nValue Chain and Component \nIntegration \nGV-6.2-002 \nDocument incidents involving third-party GAI data and systems, including open-\ndata and open-source software. \nIntellectual Property; Value Chain \nand Component Integration", "f665614c-4198-4864-8530-d72d7e97d50e": "49 \nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \nrecommended \u201cpre-deployment testing\u201d practices to measure performance, capabilities, limits, risks, \nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \nand examines the state of play for pre-deployment testing methodologies.  \nLimitations of Current Pre-deployment Test Approaches \nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\nsystematically applied, or fail to re\ufb02ect or mismatched to deployment contexts. For example, the \nanecdotal testing of GAI system capabilities through video games or standardized tests designed for \nhumans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \nassess validity or reliability risks.", "b168e895-45c3-49d6-b925-ee2b05acdd5e": "\u00ad\u00ad\u00ad\u00ad\u00ad\u00ad\u00ad\nALGORITHMIC DISCRIMINATION Protections\nYou should not face discrimination by algorithms \nand systems should be used and designed in an \nequitable \nway. \nAlgorithmic \ndiscrimination \noccurs when \nautomated systems contribute to unjustified different treatment or \nimpacts disfavoring people based on their race, color, ethnicity, \nsex \n(including \npregnancy, \nchildbirth, \nand \nrelated \nmedical \nconditions, \ngender \nidentity, \nintersex \nstatus, \nand \nsexual \norientation), religion, age, national origin, disability, veteran status, \ngenetic infor-mation, or any other classification protected by law. \nDepending on the specific circumstances, such algorithmic \ndiscrimination may violate legal protections. Designers, developers, \nand deployers of automated systems should take proactive and \ncontinuous measures to protect individuals and communities \nfrom algorithmic discrimination and to use and design systems in \nan equitable way.  This protection should include proactive equity", "45e161fa-0aea-477c-9e2b-d72c58d0b8ae": "6. Andrew Wong et al. External validation of a widely implemented proprietary sepsis prediction model in\nhospitalized patients. JAMA Intern Med. 2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626\n7. Jessica Guynn. Facebook while black: Users call it getting 'Zucked,' say talking about racism is censored as hate\nspeech. USA Today. Apr. 24, 2019. https://www.usatoday.com/story/news/2019/04/24/facebook-while-black\u00ad\nzucked-users-say-they-get-blocked-racism-discussion/2859593002/\n8. See, e.g., Michael Levitt. AirTags are being used to track people and cars. Here's what is being done about it.\nNPR. Feb. 18, 2022. https://www.npr.org/2022/02/18/1080944193/apple-airtags-theft-stalking-privacy-tech;\nSamantha Cole. Police Records Show Women Are Being Stalked With Apple AirTags Across the Country.\nMotherboard. Apr. 6, 2022. https://www.vice.com/en/article/y3vj3y/apple-airtags-police-reports-stalking\u00ad\nharassment", "4d4009c8-827f-49e6-86fe-9cd3de962e18": "narrow identified goals, to avoid \"mission creep.\"  Anticipated data collection should be determined to be \nstrictly necessary to the identified goals and should be minimized as much as possible. Data collected based on \nthese identified goals and for a specific context should not be used in a different context without assessing for \nnew privacy risks and implementing appropriate mitigation measures, which may include express consent. \nClear timelines for data retention should be established, with data deleted as soon as possible in accordance \nwith legal or policy-based limitations. Determined data retention timelines should be documented and justi\u00ad\nfied. \nRisk identification and mitigation. Entities that collect, use, share, or store sensitive data should \nattempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropri\u00ad\nately to identified risks. Appropriate responses include determining not to process data when the privacy risks", "24e7b8e5-4c92-4469-8ba9-afc9b858a8d0": "analyses. arXiv. https://arxiv.org/pdf/2402.02008 \nYin, L. et al. (2024) OpenAI\u2019s GPT Is A Recruiter\u2019s Dream Tool. Tests Show There\u2019s Racial Bias. Bloomberg. \nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \nYu, Z. et al. (March 2024) Don\u2019t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf", "398bfad3-816a-4e6b-bd54-087e787ca143": "especially those traditionally marginalized by technological advances. Some panelists also raised the issue of \npower structures \u2013 providing examples of how strong transparency requirements in smart city projects \nhelped to reshape power and give more voice to those lacking the financial or political power to effect change. \nIn discussion of technical and governance interventions that that are needed to protect against the harms \nof these technologies, various panelists emphasized the need for transparency, data collection, and \nflexible and reactive policy development, analogous to how software is continuously updated and deployed. \nSome panelists pointed out that companies need clear guidelines to have a consistent environment for \ninnovation, with principles and guardrails being the key to fostering responsible innovation. \nPanel 2: The Criminal Justice System. This event explored current and emergent uses of technology in", "c06b3759-7d1b-4e0a-983c-fcbb7e7b0b8c": "About AI at NIST: The National Institute of Standards and Technology (NIST) develops measurements, \ntechnology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, \nand fair arti\ufb01cial intelligence (AI) so that its full commercial and societal bene\ufb01ts can be realized without \nharm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for \nmore than a decade, is also helping to ful\ufb01ll the 2023 Executive Order on Safe, Secure, and Trustworthy \nAI. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to \ncontinue the e\ufb00orts set in motion by the E.O. to build the science necessary for safe, secure, and \ntrustworthy development and use of AI. \nAcknowledgments: This report was accomplished with the many helpful comments and contributions \nfrom the community, including the NIST Generative AI Public Working Group, and NIST sta\ufb00 and guest", "e7ca6b6b-c04d-4d3c-bd64-4b77a1ef31bc": "content.\u201d While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers \nto generative foundation models. The foundation model subcategory of \u201cdual-use foundation models\u201d is de\ufb01ned by \nEO 14110 as \u201can AI model that is trained on broad data; generally uses self-supervision; contains at least tens of \nbillions of parameters; is applicable across a wide range of contexts.\u201d  \n2 This pro\ufb01le was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting \nthrough the Director of the National Institute of Standards and Technology (NIST), to develop a companion \nresource to the AI RMF, NIST AI 100\u20131, for generative AI.", "ebd87564-c837-44c5-97f3-1e653bf21e50": "NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1 \n \nJuly 2024 \n \n \n \n \nU.S. Department of Commerce  \nGina M. Raimondo, Secretary \nNational Institute of Standards and Technology  \nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology", "4eddb830-5ece-4763-bf97-69e9f24b61cb": "when implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards\u201d. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify \ufb02aws in the \nvarying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results", "c6b21473-17b3-4411-a801-ff40b859a3ba": "or stereotyping content. \n4. Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of \nbiometric, health, location, or other personally identi\ufb01able information or sensitive data.7 \n5. Environmental Impacts: Impacts due to high compute resource utilization in training or \noperating GAI models, and related outcomes that may adversely impact ecosystems.  \n6. Harmful Bias or Homogenization: Ampli\ufb01cation and exacerbation of historical, societal, and \nsystemic biases; performance disparities8 between sub-groups or languages, possibly due to \nnon-representative training data, that result in discrimination, ampli\ufb01cation of biases, or \nincorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Con\ufb01guration: Arrangements of or interactions between a human and an AI system", "12ead7ff-d728-404a-9342-1331972fe95a": "of a prediction process should receive particular attention to the quality and validity of the predicted outcome \nor label to ensure the goal of the automated system is appropriately identified and measured. Additionally, \njustification should be documented for each data attribute and source to explain why it is appropriate to use \nthat data to inform the results of the automated system and why such use will not violate any applicable laws. \nIn cases of high-dimensional and/or derived attributes, such justifications can be provided as overall \ndescriptions of the attribute generation process and appropriateness. \n19", "fb87ecf3-4b7f-403c-83e3-8df71ec844cc": "DATA PRIVACY \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nTraditional terms of service\u2014the block of text that the public is accustomed to clicking through when using a web\u00ad\nsite or digital app\u2014are not an adequate mechanism for protecting privacy. The American public should be protect\u00ad\ned via built-in privacy protections, data minimization, use and collection limitations, and transparency, in addition \nto being entitled to clear mechanisms to control access to and use of their data\u2014including their metadata\u2014in a \nproactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal data \nshould meet these expectations. \nProtect privacy by design and by default", "cac20d2e-a376-4dbd-a593-6f518f9b3173": "Reduction. arXiv. https://arxiv.org/pdf/2210.05791 \nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324 \nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \nhttps://arxiv.org/pdf/2305.17493v2 \nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language \nModels. PLOS Digital Health. \nhttps://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 \nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \nhttps://arxiv.org/abs/2306.03809 \nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \nhttps://arxiv.org/abs/2302.04844 \nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language \nModels. arXiv. https://arxiv.org/pdf/2310.07298", "d1511faa-d630-4d98-a77c-ed3b7a850b43": "58 \nSatariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \nhttps://www.nytimes.com/2023/02/07/technology/arti\ufb01cial-intelligence-training-deepfake.html \nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart. \nWashington Post. https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/ \nScheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users \nwhen put under pressure. arXiv. https://arxiv.org/abs/2311.07590 \nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm \nReduction. arXiv. https://arxiv.org/pdf/2210.05791 \nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324 \nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \nhttps://arxiv.org/pdf/2305.17493v2", "6c36f7a4-8b20-4e1b-a85d-d8fc8b91c791": "on additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A \nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST\u2019s Trustworthy & \nResponsible AI Resource Center (AIRC), and added to The Language of Trustworthy AI: An In-Depth Glossary of \nTerms. \nThis document was also informed by public comments and consultations from several Requests for Information. \n \n2. \nOverview of Risks Unique to or Exacerbated by GAI \nIn the context of the AI RMF, risk refers to the composite measure of an event\u2019s probability (or \nlikelihood) of occurring and the magnitude or degree of the consequences of the corresponding event. \nSome risks can be assessed as likely to materialize in a given context, particularly those that have been \nempirically demonstrated in similar contexts. Other risks may be unlikely to materialize in a given \ncontext, or may be more speculative and therefore uncertain.", "4ceeb692-63b5-452b-8f9a-fa3268c1d60a": "quality of service and any allocation of services and resources. Quantify harms \nusing: \ufb01eld testing with sub-group populations to determine likelihood of \nexposure to generated content exhibiting harmful bias, AI red-teaming with \ncounterfactual and low-context (e.g., \u201cleader,\u201d \u201cbad guys\u201d) prompts. For ML \npipelines or business processes with categorical or numeric outcomes that rely \non GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, \nequal opportunity, statistical hypothesis tests), to the pipeline or business \noutcome where appropriate; Custom, context-speci\ufb01c metrics developed in \ncollaboration with domain experts and a\ufb00ected communities; Measurements of \nthe prevalence of denigration in generated content in deployment (e.g., sub-\nsampling a fraction of tra\ufb03c and manually annotating denigrating content). \nHarmful Bias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMS-2.11-003", "c0408cd6-041b-43c9-ad5e-684889bf6f95": "they decide they no longer want to deal exclusively with the automated system or be impacted by its results. As a result \nof this lack of human reconsideration, many receive delayed access, or lose access, to rights, opportunities, benefits, \nand critical services. The American public deserves the assurance that, when rights, opportunities, or access are \nmeaningfully at stake and there is a reasonable expectation of an alternative to an automated system, they can conve\u00ad\nniently opt out of an automated system and will not be disadvantaged for that choice. In some cases, such a human or \nother alternative may be required by law, for example it could be required as \u201creasonable accommodations\u201d for people \nwith disabilities. \nIn addition to being able to opt out and use a human alternative, the American public deserves a human fallback \nsystem in the event that an automated system fails or causes harm. No matter how rigorously an automated system is", "17a5f8ab-6ccd-4a9c-8912-f29e87222bca": "sensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, \ngeolocation data, data related to interaction with the criminal justice system, relationship history and legal status such \nas custody and divorce information, and home, work, or school environmental data); or have the reasonable potential \nto be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm \ndue to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even \nif not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video \ndata. \u201cSensitive domains\u201d are those in which activities being conducted can cause material harms, including signifi\u00ad\ncant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains", "8cca91f3-a696-4087-92c9-7800cd79a85f": "tently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harm\u00ad\nful outcomes. \n\u2022\nA proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was imple\u00ad\nmented at hundreds of hospitals around the country. An independent study showed that the model predictions\nunderperformed relative to the designer\u2019s claims while also causing \u2018alert fatigue\u2019 by falsely alerting\nlikelihood of sepsis.6\n\u2022\nOn social media, Black people who quote and criticize racist messages have had their own speech silenced when\na platform\u2019s automated moderation system failed to distinguish this \u201ccounter speech\u201d (or other critique\nand journalism) from the original hateful messages to which such speech responded.7\n\u2022\nA device originally developed to help people track and find lost items has been used as a tool by stalkers to track", "3f8e1815-4f3e-4174-a2a4-e9f13ef07c96": "use under the fair use doctrine. If a GAI system\u2019s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not \nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.  \nTrustworthy AI Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy \nEnhanced  \n2.11. \nObscene, Degrading, and/or Abusive Content \nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, \nand/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can", "fbab112e-6429-4ac4-bc7a-ff1623906f3a": "infrastructure\u201d is any intermediary technology that does not influence or determine the outcome of decision, \nmake or aid in decisions, inform policy implementation, or collect data or observations, including web \nhosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout this \nframework, automated systems that are considered in scope are only those that have the potential to \nmeaningfully impact individuals\u2019 or communi-ties\u2019 rights, opportunities, or access. \nCOMMUNITIES: \u201cCommunities\u201d include: neighborhoods; social network connections (both online and \noffline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organi-\nzational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AI \nand other data-driven automated systems most directly collect data on, make inferences about, and may cause", "084954bd-6218-42f6-9aca-f30820c5110a": "Systems should not employ user experience and design decisions that obfus\u00ad\ncate user choice or burden users with defaults that are privacy invasive. Con\u00ad\nsent should only be used to justify collection of data in cases where it can be \nappropriately and meaningfully given. Any consent requests should be brief, \nbe understandable in plain language, and give you agency over data collection \nand the specific context of use; current hard-to-understand no\u00ad\ntice-and-choice practices for broad uses of data should be changed. Enhanced \nprotections and restrictions for data and inferences related to sensitive do\u00ad\nmains, including health, work, education, criminal justice, and finance, and \nfor data pertaining to youth should put you first. In sensitive domains, your \ndata and related inferences should only be used for necessary functions, and \nyou should be protected by ethical review and use prohibitions. You and your \ncommunities should be free from unchecked surveillance; surveillance tech\u00ad", "a8df712a-6706-492f-9518-a6262542ae49": "international community spoke up about both the promises and potential harms of these technologies, and \nplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these \ndiscussions include that AI has transformative potential to improve Americans\u2019 lives, and that preventing the \nharms of these technologies is both necessary and achievable. The Appendix includes a full list of public engage-\nments. \n4", "8e8dd93e-de2e-417e-a03f-27007f1db51a": "who have trouble with the automated system are able to use human consideration and fallback, with the under\u00ad\nstanding that it may be these users who are most likely to need the human assistance. Similarly, it should be \ntested to ensure that users with disabilities are able to find and use human consideration and fallback and also \nrequest reasonable accommodations or modifications. \nConvenient. Mechanisms for human consideration and fallback should not be unreasonably burdensome as \ncompared to the automated system\u2019s equivalent. \n49", "427aa6e7-aabe-492c-ac98-bd8c53a3021c": "Pel Abbott \nPhiladelphia Unemployment \nProject \nProject On Government Oversight \nRecording Industry Association of \nAmerica \nRobert Wilkens \nRon Hedges \nScience, Technology, and Public \nPolicy Program at University of \nMichigan Ann Arbor \nSecurity Industry Association \nSheila Dean \nSoftware & Information Industry \nAssociation \nStephanie Dinkins and the Future \nHistories Studio at Stony Brook \nUniversity \nTechNet \nThe Alliance for Media Arts and \nCulture, MIT Open Documentary \nLab and Co-Creation Studio, and \nImmerse \nThe International Brotherhood of \nTeamsters \nThe Leadership Conference on \nCivil and Human Rights \nThorn \nU.S. Chamber of Commerce\u2019s \nTechnology Engagement Center \nUber Technologies \nUniversity of Pittsburgh \nUndergraduate Student \nCollaborative \nUpturn \nUS Technology Policy Committee \nof the Association of Computing \nMachinery \nVirginia Puccio \nVisar Berisha and Julie Liss \nXR Association \nXR Safety Initiative", "33f85d63-d76b-429f-baf4-146cf3f0cd92": "Samantha Cole. Police Records Show Women Are Being Stalked With Apple AirTags Across the Country.\nMotherboard. Apr. 6, 2022. https://www.vice.com/en/article/y3vj3y/apple-airtags-police-reports-stalking\u00ad\nharassment\n9. Kristian Lum and William Isaac. To Predict and Serve? Significance. Vol. 13, No. 5, p. 14-19. Oct. 7, 2016.\nhttps://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x; Aaron Sankin, Dhruv Mehrotra,\nSurya Mattu, and Annie Gilbertson. Crime Prediction Software Promised to Be Free of Biases. New Data Shows\nIt Perpetuates Them. The Markup and Gizmodo. Dec. 2, 2021. https://themarkup.org/prediction\u00ad\nbias/2021/12/02/crime-prediction-software-promised-to-be-free-of-biases-new-data-shows-it-perpetuates\u00ad\nthem\n10. Samantha Cole. This Horrifying App Undresses a Photo of Any Woman With a Single Click. Motherboard.\nJune 26, 2019. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman", "ccd503f4-3054-4865-84ae-52140797e686": "including tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. \nInformation Integrity; Information \nSecurity \nMS-4.2-002 \nEvaluate GAI system performance in real-world scenarios to observe its \nbehavior in practical environments and reveal issues that might not surface in \ncontrolled and optimized testing environments. \nHuman-AI Con\ufb01guration; \nConfabulation; Information \nSecurity \nMS-4.2-003 \nImplement interpretability and explainability methods to evaluate GAI system \ndecisions and verify alignment with intended purpose. \nInformation Integrity; Harmful Bias \nand Homogenization \nMS-4.2-004 \nMonitor and document instances where human operators or other systems \noverride the GAI's decisions. Evaluate these cases to understand if the overrides \nare linked to issues related to content provenance. \nInformation Integrity \nMS-4.2-005", "f6345ae8-5537-4545-805a-d419ccbd7b58": "Criminal Defense Lawyers \nO\u2019Neil Risk Consulting & \nAlgorithmic Auditing \nThe Partnership on AI \nPinterest \nThe Plaintext Group \npymetrics \nSAP \nThe Security Industry Association \nSoftware and Information Industry \nAssociation (SIIA) \nSpecial Competitive Studies Project \nThorn \nUnited for Respect \nUniversity of California at Berkeley \nCitris Policy Lab \nUniversity of California at Berkeley \nLabor Center \nUnfinished/Project Liberty \nUpturn \nUS Chamber of Commerce \nUS Chamber of Commerce \nTechnology Engagement Center \nA.I. Working Group\nVibrent Health\nWarehouse Worker Resource\nCenter\nWaymap\n62", "e93a97bd-f7f2-494f-815a-6c2483c7f929": "3 \nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \nfrom interactions between a human and an AI system.  \n\u2022 \nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \ndistribution of harmful deepfake images, or the long-term e\ufb00ect of disinformation on societal \ntrust in public institutions. \nThe presence of risks and where they fall along the dimensions above will vary depending on the \ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not \nlimited to GAI model or system architecture, training mechanisms and libraries, data types used for \ntraining or \ufb01ne-tuning, levels of model access or availability of model weights, and application or use \ncase context. \nOrganizations may choose to tailor how they measure GAI risks based on these characteristics. They may", "55a82d79-3651-4c07-9925-501bd29b486b": "of associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual \nproperty law), in order to perform such evaluations. Mechanisms should be included to ensure that system \naccess for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to \nprovide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot \nbe revoked without reasonable and verified justification. \nReporting.12 Entities responsible for the development or use of automated systems should provide \nregularly-updated reports that include: an overview of the system, including how it is embedded in the \norganization\u2019s business processes or other activities, system goals, any human-run procedures that form a \npart of the system, and specific performance expectations; a description of any data used to train machine", "6bd50fba-21b5-4c8d-9a33-7e31811f4688": "43 \nMG-3.1-005 Review various transparency artifacts (e.g., system cards and model cards) for \nthird-party models. \nInformation Integrity; Information \nSecurity; Value Chain and \nComponent Integration \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \n \nMANAGE 3.2: Pre-trained models which are used for development are monitored as part of AI system regular monitoring and \nmaintenance. \nAction ID \nSuggested Action \nGAI Risks \nMG-3.2-001 \nApply explainable AI (XAI) techniques (e.g., analysis of embeddings, model \ncompression/distillation, gradient-based attributions, occlusion/term reduction, \ncounterfactual prompts, word clouds) as part of ongoing continuous \nimprovement processes to mitigate risks related to unexplainable GAI systems. \nHarmful Bias and Homogenization \nMG-3.2-002 \nDocument how pre-trained models have been adapted (e.g., \ufb01ne-tuned, or \nretrieval-augmented generation) for the speci\ufb01c generative task, including any", "495a1d1a-9cc6-41a4-9e4c-e23a3383de45": "results of any surveillance pre-deployment assessment, including disparity assessment in the real-world \ndeployment context, the specific identified goals of any data collection, and the assessment done to ensure \nonly the minimum required data is collected. It should also include documentation about the scope limit \nassessments, including data retention timelines and associated justification, and an assessment of the \nimpact of surveillance or data collection on rights, opportunities, and access. Where possible, this \nassessment of the impact of surveillance should be done by an independent party. Reporting should be \nprovided in a clear and machine-readable manner.  \n35", "67f861ac-d5cf-471f-9f1d-2de2d9b2605c": "ENDNOTES\n23. National Science Foundation. National Artificial Intelligence Research Institutes. Accessed Sept. 12,\n2022. https://beta.nsf.gov/funding/opportunities/national-artificial-intelligence-research-institutes\n24. National Science Foundation. Cyber-Physical Systems. Accessed Sept. 12, 2022. https://beta.nsf.gov/\nfunding/opportunities/cyber-physical-systems-cps\n25. National Science Foundation. Secure and Trustworthy Cyberspace. Accessed Sept. 12, 2022. https://\nbeta.nsf.gov/funding/opportunities/secure-and-trustworthy-cyberspace-satc\n26. National Science Foundation. Formal Methods in the Field. Accessed Sept. 12, 2022. https://\nbeta.nsf.gov/funding/opportunities/formal-methods-field-fmitf\n27. National Science Foundation. Designing Accountable Software Systems. Accessed Sept. 12, 2022.\nhttps://beta.nsf.gov/funding/opportunities/designing-accountable-software-systems-dass\n28. The Leadership Conference Education Fund. The Use Of Pretrial \u201cRisk Assessment\u201d Instruments: A", "a923ff4d-0a75-48f6-9690-5debb7aea714": "APPENDIX\nPanel 4: Artificial Intelligence and Democratic Values. This event examined challenges and opportunities in \nthe design of technology that can help support a democratic vision for AI. It included discussion of the \ntechnical aspects \nof \ndesigning \nnon-discriminatory \ntechnology, \nexplainable \nAI, \nhuman-computer \ninteraction with an emphasis on community participation, and privacy-aware design. \nWelcome:\n\u2022\nSorelle Friedler, Assistant Director for Data and Democracy, White House Office of Science and\nTechnology Policy\n\u2022\nJ. Bob Alotta, Vice President for Global Programs, Mozilla Foundation\n\u2022\nNavrina Singh, Board Member, Mozilla Foundation\nModerator: Kathy Pham Evans, Deputy Chief Technology Officer for Product and Engineering, U.S \nFederal Trade Commission. \nPanelists: \n\u2022\nLiz O\u2019Sullivan, CEO, Parity AI\n\u2022\nTimnit Gebru, Independent Scholar\n\u2022\nJennifer Wortman Vaughan, Senior Principal Researcher, Microsoft Research, New York City\n\u2022", "2b61969b-a4d8-4840-9763-2a8aab744de4": "Additionally, these systems can produce feedback loops and compounded harm, collecting data from \ncommunities and using it to reinforce inequality. Various panelists suggested that these harms could be \nmitigated by ensuring community input at the beginning of the design process, providing ways to opt out of \nthese systems and use associated human-driven mechanisms instead, ensuring timeliness of benefit payments, \nand providing clear notice about the use of these systems and clear explanations of how and what the \ntechnologies are doing. Some panelists suggested that technology should be used to help people receive \nbenefits, e.g., by pushing benefits to those in need and ensuring automated decision-making systems are only \nused to provide a positive outcome; technology shouldn't be used to take supports away from people who need \nthem. \nPanel 6: The Healthcare System. This event explored current and emerging uses of technology in the", "70d41c71-c073-4d34-9f2d-1e5d9bec9f7f": "control over their data \nUse-specific consent. Consent practices should not allow for abusive surveillance practices. Where data \ncollectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specif\u00ad\nic time durations, and for use by specific entities. Consent should not extend if any of these conditions change; \nconsent should be re-acquired before using data if the use case changes, a time limit elapses, or data is trans\u00ad\nferred to another entity (including being shared or sold). Consent requested should be limited in scope and \nshould not request consent beyond what is required. Refusal to provide consent should be allowed, without \nadverse effects, to the greatest extent possible based on the needs of the use case. \nBrief and direct consent requests. When seeking consent from users short, plain language consent \nrequests should be used so that users understand for what use contexts, time span, and entities they are", "0bf3e239-cb0c-4e32-8add-9935c1c54207": "be enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in \nour lives. \nThis framework describes protections that should be applied with respect to all automated systems that \nhave the potential to meaningfully impact individuals' or communities' exercise of: \nRIGHTS, OPPORTUNITIES, OR ACCESS\nCivil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimi\u00ad\nnation, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both \npublic and private sector contexts; \nEqual opportunities, including equitable access to education, housing, credit, employment, and other \nprograms; or, \nAccess to critical resources or services, such as healthcare, financial services, safety, social services, \nnon-deceptive information about goods and services, and government benefits.", "82872239-471a-4c64-a6f1-656a7a72f779": "parent models where the model itself can be understood by people needing to directly examine it. \nDemonstrate access to human alternatives, consideration, and fallback \nReporting. Reporting should include an assessment of timeliness and the extent of additional burden for \nhuman alternatives, aggregate statistics about who chooses the human alternative, along with the results of \nthe assessment about brevity, clarity, and accessibility of notice and opt-out instructions. Reporting on the \naccessibility, timeliness, and effectiveness of human consideration and fallback should be made public at regu\u00ad\nlar intervals for as long as the system is in use. This should include aggregated information about the number \nand type of requests for consideration, fallback employed, and any repeated requests; the timeliness of the \nhandling of these requests, including mean wait times for different types of requests as well as maximum wait", "1d4dbe76-6294-4c51-bd9b-b22f92caf958": "and deployers of automated systems should take proactive and \ncontinuous measures to protect individuals and communities \nfrom algorithmic discrimination and to use and design systems in \nan equitable way.  This protection should include proactive equity \nassessments as part of the system design, use of representative data \nand protection against proxies for demographic features, ensuring \naccessibility for people with disabilities in design and development, \npre-deployment and ongoing disparity testing and mitigation, and \nclear organizational oversight. Independent evaluation and plain \nlanguage reporting in the form of an algorithmic impact assessment, \nincluding disparity testing results and mitigation information, \nshould be performed and made public whenever possible to confirm \nthese protections.\n23", "d46813ed-51ab-4c0a-b988-dee378421920": "and journalism) from the original hateful messages to which such speech responded.7\n\u2022\nA device originally developed to help people track and find lost items has been used as a tool by stalkers to track\nvictims\u2019 locations in violation of their privacy and safety. The device manufacturer took steps after release to\nprotect people from unwanted tracking by alerting people on their phones when a device is found to be moving\nwith them over time and also by having the device make an occasional noise, but not all phones are able\nto receive the notification and the devices remain a safety concern due to their misuse.8 \n\u2022\nAn algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit,\neven if those neighborhoods were not the ones with the highest crime rates. These incorrect crime predictions\nwere the result of a feedback loop generated from the reuse of data from previous arrests and algorithm\npredictions.9\n16", "204e22f0-74a0-479d-8ecf-3221e37181c2": "Applying The Blueprint for an AI Bill of Rights \nDEFINITIONS\nALGORITHMIC DISCRIMINATION: \u201cAlgorithmic discrimination\u201d occurs when automated systems \ncontribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, \nsex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \norientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifica-\ntion protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate \nlegal protections. Throughout this framework the term \u201calgorithmic discrimination\u201d takes this meaning (and \nnot a technical understanding of discrimination as distinguishing between items). \nAUTOMATED SYSTEM: An \"automated system\" is any system, software, or process that uses computation as", "5c8efab7-5f74-4a2c-b30b-6009f3cb9f05": "systems and some other digital technologies, the risk is exacerbated for GAI due to the scale of the \ntraining data, which may be too large for humans to vet; the di\ufb03culty of training foundation models, \nwhich leads to extensive reuse of limited numbers of models; and the extent to which GAI may be \nintegrated into other devices and services. As GAI systems often involve many distinct third-party \ncomponents and data sources, it may be di\ufb03cult to attribute issues in a system\u2019s behavior to any one of \nthese sources. \nErrors in third-party GAI components can also have downstream impacts on accuracy and robustness. \nFor example, test datasets commonly used to benchmark or validate models can contain label errors. \nInaccuracies in these labels can impact the \u201cstability\u201d or robustness of these benchmarks, which many \nGAI practitioners consider during the model selection process.  \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with", "917b2edd-2501-4d54-9cc2-3c49c582396b": "Cantellus Group \nCenter for American Progress \nCenter for Democracy and \nTechnology \nCenter on Privacy and Technology \nat Georgetown Law \nChristiana Care \nColor of Change \nCoworker \nData Robot \nData Trust Alliance \nData and Society Research Institute \nDeepmind \nEdSAFE AI Alliance \nElectronic Privacy Information \nCenter (EPIC) \nEncode Justice \nEqual AI \nGoogle \nHitachi's AI Policy Committee \nThe Innocence Project \nInstitute of Electrical and \nElectronics Engineers (IEEE) \nIntuit \nLawyers Committee for Civil Rights \nUnder Law \nLegal Aid Society \nThe Leadership Conference on \nCivil and Human Rights \nMeta \nMicrosoft \nThe MIT AI Policy Forum \nMovement Alliance Project \nThe National Association of \nCriminal Defense Lawyers \nO\u2019Neil Risk Consulting & \nAlgorithmic Auditing \nThe Partnership on AI \nPinterest \nThe Plaintext Group \npymetrics \nSAP \nThe Security Industry Association \nSoftware and Information Industry \nAssociation (SIIA) \nSpecial Competitive Studies Project \nThorn", "5cd28879-abe6-4109-a650-321a9334313b": "rience of harm to victims of non-consensual intimate images can be devastatingly real\u2014affecting their personal\nand professional lives, and impacting their mental and physical health.10\n\u2022\nA company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its driv\u00ad\ners, but the system incorrectly penalized drivers when other cars cut them off or when other events beyond\ntheir control took place on the road. As a result, drivers were incorrectly ineligible to receive a bonus.11\n17", "b0312159-0b91-4089-8744-dd57e4b8672f": "Information Security \nGV-1.6-002 De\ufb01ne any inventory exemptions in organizational policies for GAI systems \nembedded into application software. \nValue Chain and Component \nIntegration \nGV-1.6-003 \nIn addition to general model, governance, and risk information, consider the \nfollowing items in GAI system inventory entries: Data provenance information \n(e.g., source, signatures, versioning, watermarks); Known issues reported from \ninternal bug tracking or external information sharing resources (e.g., AI incident \ndatabase, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles \nand responsibilities; Special rights and considerations for intellectual property, \nlicensed works, or personal, privileged, proprietary or sensitive data; Underlying \nfoundation models, versions of underlying models, and access modes. \nData Privacy; Human-AI \nCon\ufb01guration; Information \nIntegrity; Intellectual Property; \nValue Chain and Component \nIntegration", "e6b2fc26-b433-4d4e-8e62-a69a52ed465c": "attended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loan\nthan an applicant who did not attend an HBCU. This was found to be true even when controlling for\nother credit-related factors.32\n\u2022\nA hiring tool that learned the features of a company's employees (predominantly men) rejected women appli\u00ad\ncants for spurious and discriminatory reasons; resumes with the word \u201cwomen\u2019s,\u201d such as \u201cwomen\u2019s\nchess club captain,\u201d were penalized in the candidate ranking.33\n\u2022\nA predictive model marketed as being able to predict whether students are likely to drop out of school was\nused by more than 500 universities across the country. The model was found to use race directly as a predictor,\nand also shown to have large disparities by race; Black students were as many as four times as likely as their\notherwise similar white peers to be deemed at high risk of dropping out. These risk scores are used by advisors", "971f64d8-51aa-4637-a438-a6f001878bf1": "ering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermar-\nket cashier ads to women and jobs with taxi companies to primarily Black people.42\u00ad\n\u2022\nBody scanners, used by TSA at airport checkpoints, require the operator to select a \u201cmale\u201d or \u201cfemale\u201d\nscanning setting based on the passenger\u2019s sex, but the setting is chosen based on the operator\u2019s perception of\nthe passenger\u2019s gender identity. These scanners are more likely to flag transgender travelers as requiring\nextra screening done by a person. Transgender travelers have described degrading experiences associated\nwith these extra screenings.43 TSA has recently announced plans to implement a gender-neutral algorithm44 \nwhile simultaneously enhancing the security effectiveness capabilities of the existing technology. \n\u2022\nThe National Disabled Law Students Association expressed concerns that individuals with disabilities were", "4a84ea14-0f78-46ed-81a3-b80591dae0e4": "safeguards against human bias, should be proportionate to the potential of the automated system to meaning\u00ad\nfully impact rights, opportunities, or access. Automated systems that have greater control over outcomes, \nprovide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to \nmeaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and over\u00ad\nsight of human consideration and fallback mechanisms. \nAccessible. Mechanisms for human consideration and fallback, whether in-person, on paper, by phone, or \notherwise provided, should be easy to find and use. These mechanisms should be tested to ensure that users \nwho have trouble with the automated system are able to use human consideration and fallback, with the under\u00ad\nstanding that it may be these users who are most likely to need the human assistance. Similarly, it should be", "73792944-31ae-40b5-9693-e7e5e7b816c3": "50 \nParticipatory Engagement Methods \nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage \nexternal stakeholders in product development or review. Focus groups with select experts can provide \nfeedback on a range of issues. Small user studies can provide feedback from representative groups or \npopulations. Anonymous surveys can be used to poll or gauge reactions to speci\ufb01c features. Participatory \nengagement methods are often less structured than \ufb01eld testing or red teaming, and are more \ncommonly used in early stages of AI or product development.  \nField Testing \nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions \nunder which the GAI system will be deployed. Field style tests can be adapted from a focus on user \npreferences and experiences towards AI risks and impacts \u2013 both negative and positive. When carried", "d75cb414-2cab-4fcf-9e3b-e79c52f40541": "should be overruled, the new decision will be effectively enacted. This includes ensuring that the new \ndecision is entered into the automated system throughout its components, any previous repercussions from \nthe old decision are also overturned, and safeguards are put in place to help ensure that future decisions do \nnot result in the same errors. \nMaintained. The human consideration and fallback process and any associated automated processes \nshould be maintained and supported as long as the relevant automated system continues to be in use. \nInstitute training, assessment, and oversight to combat automation bias and ensure any \nhuman-based components of a system are effective. \nTraining and assessment. Anyone administering, interacting with, or interpreting the outputs of an auto\u00ad\nmated system should receive training in that system, including how to properly interpret outputs of a system", "e6de1e79-112d-41df-bb0f-ed5f0973ae4f": "ENDNOTES\n107. Centers for Medicare & Medicaid Services. Biden-Harris Administration Quadruples the Number\nof Health Care Navigators Ahead of HealthCare.gov Open Enrollment Period. Aug. 27, 2021.\nhttps://www.cms.gov/newsroom/press-releases/biden-harris-administration-quadruples-number\u00ad\nhealth-care-navigators-ahead-healthcaregov-open\n108. See, e.g., McKinsey & Company. The State of Customer Care in 2022. July 8, 2022. https://\nwww.mckinsey.com/business-functions/operations/our-insights/the-state-of-customer-care-in-2022;\nSara Angeles. Customer Service Solutions for Small Businesses. Business News Daily.\nJun. 29, 2022. https://www.businessnewsdaily.com/7575-customer-service-solutions.html\n109. Mike Hughes. Are We Getting The Best Out Of Our Bots? Co-Intelligence Between Robots &\nHumans. Forbes. Jul. 14, 2022.\nhttps://www.forbes.com/sites/mikehughes1/2022/07/14/are-we-getting-the-best-out-of-our-bots-co\u00ad\nintelligence-between-robots--humans/?sh=16a2bd207395", "0d78c95c-b646-4b52-8878-773e2425c443": "National Institute of Standards and Technology (2023) AI Risk Management Framework. \nhttps://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \nRisks and Trustworthiness. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3-sec-characteristics \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 6: AI \nRMF Pro\ufb01les. https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Pro\ufb01les/6-sec-pro\ufb01le \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: \nDescriptions of AI Actor Tasks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors%\n20in%20this%20category,data%20providers%2C%20system%20funders%2C%20product", "ad832177-3f64-4cce-9bfb-43a601c01c55": "contest its impacts on you. Human consideration and fallback \nshould be accessible, equitable, effective, maintained, accompanied \nby appropriate operator training, and should not impose an unrea\u00ad\nsonable burden on the public. Automated systems with an intended \nuse within sensitive domains, including, but not limited to, criminal \njustice, employment, education, and health, should additionally be \ntailored to the purpose, provide meaningful access for oversight, \ninclude training for any people interacting with the system, and in\u00ad\ncorporate human consideration for adverse or high-risk decisions. \nReporting that includes a description of these human governance \nprocesses and assessment of their timeliness, accessibility, out\u00ad\ncomes, and effectiveness should be made public whenever possible. \nHUMAN ALTERNATIVES, CONSIDERATION\nALLBACK\nF\nAND\n, \n46", "7acd026f-2afd-41ca-9d24-66c6d7b4e20d": "DATA PRIVACY \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nData access and correction. People whose data is collected, used, shared, or stored by automated \nsystems should be able to access data and metadata about themselves, know who has access to this data, and \nbe able to correct it if necessary. Entities should receive consent before sharing data with other entities and \nshould keep records of what data is shared and with whom. \nConsent withdrawal and data deletion. Entities should allow (to the extent legally permissible) with\u00ad\ndrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of \ntheir data from any systems (e.g., machine learning models) derived from that data.68", "343696f3-0d02-443f-ac4c-ed3e2871430d": "52. PAVE Interagency Task Force on Property Appraisal and Valuation Equity. Action Plan to Advance\nProperty Appraisal and Valuation Equity: Closing the Racial Wealth Gap by Addressing Mis-valuations for\nFamilies and Communities of Color. March 2022. https://pave.hud.gov/sites/pave.hud.gov/files/\ndocuments/PAVEActionPlan.pdf\n53. U.S. Equal Employment Opportunity Commission. The Americans with Disabilities Act and the Use of\nSoftware, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees. EEOC\u00ad\nNVTA-2022-2. May 12, 2022. https://www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use\u00ad\nsoftware-algorithms-and-artificial-intelligence; U.S. Department of Justice. Algorithms, Artificial\nIntelligence, and Disability Discrimination in Hiring. May 12, 2022. https://beta.ada.gov/resources/ai\u00ad\nguidance/\n54. Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in", "974b2f2d-fc44-4056-83c8-1c1bd4129f43": "Solaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \nhttps://arxiv.org/abs/2302.04844 \nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language \nModels. arXiv. https://arxiv.org/pdf/2310.07298 \nStanford, S. et al. (2023) Whose Opinions Do Language Models Re\ufb02ect? arXiv. \nhttps://arxiv.org/pdf/2303.17548 \nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \nhttps://arxiv.org/pdf/1906.02243 \nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \nhttps://www.whitehouse.gov/wp-\ncontent/uploads/legacy_drupal_\ufb01les/omb/circulars/A130/a130revised.pdf \nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of \nArti\ufb01cial Intelligence. https://www.whitehouse.gov/brie\ufb01ng-room/presidential-\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-", "de313654-8f9f-427c-a57e-9f8d1a65342a": "SAFE AND EFFECTIVE \nSYSTEMS \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \n\u2022\nAI-enabled \u201cnudification\u201d technology that creates images where people appear to be nude\u2014including apps that\nenable non-technical users to create or alter images of individuals without their consent\u2014has proliferated at an\nalarming rate. Such technology is becoming a common form of image-based abuse that disproportionately\nimpacts women. As these tools become more sophisticated, they are producing altered images that are increasing\u00ad\nly realistic and are difficult for both humans and AI to detect as inauthentic. Regardless of authenticity, the expe\u00ad\nrience of harm to victims of non-consensual intimate images can be devastatingly real\u2014affecting their personal\nand professional lives, and impacting their mental and physical health.10\n\u2022", "be0e1082-2a53-4197-82a9-a27e52afe77c": "professionals in mapping, measuring, and managing those risks. \nCBRN Information or Capabilities; \nDangerous, Violent, or Hateful \nContent; Information Security \nGV-2.1-005 \nCreate mechanisms to provide protections for whistleblowers who report, based \non reasonable belief, when the organization violates relevant laws or poses a \nspeci\ufb01c and empirically well-substantiated negative risk to public safety (or has \nalready caused harm). \nCBRN Information or Capabilities; \nDangerous, Violent, or Hateful \nContent \nAI Actor Tasks: Governance and Oversight", "7618ed6c-bb6e-48be-9c01-0277f256a286": "CBRN Information or Capabilities; \nDangerous, Violent, or Hateful \nContent; Obscene, Degrading, \nand/or Abusive Content \nAI Actor Tasks: AI Deployment \n \nMAP 1.2: Interdisciplinary AI Actors, competencies, skills, and capacities for establishing context re\ufb02ect demographic diversity and \nbroad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary \ncollaboration are prioritized. \nAction ID \nSuggested Action \nGAI Risks \nMP-1.2-001 \nEstablish and empower interdisciplinary teams that re\ufb02ect a wide range of \ncapabilities, competencies, demographic groups, domain expertise, educational \nbackgrounds, lived experiences, professions, and skills across the enterprise to \ninform and conduct risk measurement and management functions. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMP-1.2-002 \nVerify that data or benchmarks used in risk measurement, and users,", "236709d1-4c94-4e47-8c2f-9b1935ed3101": "there is not algorithmic discrimination, especially based on community membership, when deployed in a \nspecific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as the \nsystem is in use. \nLimited and proportionate surveillance. Surveillance should be avoided unless it is strictly necessary \nto achieve a legitimate purpose and it is proportionate to the need. Designers, developers, and deployers of \nsurveillance systems should use the least invasive means of monitoring available and restrict monitoring to the \nminimum number of subjects possible. To the greatest extent possible consistent with law enforcement and \nnational security needs, individuals subject to monitoring should be provided with clear and specific notice \nbefore it occurs and be informed about how the data gathered through surveillance will be used. \nScope limits on surveillance to protect rights and democratic values. Civil liberties and civil", "56844b37-a0b4-46b0-ab49-2e91ba5cc3ed": "systematic, and anecdotal assessments. \nHuman-AI Con\ufb01guration; \nConfabulation \nMS-2.5-002 \nDocument the extent to which human domain knowledge is employed to \nimprove GAI system performance, via, e.g., RLHF, \ufb01ne-tuning, retrieval-\naugmented generation, content moderation, business rules. \nHuman-AI Con\ufb01guration \nMS-2.5-003 Review and verify sources and citations in GAI system outputs during pre-\ndeployment risk measurement and ongoing monitoring activities. \nConfabulation \nMS-2.5-004 Track and document instances of anthropomorphization (e.g., human images, \nmentions of human feelings, cyborg imagery or motifs) in GAI system interfaces. Human-AI Con\ufb01guration \nMS-2.5-005 Verify GAI system training data and TEVV data provenance, and that \ufb01ne-tuning \nor retrieval-augmented generation data is grounded. \nInformation Integrity \nMS-2.5-006 \nRegularly review security and safety guardrails, especially if the GAI system is", "2637f07f-0e18-4e00-95a1-017302c1eebf": "tolerance, and resources of the Framework user. AI RMF pro\ufb01les assist organizations in deciding how to \nbest manage AI risks in a manner that is well-aligned with their goals, considers legal/regulatory \nrequirements and best practices, and re\ufb02ects risk management priorities. Consistent with other AI RMF \npro\ufb01les, this pro\ufb01le o\ufb00ers insights into how risk can be managed across various stages of the AI lifecycle \nand for GAI as a technology.  \nAs GAI covers risks of models or applications that can be used across use cases or sectors, this document \nis an AI RMF cross-sectoral pro\ufb01le. Cross-sectoral pro\ufb01les can be used to govern, map, measure, and \nmanage risks associated with activities or business processes common across sectors, such as the use of \nlarge language models (LLMs), cloud-based services, or acquisition. \nThis document de\ufb01nes risks that are novel to or exacerbated by the use of GAI. After introducing and", "e5a60d6b-9c57-480a-9340-dfc5264e7bf6": "guidelines for publicly available incident reporting which include information about AI actor \nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \ninputs and content delivered through these plugins is often distributed, with inconsistent or insu\ufb03cient \naccess control. \nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate \nsmoother sharing of information with relevant AI Actors. Regular information sharing, change \nmanagement records, version history and metadata can also empower AI Actors responding to and \nmanaging AI incidents.", "a263bf7c-1285-4818-b081-b61114301e97": "AI Actor Tasks: AI Development, Human Factors, TEVV \n \nMEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for \nconditions similar to deployment setting(s). Measures are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.3-001 Consider baseline model performance on suites of benchmarks when selecting a \nmodel for \ufb01ne tuning or enhancement with retrieval-augmented generation. \nInformation Security; \nConfabulation \nMS-2.3-002 Evaluate claims of model capabilities using empirically validated methods. \nConfabulation; Information \nSecurity \nMS-2.3-003 Share results of pre-deployment testing with relevant GAI Actors, such as those \nwith system release approval authority. \nHuman-AI Con\ufb01guration", "c9cd577c-1f3d-4db8-927a-f495c3f274be": "28 \nMAP 5.2: Practices and personnel for supporting regular engagement with relevant AI Actors and integrating feedback about \npositive, negative, and unanticipated impacts are in place and documented. \nAction ID \nSuggested Action \nGAI Risks \nMP-5.2-001 \nDetermine context-based measures to identify if new impacts are present due to \nthe GAI system, including regular engagements with downstream AI Actors to \nidentify and quantify new contexts of unanticipated impacts of GAI systems. \nHuman-AI Con\ufb01guration; Value \nChain and Component Integration \nMP-5.2-002 \nPlan regular engagements with AI Actors responsible for inputs to GAI systems, \nincluding third-party data and algorithms, to review and evaluate unanticipated \nimpacts. \nHuman-AI Con\ufb01guration; Value \nChain and Component Integration \nAI Actor Tasks: AI Deployment, AI Design, AI Impact Assessment, A\ufb00ected Individuals and Communities, Domain Experts, End-\nUsers, Human Factors, Operation and Monitoring", "c282a38e-4939-4b5c-ab4b-7db57caa6092": "about the availability of these feedback channels. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \nintended. Results are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-4.2-001 \nConduct adversarial testing at a regular cadence to map and measure GAI risks, \nincluding tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. \nInformation Integrity; Information \nSecurity \nMS-4.2-002", "aa70db96-141a-48b6-b812-bc2d7cbd4f2d": "24 \nMAP 2.1: The speci\ufb01c tasks and methods used to implement the tasks that the AI system will support are de\ufb01ned (e.g., classi\ufb01ers, \ngenerative models, recommenders). \nAction ID \nSuggested Action \nGAI Risks \nMP-2.1-001 \nEstablish known assumptions and practices for determining data origin and \ncontent lineage, for documentation and evaluation purposes. \nInformation Integrity \nMP-2.1-002 \nInstitute test and evaluation for data and content \ufb02ows within the GAI system, \nincluding but not limited to, original data sources, data transformations, and \ndecision-making criteria. \nIntellectual Property; Data Privacy \nAI Actor Tasks: TEVV \n \nMAP 2.2: Information about the AI system\u2019s knowledge limits and how system output may be utilized and overseen by humans is \ndocumented. Documentation provides su\ufb03cient information to assist relevant AI Actors when making decisions and taking \nsubsequent actions. \nAction ID \nSuggested Action \nGAI Risks \nMP-2.2-001", "67a07331-d1a7-44fe-bac2-0b7a08a0336b": "consequential decision-making settings like employment and lending can result in increased susceptibility by \nsystems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm.  \n4 Many studies have projected the impact of AI on the workforce and labor markets. Fewer studies have examined \nthe impact of GAI on the labor market, though some industry surveys indicate that that both employees and \nemployers are pondering this disruption.", "11142b8e-6ed2-4e1e-97b6-f5aa0ae91259": "such statistical prediction can produce factually accurate and consistent outputs, it can also produce \noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when \nit comes to open-ended prompts for long-form responses and in domains which require highly \ncontextual and/or domain expertise.  \nRisks from confabulations may arise when users believe false content \u2013 often due to the con\ufb01dent nature \nof the response \u2013 leading users to act upon or promote the false information. This poses a challenge for \nmany real-world applications, such as in healthcare, where a confabulated summary of patient \ninformation reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \ninto applications involving consequential decision making.", "e6ef7c2b-18a3-4c51-b0a8-68e65f03912c": "Protect the public from harm in a proactive and ongoing manner \nConsultation. The public should be consulted in the design, implementation, deployment, acquisition, and \nmaintenance phases of automated system development, with emphasis on early-stage consultation before a \nsystem is introduced or a large change implemented. This consultation should directly engage diverse impact\u00ad\ned communities to consider concerns and risks that may be unique to those communities, or disproportionate\u00ad\nly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold\u00ad\ners may differ depending on the specific automated system and development phase, but should include \nsubject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as \ncivil rights, civil liberties, and privacy experts. For private sector applications, consultations before product", "7d17f7f1-2c22-4077-9a47-d1a94a8e4666": "Further, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does not \ncomply with the Privacy Act\u2019s requirements. Among other things, a court may order a federal agency to amend or \ncorrect an individual\u2019s information in its records or award monetary damages if an inaccurate, irrelevant, untimely, \nor incomplete record results in an adverse determination about an individual\u2019s \u201cqualifications, character, rights, \u2026 \nopportunities\u2026, or benefits.\u201d \nNIST\u2019s Privacy Framework provides a comprehensive, detailed and actionable approach for \norganizations to manage privacy risks. The NIST Framework gives organizations ways to identify and \ncommunicate their privacy risks and goals to support ethical decision-making in system, product, and service \ndesign or deployment, as well as the measures they are taking to demonstrate compliance with applicable laws", "c018662e-8711-4e1e-b1c2-996ba8b8ac4b": "the severity of certain diseases in Black Americans. Instances of discriminatory practices built into and \nresulting from AI and other automated systems exist across many industries, areas, and contexts. While automated \nsystems have the capacity to drive extraordinary advances and innovations, algorithmic discrimination \nprotections should be built into their design, deployment, and ongoing use. \nMany companies, non-profits, and federal government agencies are already taking steps to ensure the public \nis protected from algorithmic discrimination. Some companies have instituted bias testing as part of their product \nquality assessment and launch procedures, and in some cases this testing has led products to be changed or not \nlaunched, preventing harm to the public. Federal government agencies have been developing standards and guidance \nfor the use of automated systems in order to help prevent bias. Non-profits and companies have developed best", "3711e070-6ed6-47f5-be5e-46ab64735ea7": "nologies. There are companies working to incorporate additional protections in their design and use of auto\u00ad\nmated systems, and researchers developing innovative guardrails. Advocates, researchers, and government \norganizations have proposed principles for the ethical use of AI and other automated systems. These include \nthe Organization for Economic Co-operation and Development\u2019s (OECD\u2019s) 2019 Recommendation on Artificial \nIntelligence, which includes principles for responsible stewardship of trustworthy AI and which the United \nStates adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \nFederal Government, which sets out principles that govern the federal government\u2019s use of AI. The Blueprint \nfor an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 \non Advancing Racial Equity and Support for Underserved Communities Through the Federal Government."}}